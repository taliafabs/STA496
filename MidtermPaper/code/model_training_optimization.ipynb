{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression TensorFlow Learning Diary\n",
        "This notebook documents my learning and practice process of using the Python `TensorFlow` framework to train a logistic regression model (or a two-layer neural network) to classify Harris and Trump voters on the 2024 CES survey dataset.\n",
        "\n",
        "I used the following resources:\n",
        "* https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/core/logistic_regression_core.ipynb#scrollTo=VMXEhrZuKECV\n",
        "* https://www.tensorflow.org/guide/core/logistic_regression_core\n",
        "* https://d2l.ai/index.html#\n",
        "* https://developers.google.com/machine-learning/crash-course/overfitting/regularization"
      ],
      "metadata": {
        "id": "VdUyXtSUcLlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression is often used for binary classification.\n",
        "\n",
        "Linear component: $z = w^\\top x$\n",
        "Apply logistic (sigmoid) activation: $y = \\sigma(z)$ where $\\sigma(z) = \\frac{e^z}{1 + e^z}$\n",
        "\n",
        "Binary cross entropy loss is often used: $L_{CE}(y\\_pred, y) = -y log(y\\_pred) - (1-y)log(1-y\\_pred)$"
      ],
      "metadata": {
        "id": "Io9YNSGAZGbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# workplace setup (copied from TensorFlow logistic regression documentation)\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as sk_metrics\n",
        "import tempfile\n",
        "import os\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras import regularizers\n",
        "\n",
        "# Preset matplotlib figure sizes.\n",
        "matplotlib.rcParams['figure.figsize'] = [9, 6]\n",
        "\n",
        "print(tf.__version__)\n",
        "# To make the results reproducible, set the random seed value.\n",
        "tf.random.set_seed(22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGMb78LacuIV",
        "outputId": "dc88fce9-ffd6-4151-eccf-c7b35d11cb23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data\n",
        "url = \"https://github.com/taliafabs/STA496/raw/main/MidtermPaper/data/ces24_analysis_data.parquet\"\n",
        "ces24_df = pd.read_parquet(url, engine=\"pyarrow\")"
      ],
      "metadata": {
        "id": "K6OihF87cmUu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data preparation\n",
        "race_map = {\n",
        "    'White': 'White',\n",
        "    'Black': 'Black',\n",
        "    'Hispanic': 'Hispanic',\n",
        "    'Asian': 'All Other Races',\n",
        "    'Two or more races': 'All Other Races',\n",
        "    'Other': 'All Other Races',\n",
        "    'Native American': 'All Other Races',\n",
        "    'Middle Eastern': 'All Other Races'\n",
        "}\n",
        "ces24_df['race'] = ces24_df['race'].map(race_map)\n",
        "\n",
        "ces24_df = ces24_df[(ces24_df['gender'] == \"Woman\") | (ces24_df['gender'] == \"Man\")]"
      ],
      "metadata": {
        "id": "Cf73KodddlS-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy and interaction variables for easier logistic model building\n",
        "def create_dummy_interaction(col1, col2):\n",
        "  result = pd.DataFrame()\n",
        "  col1_dummies = pd.get_dummies(col1, drop_first=True)\n",
        "  col2_dummies = pd.get_dummies(col2, drop_first=True)\n",
        "  for col1_val in col1_dummies.columns:\n",
        "    for col2_val in col2_dummies.columns:\n",
        "      result[f'{col1_val}_{col2_val}'] = col1_dummies[col1_val] * col2_dummies[col2_val]\n",
        "  return result"
      ],
      "metadata": {
        "id": "xNfKjwxrd32E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the interaction terms\n",
        "race_gender_interaction_terms = create_dummy_interaction(ces24_df['race'], ces24_df['gender'])\n",
        "race_educ_interaction_terms = create_dummy_interaction(ces24_df['race'], ces24_df['educ'])\n",
        "race_region_interaction_terms = create_dummy_interaction(ces24_df['race'], ces24_df['region'])\n",
        "race_urban_interaction_terms = create_dummy_interaction(ces24_df['race'], ces24_df['urbancity'])\n",
        "\n",
        "# ces24_binary = ces24_df[ces24_df['presvote24'] != 'Other']\n",
        "ces24_dummies = pd.concat([\n",
        "    pd.get_dummies(ces24_df, drop_first=True),\n",
        "    race_gender_interaction_terms,\n",
        "    race_educ_interaction_terms,\n",
        "    race_region_interaction_terms,\n",
        "    race_urban_interaction_terms,\n",
        "], axis=1)\n",
        "\n",
        "# make it binary (remove third party voters for modeling part)\n",
        "ces24_dummies[ces24_dummies['presvote24_Other'] == 0]\n",
        "\n",
        "# drop the presvote24 related columns\n",
        "ces24_dummies = ces24_dummies.drop(columns=['presvote24_Other'])\n",
        "# ces24_dummies = ces24_dummies.drop(columns=['presvote24_Donald Trump (R)'])\n",
        "ces24_dummies = ces24_dummies.drop(columns=['presvote24_Kamala Harris (D)'])\n",
        "# drop birth year\n",
        "ces24_dummies = ces24_dummies.drop(columns=['birthyr'])\n",
        "ces24_dummies.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "DTGgzRuFeDHS",
        "outputId": "4ac8053a-a356-4788-8b6c-78f3ec5cf0af"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   vote_trump  commonweight  commonpostweight  race_Black  race_Hispanic  \\\n",
              "0           0      0.418897          0.180057        True          False   \n",
              "1           0      0.946650          0.700897       False           True   \n",
              "2           0      0.194303          0.046275       False          False   \n",
              "3           0      0.083223          0.008897       False          False   \n",
              "4           0      0.596598          0.555395       False          False   \n",
              "\n",
              "   race_White  age_bracket_25–34  age_bracket_35–44  age_bracket_45–54  \\\n",
              "0       False              False              False               True   \n",
              "1       False              False              False              False   \n",
              "2        True              False              False              False   \n",
              "3        True              False              False              False   \n",
              "4        True              False              False              False   \n",
              "\n",
              "   age_bracket_55–64  ...  Black_Rural area  Black_Other  Hispanic_Suburb  \\\n",
              "0              False  ...             False        False            False   \n",
              "1               True  ...             False        False            False   \n",
              "2              False  ...             False        False            False   \n",
              "3              False  ...             False        False            False   \n",
              "4              False  ...             False        False            False   \n",
              "\n",
              "   Hispanic_Town  Hispanic_Rural area  Hispanic_Other  White_Suburb  \\\n",
              "0          False                False           False         False   \n",
              "1          False                False           False         False   \n",
              "2          False                False           False          True   \n",
              "3          False                False           False          True   \n",
              "4          False                False           False         False   \n",
              "\n",
              "   White_Town  White_Rural area  White_Other  \n",
              "0       False             False        False  \n",
              "1       False             False        False  \n",
              "2       False             False        False  \n",
              "3       False             False        False  \n",
              "4        True             False        False  \n",
              "\n",
              "[5 rows x 141 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9ebaea3-a9a0-49b3-9fbe-3305ab2f8221\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>vote_trump</th>\n",
              "      <th>commonweight</th>\n",
              "      <th>commonpostweight</th>\n",
              "      <th>race_Black</th>\n",
              "      <th>race_Hispanic</th>\n",
              "      <th>race_White</th>\n",
              "      <th>age_bracket_25–34</th>\n",
              "      <th>age_bracket_35–44</th>\n",
              "      <th>age_bracket_45–54</th>\n",
              "      <th>age_bracket_55–64</th>\n",
              "      <th>...</th>\n",
              "      <th>Black_Rural area</th>\n",
              "      <th>Black_Other</th>\n",
              "      <th>Hispanic_Suburb</th>\n",
              "      <th>Hispanic_Town</th>\n",
              "      <th>Hispanic_Rural area</th>\n",
              "      <th>Hispanic_Other</th>\n",
              "      <th>White_Suburb</th>\n",
              "      <th>White_Town</th>\n",
              "      <th>White_Rural area</th>\n",
              "      <th>White_Other</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.418897</td>\n",
              "      <td>0.180057</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0.946650</td>\n",
              "      <td>0.700897</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0.194303</td>\n",
              "      <td>0.046275</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0.083223</td>\n",
              "      <td>0.008897</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0.596598</td>\n",
              "      <td>0.555395</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 141 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9ebaea3-a9a0-49b3-9fbe-3305ab2f8221')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9ebaea3-a9a0-49b3-9fbe-3305ab2f8221 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9ebaea3-a9a0-49b3-9fbe-3305ab2f8221');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4fcad061-397a-4097-bbb1-d8f0aafed661\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4fcad061-397a-4097-bbb1-d8f0aafed661')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4fcad061-397a-4097-bbb1-d8f0aafed661 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ces24_dummies"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the data into training and test datasets"
      ],
      "metadata": {
        "id": "GuNEoss6emYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ces24_dummies.sample(frac=0.25, random_state=1)\n",
        "test_dataset = ces24_dummies.drop(train_dataset.index)"
      ],
      "metadata": {
        "id": "1HPXRrV7euAv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvZyk3TZezhy",
        "outputId": "9d7f6805-d086-4f33-a9b1-36be6577bba0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11409"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_Hy8NTte12n",
        "outputId": "78eb148a-c8b7-47cb-bff0-f6ae870af9c6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34227"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset) + len(test_dataset) == len(ces24_dummies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKFuFp6ce2zG",
        "outputId": "3db0a21d-0ae3-4dd5-82f8-3124d0a085a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(test_dataset), type(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqIw2xtpRoy-",
        "outputId": "a88af17c-6e9f-423d-ae88-5b648a018a8c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(pandas.core.frame.DataFrame, pandas.core.frame.DataFrame)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get features and targets for training and test data\n",
        "X_train, y_train = train_dataset.drop(columns=['vote_trump']).values, train_dataset['vote_trump'].values\n",
        "X_test, y_test = test_dataset.drop(columns=['vote_trump']).values, test_dataset['vote_trump'].values\n",
        "\n",
        "# convert to tensors\n",
        "X_train, y_train = tf.convert_to_tensor(X_train, dtype=tf.float32), tf.convert_to_tensor(y_train, dtype=tf.float32)\n",
        "X_test, y_test = tf.convert_to_tensor(X_test, dtype=tf.float32), tf.convert_to_tensor(y_test, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "ZYrutqPjQRgw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train) # <tensorflow.python.framework.ops.EagerTensor>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi6wCoIrSKGe",
        "outputId": "a2d53d96-3ed0-41d1-ee3f-d94bf65cf7a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reshape (I got this correction from ChatGPT)\n",
        "# X_train = tf.reshape(X_train, (-1, 1))\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftM7FdrRSi0L",
        "outputId": "b64a96be-38f5-4960-99a8-9a86fa8f0deb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([11409, 140])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X_test = tf.reshape(X_test, (-1, 1))\n",
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyVXGkvmSmw4",
        "outputId": "729c1b82-3580-4a2b-ba74-9da170f691d5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([34227, 140])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsOMNiFxSy83",
        "outputId": "89d986d9-25d9-4df4-9e15-8908fc4a9727"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([11409]), TensorShape([34227]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use binary cross entropy loss for logistic ML models.\n"
      ],
      "metadata": {
        "id": "IjfldTE9jJbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# helper function to compute the binary cross entropy loss\n",
        "def loss(y_pred, y):\n",
        "  cross_entropy_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
        "  return tf.reduce_mean(cross_entropy_loss)"
      ],
      "metadata": {
        "id": "29YfSKGRhwoa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the accuracy is the overall percentage of examples that the model classified correctly\n",
        "def accuracy(y_pred, y):\n",
        "  y_pred = tf.math.sigmoid(y_pred)\n",
        "  y_pred_class = tf.cast(y_pred >= 0.5, tf.float32) # classify an example in the positive class if the prob of this is >= 0.5\n",
        "  return tf.reduce_mean(tf.cast(y_pred_class == y, tf.float32))"
      ],
      "metadata": {
        "id": "njIeiRX7jRJg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logistic regression model (using code from TensorFlow documentation)\n",
        "class LogisticRegression(tf.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    \"\"\"\n",
        "    Initialize the logistic regression model class.\n",
        "    \"\"\"\n",
        "    self.built = False\n",
        "\n",
        "  def __call__(self, x, train=True):\n",
        "    if not self.built: # if not pre built, randomly generate the weights and bias\n",
        "      rand_weights = tf.random.normal(shape=[x.shape[-1], 1], seed=24)\n",
        "      rand_bias = tf.random.normal(shape=[], seed=24)\n",
        "      self.W = tf.Variable(rand_weights)\n",
        "      self.b = tf.Variable(0.0)\n",
        "      self.built = True\n",
        "    z = tf.add(tf.matmul(x, self.W), self.b) # z = Xw + b\n",
        "    z = tf.squeeze(z, axis=1) # remove the extra dimension\n",
        "    if train:\n",
        "      return z\n",
        "    return tf.nn.sigmoid(z) # y_pred = sigmoid(z)"
      ],
      "metadata": {
        "id": "MFBvMZX7iDHP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimization"
      ],
      "metadata": {
        "id": "hxcYXztFnhBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try the model out once, untrained\n",
        "model = LogisticRegression()\n",
        "model(X_train)\n",
        "# X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw_hIoz0RGoB",
        "outputId": "8bad1b06-ea75-49fa-b23d-23847fd72859"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(11409,), dtype=float32, numpy=\n",
              "array([-10.79324  ,  -3.2802353,   1.3278469, ...,  -1.2776482,\n",
              "        -0.5430082,   2.6860433], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat GPT interaction to fix indentation error:\n",
        "\n",
        "\n",
        "```\n",
        "fix indentation error       # print(f\"batch size: {batch_size}, learning rate: {learning_rate}, num epochs: {num_epoch}\")\n",
        "      # z = model(train_dataset.drop(columns=['presvote24_Donald Trump (R)']).values)\n",
        "\n",
        "# dictionary to store the different accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# try out the different batch sizes and learning rates\n",
        "for batch_size in batch_sizes:\n",
        "  for learning_rate in learning_rates:\n",
        "    for num_epoch in num_epochs:\n",
        "      model = LogisticRegression()\n",
        "      optimizer = tf.optimizers.SGD(learning_rate)\n",
        "      train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "      train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
        "      epoch_loss = 0\n",
        "      num_batches = 0\n",
        "\n",
        "      # training loop\n",
        "      for epoch in range(num_epoch): # Fixed: range(num_epochs) -> range(num_epoch)\n",
        "        for x_batch, y_batch in train_dataset:\n",
        "          with tf.GradientTape() as tape:\n",
        "            z = model(x_batch)\n",
        "            loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_batch, logits=z)\n",
        "            loss = tf.reduce_mean(loss)\n",
        "          gradients = tape.gradient(loss, [model.W, model.b])\n",
        "          optimizer.apply_gradients(zip(gradients, [model.W, model.b]))\n",
        "          epoch_loss += loss.numpy()\n",
        "          num_batches += 1\n",
        "        print(f\"Epoch {epoch+1}/{num_epoch} loss: {epoch_loss / num_batches:.4f}\")\n",
        "      # train_loss = epoch_loss / num_batches\n",
        "      # train_accuracy = accuracy(model(X_train), y_train)\n",
        "      # test_accuracy = accuracy(model(X_test), y_test)\n",
        "      # report validation accuracy\n",
        "      probs = model(X_test, train=False)\n",
        "      y_preds = tf.cast(probs >= 0.5, tf.float32)\n",
        "      accuracies[(batch_size, learning_rate, num_epoch)] = tf.reduce_mean(tf.cast(y_preds == y_test, tf.float32)).numpy()\n",
        "\n",
        "accuracies\n",
        "\n",
        "\n",
        "ChatGPT said:\n",
        "Here's the cleaned-up version of your code with proper indentation and minor clarifications:\n",
        "\n",
        "python\n",
        "Copy\n",
        "Edit\n",
        "# dictionary to store the different accuracies\n",
        "accuracies = {}\n",
        "\n",
        "# try out the different batch sizes and learning rates\n",
        "for batch_size in batch_sizes:\n",
        "    for learning_rate in learning_rates:\n",
        "        for num_epoch in num_epochs:\n",
        "            model = LogisticRegression()\n",
        "            optimizer = tf.optimizers.SGD(learning_rate)\n",
        "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "            train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
        "            epoch_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            # training loop\n",
        "            for epoch in range(num_epoch):\n",
        "                epoch_loss = 0  # reset loss per epoch\n",
        "                num_batches = 0\n",
        "                for x_batch, y_batch in train_dataset:\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        z = model(x_batch)  # raw logits\n",
        "                        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_batch, logits=z)\n",
        "                        loss = tf.reduce_mean(loss)\n",
        "                    gradients = tape.gradient(loss, [model.W, model.b])\n",
        "                    optimizer.apply_gradients(zip(gradients, [model.W, model.b]))\n",
        "                    epoch_loss += loss.numpy()\n",
        "                    num_batches += 1\n",
        "                print(f\"Epoch {epoch + 1}/{num_epoch} loss: {epoch_loss / num_batches:.4f}\")\n",
        "\n",
        "            # Evaluate accuracy on test set\n",
        "            probs = model(X_test, train=False)  # apply sigmoid\n",
        "            y_preds = tf.cast(probs >= 0.5, tf.float32)\n",
        "            accuracy_val = tf.reduce_mean(tf.cast(y_preds == y_test, tf.float32)).numpy()\n",
        "            accuracies[(batch_size, learning_rate, num_epoch)] = accuracy_val\n",
        "\n",
        "print(accuracies)\n",
        "Notes:\n",
        "Reset epoch_loss and num_batches inside the epoch loop to avoid accumulating losses across epochs.\n",
        "\n",
        "Make sure X_test and y_test are tensors with proper shapes and types.\n",
        "\n",
        "The final print(accuracies) will show validation accuracies for all hyperparameter combos.\n",
        "\n",
        "Let me know if you want me to help add validation splits or save the best model!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "You said:\n",
        "where was the indent error\n",
        "\n",
        "\n",
        "ChatGPT said:\n",
        "The main indentation issues were:\n",
        "\n",
        "epoch_loss and num_batches counters were initialized outside the epoch loop and never reset inside each epoch, causing cumulative loss across epochs.\n",
        "\n",
        "The entire epoch loop body needed consistent indentation — especially the inner loop over batches and the print statement.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Mwd08tUIYDHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try out different numbers of iterations and different learning rates\n",
        "num_epochs = [10, 20, 50, 100, 150]\n",
        "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
        "batch_sizes = [16, 32, 64, 128]\n",
        "optimizers = ['sgd', 'adam'] # should i try out rms prop?"
      ],
      "metadata": {
        "id": "7oHXRV6xlDlS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dictionary to store the different accuracies\n",
        "# i used chat gpt to debug this and fix an indentation error\n",
        "accuracies = {}\n",
        "losses = {}\n",
        "\n",
        "# try out the different batch sizes and learning rates\n",
        "for batch_size in batch_sizes:\n",
        "    for learning_rate in learning_rates:\n",
        "        for num_epoch in num_epochs:\n",
        "            print(f\"batch size: {batch_size}, learning rate: {learning_rate}, num epochs: {num_epoch}\")\n",
        "            model = LogisticRegression()\n",
        "            optimizer = tf.optimizers.SGD(learning_rate)\n",
        "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "            train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
        "            epoch_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            # training loop\n",
        "            for epoch in range(num_epoch):\n",
        "                epoch_loss = 0  # reset loss per epoch\n",
        "                num_batches = 0\n",
        "                for x_batch, y_batch in train_dataset:\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        z = model(x_batch)  # raw logits\n",
        "                        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_batch, logits=z)\n",
        "                        loss = tf.reduce_mean(loss)\n",
        "                    gradients = tape.gradient(loss, [model.W, model.b])\n",
        "                    optimizer.apply_gradients(zip(gradients, [model.W, model.b]))\n",
        "                    epoch_loss += loss.numpy()\n",
        "                    num_batches += 1\n",
        "                print(f\"Epoch {epoch + 1}/{num_epoch} loss: {epoch_loss / num_batches:.4f}\")\n",
        "\n",
        "            # Evaluate accuracy on test set\n",
        "            probs = model(X_test, train=False)  # apply sigmoid\n",
        "            y_preds = tf.cast(probs >= 0.5, tf.float32)\n",
        "            accuracy_val = tf.reduce_mean(tf.cast(y_preds == y_test, tf.float32)).numpy()\n",
        "            accuracies[(batch_size, learning_rate, num_epoch)] = accuracy_val\n",
        "\n",
        "\n",
        "# print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "collapsed": true,
        "id": "tO6uYi0KWVtg",
        "outputId": "02883be1-b0f1-4a85-97d1-f1904345a339"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch size: 16, learning rate: 0.01, num epochs: 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-21-1980471307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_SelectGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1638\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m   \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1640\u001b[0m   return (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros_like\u001b[0;34m(tensor, dtype, name, optimize)\u001b[0m\n\u001b[1;32m   2696\u001b[0m   \"\"\"\n\u001b[0;32m-> 2697\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mzeros_like_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2590\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2591\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2592\u001b[0m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_zeros_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mzeros_like_impl\u001b[0;34m(tensor, dtype, name, optimize, layout)\u001b[0m\n\u001b[1;32m   2802\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"zeros_like\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2803\u001b[0;31m     return array_like_impl(\n\u001b[0m\u001b[1;32m   2804\u001b[0m         \u001b[0mzeros\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36marray_like_impl\u001b[0;34m(array_fn, array_like_fn, tensor, dtype, name, optimize, layout)\u001b[0m\n\u001b[1;32m   2754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2755\u001b[0;31m def array_like_impl(\n\u001b[0m\u001b[1;32m   2756\u001b[0m     \u001b[0marray_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_like_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_code\u001b[0;34m(self, code_obj, result, async_)\u001b[0m\n\u001b[1;32m   3572\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3573\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3574\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3575\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_compiled_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3576\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracies)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kNDSD5NWrFP",
        "outputId": "78640e3d-1ff1-49a6-f8c7-6d98b93a4c5f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{(16, 0.01, 10): np.float32(0.9102755), (16, 0.01, 20): np.float32(0.9172583), (16, 0.01, 50): np.float32(0.92482543), (16, 0.01, 100): np.float32(0.9256143), (16, 0.01, 150): np.float32(0.9263155), (16, 0.001, 10): np.float32(0.6817425), (16, 0.001, 20): np.float32(0.79486954), (16, 0.001, 50): np.float32(0.88725275), (16, 0.001, 100): np.float32(0.91369385), (16, 0.001, 150): np.float32(0.91699535), (16, 0.0001, 10): np.float32(0.46539283), (16, 0.0001, 20): np.float32(0.51360035), (16, 0.0001, 50): np.float32(0.58792764), (16, 0.0001, 100): np.float32(0.6848102), (16, 0.0001, 150): np.float32(0.69792855), (16, 1e-05, 10): np.float32(0.5665118), (16, 1e-05, 20): np.float32(0.47897857), (16, 1e-05, 50): np.float32(0.46784702), (16, 1e-05, 100): np.float32(0.5260759), (16, 1e-05, 150): np.float32(0.5434014), (32, 0.01, 10): np.float32(0.88690215), (32, 0.01, 20): np.float32(0.9155053), (32, 0.01, 50): np.float32(0.92123175), (32, 0.01, 100): np.float32(0.9214655), (32, 0.01, 150): np.float32(0.9244164), (32, 0.001, 10): np.float32(0.62535423), (32, 0.001, 20): np.float32(0.6678061), (32, 0.001, 50): np.float32(0.77859586), (32, 0.001, 100): np.float32(0.87597513), (32, 0.001, 150): np.float32(0.907237), (32, 0.0001, 10): np.float32(0.46770093), (32, 0.0001, 20): np.float32(0.59438455), (32, 0.0001, 50): np.float32(0.6189266), (32, 0.0001, 100): np.float32(0.655652), (32, 0.0001, 150): np.float32(0.68361235), (32, 1e-05, 10): np.float32(0.5488357), (32, 1e-05, 20): np.float32(0.3394104), (32, 1e-05, 50): np.float32(0.48771438), (32, 1e-05, 100): np.float32(0.5072019), (32, 1e-05, 150): np.float32(0.6399626), (64, 0.01, 10): np.float32(0.7921816), (64, 0.01, 20): np.float32(0.8774944), (64, 0.01, 50): np.float32(0.92137784), (64, 0.01, 100): np.float32(0.9186899), (64, 0.01, 150): np.float32(0.9228095), (64, 0.001, 10): np.float32(0.5676805), (64, 0.001, 20): np.float32(0.5921349), (64, 0.001, 50): np.float32(0.6030327), (64, 0.001, 100): np.float32(0.8179507), (64, 0.001, 150): np.float32(0.87565374), (64, 0.0001, 10): np.float32(0.4368481), (64, 0.0001, 20): np.float32(0.5837789), (64, 0.0001, 50): np.float32(0.57998073), (64, 0.0001, 100): np.float32(0.6230169), (64, 0.0001, 150): np.float32(0.65348995), (64, 1e-05, 10): np.float32(0.5211967), (64, 1e-05, 20): np.float32(0.4412014), (64, 1e-05, 50): np.float32(0.56449586), (64, 1e-05, 100): np.float32(0.47214186), (64, 1e-05, 150): np.float32(0.4670874), (128, 0.01, 10): np.float32(0.6924066), (128, 0.01, 20): np.float32(0.8519298), (128, 0.01, 50): np.float32(0.9001081), (128, 0.01, 100): np.float32(0.9182517), (128, 0.01, 150): np.float32(0.92120254), (128, 0.001, 10): np.float32(0.56396997), (128, 0.001, 20): np.float32(0.5105326), (128, 0.001, 50): np.float32(0.66064805), (128, 0.001, 100): np.float32(0.64633185), (128, 0.001, 150): np.float32(0.777807), (128, 0.0001, 10): np.float32(0.41213077), (128, 0.0001, 20): np.float32(0.56841093), (128, 0.0001, 50): np.float32(0.54331374), (128, 0.0001, 100): np.float32(0.66803986), (128, 0.0001, 150): np.float32(0.40137902), (128, 1e-05, 10): np.float32(0.48631197), (128, 1e-05, 20): np.float32(0.58672976), (128, 1e-05, 50): np.float32(0.50235194), (128, 1e-05, 100): np.float32(0.44006193), (128, 1e-05, 150): np.float32(0.547258)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make it a df\n",
        "accuracies_df = pd.DataFrame.from_dict(accuracies, orient='index', columns=['accuracy'])\n",
        "accuracies_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "collapsed": true,
        "id": "545DFOaNYz4h",
        "outputId": "ad1ec12f-388a-442f-8c62-23c04fc40fa2"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   accuracy\n",
              "(16, 0.01, 10)     0.910276\n",
              "(16, 0.01, 20)     0.917258\n",
              "(16, 0.01, 50)     0.924825\n",
              "(16, 0.01, 100)    0.925614\n",
              "(16, 0.01, 150)    0.926315\n",
              "...                     ...\n",
              "(128, 1e-05, 10)   0.486312\n",
              "(128, 1e-05, 20)   0.586730\n",
              "(128, 1e-05, 50)   0.502352\n",
              "(128, 1e-05, 100)  0.440062\n",
              "(128, 1e-05, 150)  0.547258\n",
              "\n",
              "[80 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba3daf23-3a0f-41c8-9960-fc8e6f371f86\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>(16, 0.01, 10)</th>\n",
              "      <td>0.910276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(16, 0.01, 20)</th>\n",
              "      <td>0.917258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(16, 0.01, 50)</th>\n",
              "      <td>0.924825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(16, 0.01, 100)</th>\n",
              "      <td>0.925614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(16, 0.01, 150)</th>\n",
              "      <td>0.926315</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(128, 1e-05, 10)</th>\n",
              "      <td>0.486312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(128, 1e-05, 20)</th>\n",
              "      <td>0.586730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(128, 1e-05, 50)</th>\n",
              "      <td>0.502352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(128, 1e-05, 100)</th>\n",
              "      <td>0.440062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>(128, 1e-05, 150)</th>\n",
              "      <td>0.547258</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>80 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba3daf23-3a0f-41c8-9960-fc8e6f371f86')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba3daf23-3a0f-41c8-9960-fc8e6f371f86 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba3daf23-3a0f-41c8-9960-fc8e6f371f86');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-96a529ba-5575-4766-9651-0c6d7201458b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96a529ba-5575-4766-9651-0c6d7201458b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-96a529ba-5575-4766-9651-0c6d7201458b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_7188c37b-3366-4100-9901-503fc589bf8b\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('accuracies_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7188c37b-3366-4100-9901-503fc589bf8b button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('accuracies_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "accuracies_df",
              "summary": "{\n  \"name\": \"accuracies_df\",\n  \"rows\": 80,\n  \"fields\": [\n    {\n      \"column\": \"accuracy\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 80,\n        \"samples\": [\n          0.46770092844963074,\n          0.9102755188941956,\n          0.921231746673584\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# best accuracy for minibatch so far\n",
        "best_accuracy = max(accuracies, key=accuracies.get)\n",
        "print(f\"Best accuracy before regularization is: {accuracies[best_accuracy]} with batch size: {best_accuracy[0]}, learning rate: {best_accuracy[1]}, num epochs: {best_accuracy[2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LABDVvcLXfKW",
        "outputId": "1c70754e-4309-4f5d-da06-47d11618bfcc"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best accuracy before regularization is: 0.9263154864311218 with batch size: 16, learning rate: 0.01, num epochs: 150\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run this"
      ],
      "metadata": {
        "id": "urc7Agvkd3tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider regularization???\n",
        "lambdas = [] # try out different regularization parameters"
      ],
      "metadata": {
        "id": "SrwMXebkYvsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see if l1 or l2 improves performance\n",
        "# why would l2 be used?\n",
        "# encourages smaller weights, prevents certain features from having too much influence, guards against overfitting"
      ],
      "metadata": {
        "id": "TGB_2aBorjw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying higher learning rates and more epochs\n",
        "num_epochs = [20, 50, 100, 200]\n",
        "learning_rates = [0.1, 0.05, 0.01, 0.001]\n",
        "batch_sizes = [32, 64, 128, 256]\n",
        "validation_accuracies = {}\n",
        "\n",
        "# try out the different combinations (this will take a while to run)\n",
        "for batch_size in batch_sizes:\n",
        "    for learning_rate in learning_rates:\n",
        "        for num_epoch in num_epochs:\n",
        "            print(f\"batch size: {batch_size}, learning rate: {learning_rate}, num epochs: {num_epoch}\")\n",
        "            model = LogisticRegression()\n",
        "            optimizer = tf.optimizers.SGD(learning_rate)\n",
        "            train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "            train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
        "            epoch_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            # training loop\n",
        "            for epoch in range(num_epoch):\n",
        "                epoch_loss = 0  # reset loss per epoch\n",
        "                num_batches = 0\n",
        "                for x_batch, y_batch in train_dataset:\n",
        "                    # forward pass\n",
        "                    with tf.GradientTape() as tape:\n",
        "                        z = model(x_batch)  # raw logits\n",
        "                        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_batch, logits=z)\n",
        "                        loss = tf.reduce_mean(loss)\n",
        "                    # backward pass\n",
        "                    gradients = tape.gradient(loss, [model.W, model.b])\n",
        "                    optimizer.apply_gradients(zip(gradients, [model.W, model.b]))\n",
        "                    epoch_loss += loss.numpy()\n",
        "                    num_batches += 1\n",
        "                print(f\"Epoch {epoch + 1}/{num_epoch} loss: {epoch_loss / num_batches:.4f}\")\n",
        "\n",
        "            # Evaluate accuracy on test set\n",
        "            probs = model(X_test, train=False)  # apply sigmoid\n",
        "            y_preds = tf.cast(probs >= 0.5, tf.float32)\n",
        "            accuracy_val = tf.reduce_mean(tf.cast(y_preds == y_test, tf.float32)).numpy()\n",
        "            accuracies[(batch_size, learning_rate, num_epoch)] = accuracy_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sy9W0iuRVmgS",
        "outputId": "f2a5ff91-8c4b-46fe-809a-9dcf547381c0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 63/200 loss: 0.2256\n",
            "Epoch 64/200 loss: 0.2258\n",
            "Epoch 65/200 loss: 0.2251\n",
            "Epoch 66/200 loss: 0.2253\n",
            "Epoch 67/200 loss: 0.2246\n",
            "Epoch 68/200 loss: 0.2244\n",
            "Epoch 69/200 loss: 0.2247\n",
            "Epoch 70/200 loss: 0.2236\n",
            "Epoch 71/200 loss: 0.2234\n",
            "Epoch 72/200 loss: 0.2234\n",
            "Epoch 73/200 loss: 0.2230\n",
            "Epoch 74/200 loss: 0.2231\n",
            "Epoch 75/200 loss: 0.2228\n",
            "Epoch 76/200 loss: 0.2224\n",
            "Epoch 77/200 loss: 0.2221\n",
            "Epoch 78/200 loss: 0.2220\n",
            "Epoch 79/200 loss: 0.2219\n",
            "Epoch 80/200 loss: 0.2218\n",
            "Epoch 81/200 loss: 0.2213\n",
            "Epoch 82/200 loss: 0.2212\n",
            "Epoch 83/200 loss: 0.2207\n",
            "Epoch 84/200 loss: 0.2205\n",
            "Epoch 85/200 loss: 0.2207\n",
            "Epoch 86/200 loss: 0.2204\n",
            "Epoch 87/200 loss: 0.2201\n",
            "Epoch 88/200 loss: 0.2199\n",
            "Epoch 89/200 loss: 0.2195\n",
            "Epoch 90/200 loss: 0.2201\n",
            "Epoch 91/200 loss: 0.2194\n",
            "Epoch 92/200 loss: 0.2193\n",
            "Epoch 93/200 loss: 0.2190\n",
            "Epoch 94/200 loss: 0.2194\n",
            "Epoch 95/200 loss: 0.2192\n",
            "Epoch 96/200 loss: 0.2187\n",
            "Epoch 97/200 loss: 0.2182\n",
            "Epoch 98/200 loss: 0.2183\n",
            "Epoch 99/200 loss: 0.2179\n",
            "Epoch 100/200 loss: 0.2179\n",
            "Epoch 101/200 loss: 0.2178\n",
            "Epoch 102/200 loss: 0.2175\n",
            "Epoch 103/200 loss: 0.2177\n",
            "Epoch 104/200 loss: 0.2171\n",
            "Epoch 105/200 loss: 0.2171\n",
            "Epoch 106/200 loss: 0.2168\n",
            "Epoch 107/200 loss: 0.2168\n",
            "Epoch 108/200 loss: 0.2166\n",
            "Epoch 109/200 loss: 0.2165\n",
            "Epoch 110/200 loss: 0.2169\n",
            "Epoch 111/200 loss: 0.2162\n",
            "Epoch 112/200 loss: 0.2162\n",
            "Epoch 113/200 loss: 0.2160\n",
            "Epoch 114/200 loss: 0.2160\n",
            "Epoch 115/200 loss: 0.2159\n",
            "Epoch 116/200 loss: 0.2154\n",
            "Epoch 117/200 loss: 0.2158\n",
            "Epoch 118/200 loss: 0.2155\n",
            "Epoch 119/200 loss: 0.2152\n",
            "Epoch 120/200 loss: 0.2150\n",
            "Epoch 121/200 loss: 0.2149\n",
            "Epoch 122/200 loss: 0.2148\n",
            "Epoch 123/200 loss: 0.2152\n",
            "Epoch 124/200 loss: 0.2146\n",
            "Epoch 125/200 loss: 0.2148\n",
            "Epoch 126/200 loss: 0.2144\n",
            "Epoch 127/200 loss: 0.2147\n",
            "Epoch 128/200 loss: 0.2142\n",
            "Epoch 129/200 loss: 0.2145\n",
            "Epoch 130/200 loss: 0.2142\n",
            "Epoch 131/200 loss: 0.2138\n",
            "Epoch 132/200 loss: 0.2137\n",
            "Epoch 133/200 loss: 0.2135\n",
            "Epoch 134/200 loss: 0.2140\n",
            "Epoch 135/200 loss: 0.2136\n",
            "Epoch 136/200 loss: 0.2132\n",
            "Epoch 137/200 loss: 0.2132\n",
            "Epoch 138/200 loss: 0.2131\n",
            "Epoch 139/200 loss: 0.2129\n",
            "Epoch 140/200 loss: 0.2132\n",
            "Epoch 141/200 loss: 0.2128\n",
            "Epoch 142/200 loss: 0.2126\n",
            "Epoch 143/200 loss: 0.2129\n",
            "Epoch 144/200 loss: 0.2125\n",
            "Epoch 145/200 loss: 0.2124\n",
            "Epoch 146/200 loss: 0.2127\n",
            "Epoch 147/200 loss: 0.2124\n",
            "Epoch 148/200 loss: 0.2121\n",
            "Epoch 149/200 loss: 0.2120\n",
            "Epoch 150/200 loss: 0.2119\n",
            "Epoch 151/200 loss: 0.2121\n",
            "Epoch 152/200 loss: 0.2121\n",
            "Epoch 153/200 loss: 0.2118\n",
            "Epoch 154/200 loss: 0.2122\n",
            "Epoch 155/200 loss: 0.2115\n",
            "Epoch 156/200 loss: 0.2114\n",
            "Epoch 157/200 loss: 0.2115\n",
            "Epoch 158/200 loss: 0.2116\n",
            "Epoch 159/200 loss: 0.2115\n",
            "Epoch 160/200 loss: 0.2113\n",
            "Epoch 161/200 loss: 0.2111\n",
            "Epoch 162/200 loss: 0.2113\n",
            "Epoch 163/200 loss: 0.2112\n",
            "Epoch 164/200 loss: 0.2107\n",
            "Epoch 165/200 loss: 0.2111\n",
            "Epoch 166/200 loss: 0.2106\n",
            "Epoch 167/200 loss: 0.2106\n",
            "Epoch 168/200 loss: 0.2104\n",
            "Epoch 169/200 loss: 0.2105\n",
            "Epoch 170/200 loss: 0.2104\n",
            "Epoch 171/200 loss: 0.2108\n",
            "Epoch 172/200 loss: 0.2106\n",
            "Epoch 173/200 loss: 0.2103\n",
            "Epoch 174/200 loss: 0.2100\n",
            "Epoch 175/200 loss: 0.2098\n",
            "Epoch 176/200 loss: 0.2098\n",
            "Epoch 177/200 loss: 0.2102\n",
            "Epoch 178/200 loss: 0.2097\n",
            "Epoch 179/200 loss: 0.2104\n",
            "Epoch 180/200 loss: 0.2099\n",
            "Epoch 181/200 loss: 0.2096\n",
            "Epoch 182/200 loss: 0.2094\n",
            "Epoch 183/200 loss: 0.2094\n",
            "Epoch 184/200 loss: 0.2096\n",
            "Epoch 185/200 loss: 0.2095\n",
            "Epoch 186/200 loss: 0.2093\n",
            "Epoch 187/200 loss: 0.2090\n",
            "Epoch 188/200 loss: 0.2090\n",
            "Epoch 189/200 loss: 0.2090\n",
            "Epoch 190/200 loss: 0.2090\n",
            "Epoch 191/200 loss: 0.2091\n",
            "Epoch 192/200 loss: 0.2089\n",
            "Epoch 193/200 loss: 0.2088\n",
            "Epoch 194/200 loss: 0.2091\n",
            "Epoch 195/200 loss: 0.2088\n",
            "Epoch 196/200 loss: 0.2085\n",
            "Epoch 197/200 loss: 0.2085\n",
            "Epoch 198/200 loss: 0.2084\n",
            "Epoch 199/200 loss: 0.2083\n",
            "Epoch 200/200 loss: 0.2086\n",
            "batch size: 32, learning rate: 0.001, num epochs: 20\n",
            "Epoch 1/20 loss: 1.0190\n",
            "Epoch 2/20 loss: 0.9886\n",
            "Epoch 3/20 loss: 0.9582\n",
            "Epoch 4/20 loss: 0.9305\n",
            "Epoch 5/20 loss: 0.9036\n",
            "Epoch 6/20 loss: 0.8784\n",
            "Epoch 7/20 loss: 0.8539\n",
            "Epoch 8/20 loss: 0.8306\n",
            "Epoch 9/20 loss: 0.8095\n",
            "Epoch 10/20 loss: 0.7883\n",
            "Epoch 11/20 loss: 0.7687\n",
            "Epoch 12/20 loss: 0.7493\n",
            "Epoch 13/20 loss: 0.7322\n",
            "Epoch 14/20 loss: 0.7142\n",
            "Epoch 15/20 loss: 0.6991\n",
            "Epoch 16/20 loss: 0.6836\n",
            "Epoch 17/20 loss: 0.6685\n",
            "Epoch 18/20 loss: 0.6537\n",
            "Epoch 19/20 loss: 0.6399\n",
            "Epoch 20/20 loss: 0.6270\n",
            "batch size: 32, learning rate: 0.001, num epochs: 50\n",
            "Epoch 1/50 loss: 3.6186\n",
            "Epoch 2/50 loss: 2.9563\n",
            "Epoch 3/50 loss: 2.4449\n",
            "Epoch 4/50 loss: 2.0913\n",
            "Epoch 5/50 loss: 1.8633\n",
            "Epoch 6/50 loss: 1.7141\n",
            "Epoch 7/50 loss: 1.6077\n",
            "Epoch 8/50 loss: 1.5255\n",
            "Epoch 9/50 loss: 1.4565\n",
            "Epoch 10/50 loss: 1.3938\n",
            "Epoch 11/50 loss: 1.3358\n",
            "Epoch 12/50 loss: 1.2825\n",
            "Epoch 13/50 loss: 1.2324\n",
            "Epoch 14/50 loss: 1.1839\n",
            "Epoch 15/50 loss: 1.1395\n",
            "Epoch 16/50 loss: 1.0973\n",
            "Epoch 17/50 loss: 1.0573\n",
            "Epoch 18/50 loss: 1.0194\n",
            "Epoch 19/50 loss: 0.9828\n",
            "Epoch 20/50 loss: 0.9496\n",
            "Epoch 21/50 loss: 0.9174\n",
            "Epoch 22/50 loss: 0.8875\n",
            "Epoch 23/50 loss: 0.8586\n",
            "Epoch 24/50 loss: 0.8317\n",
            "Epoch 25/50 loss: 0.8068\n",
            "Epoch 26/50 loss: 0.7828\n",
            "Epoch 27/50 loss: 0.7598\n",
            "Epoch 28/50 loss: 0.7388\n",
            "Epoch 29/50 loss: 0.7181\n",
            "Epoch 30/50 loss: 0.6992\n",
            "Epoch 31/50 loss: 0.6809\n",
            "Epoch 32/50 loss: 0.6632\n",
            "Epoch 33/50 loss: 0.6469\n",
            "Epoch 34/50 loss: 0.6318\n",
            "Epoch 35/50 loss: 0.6167\n",
            "Epoch 36/50 loss: 0.6028\n",
            "Epoch 37/50 loss: 0.5897\n",
            "Epoch 38/50 loss: 0.5773\n",
            "Epoch 39/50 loss: 0.5649\n",
            "Epoch 40/50 loss: 0.5539\n",
            "Epoch 41/50 loss: 0.5430\n",
            "Epoch 42/50 loss: 0.5328\n",
            "Epoch 43/50 loss: 0.5229\n",
            "Epoch 44/50 loss: 0.5137\n",
            "Epoch 45/50 loss: 0.5047\n",
            "Epoch 46/50 loss: 0.4962\n",
            "Epoch 47/50 loss: 0.4878\n",
            "Epoch 48/50 loss: 0.4806\n",
            "Epoch 49/50 loss: 0.4726\n",
            "Epoch 50/50 loss: 0.4656\n",
            "batch size: 32, learning rate: 0.001, num epochs: 100\n",
            "Epoch 1/100 loss: 1.5358\n",
            "Epoch 2/100 loss: 1.4570\n",
            "Epoch 3/100 loss: 1.3899\n",
            "Epoch 4/100 loss: 1.3335\n",
            "Epoch 5/100 loss: 1.2836\n",
            "Epoch 6/100 loss: 1.2391\n",
            "Epoch 7/100 loss: 1.1972\n",
            "Epoch 8/100 loss: 1.1599\n",
            "Epoch 9/100 loss: 1.1250\n",
            "Epoch 10/100 loss: 1.0919\n",
            "Epoch 11/100 loss: 1.0615\n",
            "Epoch 12/100 loss: 1.0319\n",
            "Epoch 13/100 loss: 1.0034\n",
            "Epoch 14/100 loss: 0.9787\n",
            "Epoch 15/100 loss: 0.9538\n",
            "Epoch 16/100 loss: 0.9303\n",
            "Epoch 17/100 loss: 0.9075\n",
            "Epoch 18/100 loss: 0.8870\n",
            "Epoch 19/100 loss: 0.8668\n",
            "Epoch 20/100 loss: 0.8476\n",
            "Epoch 21/100 loss: 0.8282\n",
            "Epoch 22/100 loss: 0.8109\n",
            "Epoch 23/100 loss: 0.7933\n",
            "Epoch 24/100 loss: 0.7776\n",
            "Epoch 25/100 loss: 0.7615\n",
            "Epoch 26/100 loss: 0.7468\n",
            "Epoch 27/100 loss: 0.7320\n",
            "Epoch 28/100 loss: 0.7179\n",
            "Epoch 29/100 loss: 0.7040\n",
            "Epoch 30/100 loss: 0.6914\n",
            "Epoch 31/100 loss: 0.6797\n",
            "Epoch 32/100 loss: 0.6673\n",
            "Epoch 33/100 loss: 0.6554\n",
            "Epoch 34/100 loss: 0.6443\n",
            "Epoch 35/100 loss: 0.6334\n",
            "Epoch 36/100 loss: 0.6231\n",
            "Epoch 37/100 loss: 0.6134\n",
            "Epoch 38/100 loss: 0.6040\n",
            "Epoch 39/100 loss: 0.5943\n",
            "Epoch 40/100 loss: 0.5864\n",
            "Epoch 41/100 loss: 0.5769\n",
            "Epoch 42/100 loss: 0.5691\n",
            "Epoch 43/100 loss: 0.5611\n",
            "Epoch 44/100 loss: 0.5529\n",
            "Epoch 45/100 loss: 0.5452\n",
            "Epoch 46/100 loss: 0.5384\n",
            "Epoch 47/100 loss: 0.5314\n",
            "Epoch 48/100 loss: 0.5249\n",
            "Epoch 49/100 loss: 0.5183\n",
            "Epoch 50/100 loss: 0.5122\n",
            "Epoch 51/100 loss: 0.5056\n",
            "Epoch 52/100 loss: 0.5006\n",
            "Epoch 53/100 loss: 0.4949\n",
            "Epoch 54/100 loss: 0.4895\n",
            "Epoch 55/100 loss: 0.4841\n",
            "Epoch 56/100 loss: 0.4787\n",
            "Epoch 57/100 loss: 0.4743\n",
            "Epoch 58/100 loss: 0.4685\n",
            "Epoch 59/100 loss: 0.4644\n",
            "Epoch 60/100 loss: 0.4600\n",
            "Epoch 61/100 loss: 0.4552\n",
            "Epoch 62/100 loss: 0.4513\n",
            "Epoch 63/100 loss: 0.4466\n",
            "Epoch 64/100 loss: 0.4433\n",
            "Epoch 65/100 loss: 0.4389\n",
            "Epoch 66/100 loss: 0.4355\n",
            "Epoch 67/100 loss: 0.4321\n",
            "Epoch 68/100 loss: 0.4287\n",
            "Epoch 69/100 loss: 0.4249\n",
            "Epoch 70/100 loss: 0.4223\n",
            "Epoch 71/100 loss: 0.4188\n",
            "Epoch 72/100 loss: 0.4160\n",
            "Epoch 73/100 loss: 0.4125\n",
            "Epoch 74/100 loss: 0.4094\n",
            "Epoch 75/100 loss: 0.4066\n",
            "Epoch 76/100 loss: 0.4038\n",
            "Epoch 77/100 loss: 0.4011\n",
            "Epoch 78/100 loss: 0.3988\n",
            "Epoch 79/100 loss: 0.3957\n",
            "Epoch 80/100 loss: 0.3936\n",
            "Epoch 81/100 loss: 0.3909\n",
            "Epoch 82/100 loss: 0.3887\n",
            "Epoch 83/100 loss: 0.3868\n",
            "Epoch 84/100 loss: 0.3843\n",
            "Epoch 85/100 loss: 0.3821\n",
            "Epoch 86/100 loss: 0.3799\n",
            "Epoch 87/100 loss: 0.3784\n",
            "Epoch 88/100 loss: 0.3758\n",
            "Epoch 89/100 loss: 0.3740\n",
            "Epoch 90/100 loss: 0.3721\n",
            "Epoch 91/100 loss: 0.3699\n",
            "Epoch 92/100 loss: 0.3682\n",
            "Epoch 93/100 loss: 0.3662\n",
            "Epoch 94/100 loss: 0.3653\n",
            "Epoch 95/100 loss: 0.3630\n",
            "Epoch 96/100 loss: 0.3626\n",
            "Epoch 97/100 loss: 0.3599\n",
            "Epoch 98/100 loss: 0.3581\n",
            "Epoch 99/100 loss: 0.3567\n",
            "Epoch 100/100 loss: 0.3553\n",
            "batch size: 32, learning rate: 0.001, num epochs: 200\n",
            "Epoch 1/200 loss: 1.7239\n",
            "Epoch 2/200 loss: 1.4688\n",
            "Epoch 3/200 loss: 1.3124\n",
            "Epoch 4/200 loss: 1.2118\n",
            "Epoch 5/200 loss: 1.1418\n",
            "Epoch 6/200 loss: 1.0869\n",
            "Epoch 7/200 loss: 1.0405\n",
            "Epoch 8/200 loss: 0.9989\n",
            "Epoch 9/200 loss: 0.9606\n",
            "Epoch 10/200 loss: 0.9251\n",
            "Epoch 11/200 loss: 0.8917\n",
            "Epoch 12/200 loss: 0.8610\n",
            "Epoch 13/200 loss: 0.8311\n",
            "Epoch 14/200 loss: 0.8043\n",
            "Epoch 15/200 loss: 0.7781\n",
            "Epoch 16/200 loss: 0.7542\n",
            "Epoch 17/200 loss: 0.7312\n",
            "Epoch 18/200 loss: 0.7099\n",
            "Epoch 19/200 loss: 0.6898\n",
            "Epoch 20/200 loss: 0.6703\n",
            "Epoch 21/200 loss: 0.6530\n",
            "Epoch 22/200 loss: 0.6354\n",
            "Epoch 23/200 loss: 0.6199\n",
            "Epoch 24/200 loss: 0.6045\n",
            "Epoch 25/200 loss: 0.5905\n",
            "Epoch 26/200 loss: 0.5772\n",
            "Epoch 27/200 loss: 0.5651\n",
            "Epoch 28/200 loss: 0.5531\n",
            "Epoch 29/200 loss: 0.5414\n",
            "Epoch 30/200 loss: 0.5311\n",
            "Epoch 31/200 loss: 0.5214\n",
            "Epoch 32/200 loss: 0.5114\n",
            "Epoch 33/200 loss: 0.5020\n",
            "Epoch 34/200 loss: 0.4939\n",
            "Epoch 35/200 loss: 0.4862\n",
            "Epoch 36/200 loss: 0.4778\n",
            "Epoch 37/200 loss: 0.4711\n",
            "Epoch 38/200 loss: 0.4635\n",
            "Epoch 39/200 loss: 0.4569\n",
            "Epoch 40/200 loss: 0.4505\n",
            "Epoch 41/200 loss: 0.4442\n",
            "Epoch 42/200 loss: 0.4387\n",
            "Epoch 43/200 loss: 0.4332\n",
            "Epoch 44/200 loss: 0.4283\n",
            "Epoch 45/200 loss: 0.4239\n",
            "Epoch 46/200 loss: 0.4185\n",
            "Epoch 47/200 loss: 0.4139\n",
            "Epoch 48/200 loss: 0.4095\n",
            "Epoch 49/200 loss: 0.4050\n",
            "Epoch 50/200 loss: 0.4012\n",
            "Epoch 51/200 loss: 0.3975\n",
            "Epoch 52/200 loss: 0.3938\n",
            "Epoch 53/200 loss: 0.3902\n",
            "Epoch 54/200 loss: 0.3870\n",
            "Epoch 55/200 loss: 0.3834\n",
            "Epoch 56/200 loss: 0.3805\n",
            "Epoch 57/200 loss: 0.3775\n",
            "Epoch 58/200 loss: 0.3746\n",
            "Epoch 59/200 loss: 0.3715\n",
            "Epoch 60/200 loss: 0.3690\n",
            "Epoch 61/200 loss: 0.3664\n",
            "Epoch 62/200 loss: 0.3637\n",
            "Epoch 63/200 loss: 0.3616\n",
            "Epoch 64/200 loss: 0.3593\n",
            "Epoch 65/200 loss: 0.3570\n",
            "Epoch 66/200 loss: 0.3546\n",
            "Epoch 67/200 loss: 0.3527\n",
            "Epoch 68/200 loss: 0.3504\n",
            "Epoch 69/200 loss: 0.3488\n",
            "Epoch 70/200 loss: 0.3466\n",
            "Epoch 71/200 loss: 0.3448\n",
            "Epoch 72/200 loss: 0.3432\n",
            "Epoch 73/200 loss: 0.3411\n",
            "Epoch 74/200 loss: 0.3395\n",
            "Epoch 75/200 loss: 0.3377\n",
            "Epoch 76/200 loss: 0.3361\n",
            "Epoch 77/200 loss: 0.3346\n",
            "Epoch 78/200 loss: 0.3333\n",
            "Epoch 79/200 loss: 0.3319\n",
            "Epoch 80/200 loss: 0.3302\n",
            "Epoch 81/200 loss: 0.3288\n",
            "Epoch 82/200 loss: 0.3278\n",
            "Epoch 83/200 loss: 0.3262\n",
            "Epoch 84/200 loss: 0.3250\n",
            "Epoch 85/200 loss: 0.3237\n",
            "Epoch 86/200 loss: 0.3229\n",
            "Epoch 87/200 loss: 0.3213\n",
            "Epoch 88/200 loss: 0.3199\n",
            "Epoch 89/200 loss: 0.3192\n",
            "Epoch 90/200 loss: 0.3181\n",
            "Epoch 91/200 loss: 0.3167\n",
            "Epoch 92/200 loss: 0.3159\n",
            "Epoch 93/200 loss: 0.3148\n",
            "Epoch 94/200 loss: 0.3138\n",
            "Epoch 95/200 loss: 0.3130\n",
            "Epoch 96/200 loss: 0.3121\n",
            "Epoch 97/200 loss: 0.3108\n",
            "Epoch 98/200 loss: 0.3100\n",
            "Epoch 99/200 loss: 0.3091\n",
            "Epoch 100/200 loss: 0.3082\n",
            "Epoch 101/200 loss: 0.3074\n",
            "Epoch 102/200 loss: 0.3071\n",
            "Epoch 103/200 loss: 0.3061\n",
            "Epoch 104/200 loss: 0.3051\n",
            "Epoch 105/200 loss: 0.3042\n",
            "Epoch 106/200 loss: 0.3041\n",
            "Epoch 107/200 loss: 0.3029\n",
            "Epoch 108/200 loss: 0.3021\n",
            "Epoch 109/200 loss: 0.3014\n",
            "Epoch 110/200 loss: 0.3005\n",
            "Epoch 111/200 loss: 0.3001\n",
            "Epoch 112/200 loss: 0.2993\n",
            "Epoch 113/200 loss: 0.2985\n",
            "Epoch 114/200 loss: 0.2978\n",
            "Epoch 115/200 loss: 0.2974\n",
            "Epoch 116/200 loss: 0.2971\n",
            "Epoch 117/200 loss: 0.2961\n",
            "Epoch 118/200 loss: 0.2953\n",
            "Epoch 119/200 loss: 0.2952\n",
            "Epoch 120/200 loss: 0.2943\n",
            "Epoch 121/200 loss: 0.2936\n",
            "Epoch 122/200 loss: 0.2932\n",
            "Epoch 123/200 loss: 0.2925\n",
            "Epoch 124/200 loss: 0.2920\n",
            "Epoch 125/200 loss: 0.2920\n",
            "Epoch 126/200 loss: 0.2914\n",
            "Epoch 127/200 loss: 0.2907\n",
            "Epoch 128/200 loss: 0.2901\n",
            "Epoch 129/200 loss: 0.2893\n",
            "Epoch 130/200 loss: 0.2891\n",
            "Epoch 131/200 loss: 0.2886\n",
            "Epoch 132/200 loss: 0.2878\n",
            "Epoch 133/200 loss: 0.2876\n",
            "Epoch 134/200 loss: 0.2872\n",
            "Epoch 135/200 loss: 0.2867\n",
            "Epoch 136/200 loss: 0.2861\n",
            "Epoch 137/200 loss: 0.2860\n",
            "Epoch 138/200 loss: 0.2851\n",
            "Epoch 139/200 loss: 0.2854\n",
            "Epoch 140/200 loss: 0.2846\n",
            "Epoch 141/200 loss: 0.2846\n",
            "Epoch 142/200 loss: 0.2836\n",
            "Epoch 143/200 loss: 0.2831\n",
            "Epoch 144/200 loss: 0.2827\n",
            "Epoch 145/200 loss: 0.2824\n",
            "Epoch 146/200 loss: 0.2819\n",
            "Epoch 147/200 loss: 0.2819\n",
            "Epoch 148/200 loss: 0.2815\n",
            "Epoch 149/200 loss: 0.2813\n",
            "Epoch 150/200 loss: 0.2804\n",
            "Epoch 151/200 loss: 0.2800\n",
            "Epoch 152/200 loss: 0.2798\n",
            "Epoch 153/200 loss: 0.2793\n",
            "Epoch 154/200 loss: 0.2792\n",
            "Epoch 155/200 loss: 0.2787\n",
            "Epoch 156/200 loss: 0.2785\n",
            "Epoch 157/200 loss: 0.2780\n",
            "Epoch 158/200 loss: 0.2782\n",
            "Epoch 159/200 loss: 0.2774\n",
            "Epoch 160/200 loss: 0.2776\n",
            "Epoch 161/200 loss: 0.2769\n",
            "Epoch 162/200 loss: 0.2763\n",
            "Epoch 163/200 loss: 0.2761\n",
            "Epoch 164/200 loss: 0.2758\n",
            "Epoch 165/200 loss: 0.2762\n",
            "Epoch 166/200 loss: 0.2755\n",
            "Epoch 167/200 loss: 0.2750\n",
            "Epoch 168/200 loss: 0.2746\n",
            "Epoch 169/200 loss: 0.2747\n",
            "Epoch 170/200 loss: 0.2744\n",
            "Epoch 171/200 loss: 0.2736\n",
            "Epoch 172/200 loss: 0.2743\n",
            "Epoch 173/200 loss: 0.2731\n",
            "Epoch 174/200 loss: 0.2728\n",
            "Epoch 175/200 loss: 0.2726\n",
            "Epoch 176/200 loss: 0.2723\n",
            "Epoch 177/200 loss: 0.2723\n",
            "Epoch 178/200 loss: 0.2719\n",
            "Epoch 179/200 loss: 0.2717\n",
            "Epoch 180/200 loss: 0.2713\n",
            "Epoch 181/200 loss: 0.2712\n",
            "Epoch 182/200 loss: 0.2706\n",
            "Epoch 183/200 loss: 0.2704\n",
            "Epoch 184/200 loss: 0.2706\n",
            "Epoch 185/200 loss: 0.2701\n",
            "Epoch 186/200 loss: 0.2699\n",
            "Epoch 187/200 loss: 0.2695\n",
            "Epoch 188/200 loss: 0.2697\n",
            "Epoch 189/200 loss: 0.2690\n",
            "Epoch 190/200 loss: 0.2691\n",
            "Epoch 191/200 loss: 0.2687\n",
            "Epoch 192/200 loss: 0.2683\n",
            "Epoch 193/200 loss: 0.2683\n",
            "Epoch 194/200 loss: 0.2678\n",
            "Epoch 195/200 loss: 0.2677\n",
            "Epoch 196/200 loss: 0.2674\n",
            "Epoch 197/200 loss: 0.2671\n",
            "Epoch 198/200 loss: 0.2673\n",
            "Epoch 199/200 loss: 0.2667\n",
            "Epoch 200/200 loss: 0.2670\n",
            "batch size: 64, learning rate: 0.1, num epochs: 20\n",
            "Epoch 1/20 loss: 0.7189\n",
            "Epoch 2/20 loss: 0.3574\n",
            "Epoch 3/20 loss: 0.2955\n",
            "Epoch 4/20 loss: 0.2715\n",
            "Epoch 5/20 loss: 0.2585\n",
            "Epoch 6/20 loss: 0.2505\n",
            "Epoch 7/20 loss: 0.2456\n",
            "Epoch 8/20 loss: 0.2413\n",
            "Epoch 9/20 loss: 0.2384\n",
            "Epoch 10/20 loss: 0.2356\n",
            "Epoch 11/20 loss: 0.2344\n",
            "Epoch 12/20 loss: 0.2311\n",
            "Epoch 13/20 loss: 0.2304\n",
            "Epoch 14/20 loss: 0.2286\n",
            "Epoch 15/20 loss: 0.2270\n",
            "Epoch 16/20 loss: 0.2272\n",
            "Epoch 17/20 loss: 0.2247\n",
            "Epoch 18/20 loss: 0.2236\n",
            "Epoch 19/20 loss: 0.2227\n",
            "Epoch 20/20 loss: 0.2214\n",
            "batch size: 64, learning rate: 0.1, num epochs: 50\n",
            "Epoch 1/50 loss: 0.4192\n",
            "Epoch 2/50 loss: 0.2942\n",
            "Epoch 3/50 loss: 0.2620\n",
            "Epoch 4/50 loss: 0.2490\n",
            "Epoch 5/50 loss: 0.2404\n",
            "Epoch 6/50 loss: 0.2343\n",
            "Epoch 7/50 loss: 0.2310\n",
            "Epoch 8/50 loss: 0.2278\n",
            "Epoch 9/50 loss: 0.2259\n",
            "Epoch 10/50 loss: 0.2243\n",
            "Epoch 11/50 loss: 0.2218\n",
            "Epoch 12/50 loss: 0.2215\n",
            "Epoch 13/50 loss: 0.2191\n",
            "Epoch 14/50 loss: 0.2180\n",
            "Epoch 15/50 loss: 0.2165\n",
            "Epoch 16/50 loss: 0.2177\n",
            "Epoch 17/50 loss: 0.2151\n",
            "Epoch 18/50 loss: 0.2139\n",
            "Epoch 19/50 loss: 0.2150\n",
            "Epoch 20/50 loss: 0.2132\n",
            "Epoch 21/50 loss: 0.2129\n",
            "Epoch 22/50 loss: 0.2113\n",
            "Epoch 23/50 loss: 0.2105\n",
            "Epoch 24/50 loss: 0.2111\n",
            "Epoch 25/50 loss: 0.2126\n",
            "Epoch 26/50 loss: 0.2086\n",
            "Epoch 27/50 loss: 0.2092\n",
            "Epoch 28/50 loss: 0.2086\n",
            "Epoch 29/50 loss: 0.2077\n",
            "Epoch 30/50 loss: 0.2073\n",
            "Epoch 31/50 loss: 0.2072\n",
            "Epoch 32/50 loss: 0.2066\n",
            "Epoch 33/50 loss: 0.2059\n",
            "Epoch 34/50 loss: 0.2060\n",
            "Epoch 35/50 loss: 0.2063\n",
            "Epoch 36/50 loss: 0.2051\n",
            "Epoch 37/50 loss: 0.2050\n",
            "Epoch 38/50 loss: 0.2041\n",
            "Epoch 39/50 loss: 0.2045\n",
            "Epoch 40/50 loss: 0.2051\n",
            "Epoch 41/50 loss: 0.2038\n",
            "Epoch 42/50 loss: 0.2043\n",
            "Epoch 43/50 loss: 0.2041\n",
            "Epoch 44/50 loss: 0.2030\n",
            "Epoch 45/50 loss: 0.2038\n",
            "Epoch 46/50 loss: 0.2026\n",
            "Epoch 47/50 loss: 0.2029\n",
            "Epoch 48/50 loss: 0.2034\n",
            "Epoch 49/50 loss: 0.2016\n",
            "Epoch 50/50 loss: 0.2015\n",
            "batch size: 64, learning rate: 0.1, num epochs: 100\n",
            "Epoch 1/100 loss: 0.4947\n",
            "Epoch 2/100 loss: 0.3135\n",
            "Epoch 3/100 loss: 0.2792\n",
            "Epoch 4/100 loss: 0.2637\n",
            "Epoch 5/100 loss: 0.2545\n",
            "Epoch 6/100 loss: 0.2489\n",
            "Epoch 7/100 loss: 0.2422\n",
            "Epoch 8/100 loss: 0.2394\n",
            "Epoch 9/100 loss: 0.2368\n",
            "Epoch 10/100 loss: 0.2336\n",
            "Epoch 11/100 loss: 0.2320\n",
            "Epoch 12/100 loss: 0.2290\n",
            "Epoch 13/100 loss: 0.2265\n",
            "Epoch 14/100 loss: 0.2248\n",
            "Epoch 15/100 loss: 0.2237\n",
            "Epoch 16/100 loss: 0.2210\n",
            "Epoch 17/100 loss: 0.2208\n",
            "Epoch 18/100 loss: 0.2186\n",
            "Epoch 19/100 loss: 0.2182\n",
            "Epoch 20/100 loss: 0.2166\n",
            "Epoch 21/100 loss: 0.2157\n",
            "Epoch 22/100 loss: 0.2157\n",
            "Epoch 23/100 loss: 0.2139\n",
            "Epoch 24/100 loss: 0.2130\n",
            "Epoch 25/100 loss: 0.2128\n",
            "Epoch 26/100 loss: 0.2120\n",
            "Epoch 27/100 loss: 0.2125\n",
            "Epoch 28/100 loss: 0.2105\n",
            "Epoch 29/100 loss: 0.2107\n",
            "Epoch 30/100 loss: 0.2094\n",
            "Epoch 31/100 loss: 0.2091\n",
            "Epoch 32/100 loss: 0.2084\n",
            "Epoch 33/100 loss: 0.2080\n",
            "Epoch 34/100 loss: 0.2087\n",
            "Epoch 35/100 loss: 0.2071\n",
            "Epoch 36/100 loss: 0.2067\n",
            "Epoch 37/100 loss: 0.2079\n",
            "Epoch 38/100 loss: 0.2067\n",
            "Epoch 39/100 loss: 0.2065\n",
            "Epoch 40/100 loss: 0.2055\n",
            "Epoch 41/100 loss: 0.2051\n",
            "Epoch 42/100 loss: 0.2060\n",
            "Epoch 43/100 loss: 0.2046\n",
            "Epoch 44/100 loss: 0.2047\n",
            "Epoch 45/100 loss: 0.2041\n",
            "Epoch 46/100 loss: 0.2040\n",
            "Epoch 47/100 loss: 0.2039\n",
            "Epoch 48/100 loss: 0.2051\n",
            "Epoch 49/100 loss: 0.2032\n",
            "Epoch 50/100 loss: 0.2028\n",
            "Epoch 51/100 loss: 0.2025\n",
            "Epoch 52/100 loss: 0.2029\n",
            "Epoch 53/100 loss: 0.2026\n",
            "Epoch 54/100 loss: 0.2019\n",
            "Epoch 55/100 loss: 0.2022\n",
            "Epoch 56/100 loss: 0.2023\n",
            "Epoch 57/100 loss: 0.2019\n",
            "Epoch 58/100 loss: 0.2011\n",
            "Epoch 59/100 loss: 0.2009\n",
            "Epoch 60/100 loss: 0.2010\n",
            "Epoch 61/100 loss: 0.2011\n",
            "Epoch 62/100 loss: 0.2009\n",
            "Epoch 63/100 loss: 0.2001\n",
            "Epoch 64/100 loss: 0.2006\n",
            "Epoch 65/100 loss: 0.2005\n",
            "Epoch 66/100 loss: 0.2003\n",
            "Epoch 67/100 loss: 0.2004\n",
            "Epoch 68/100 loss: 0.2001\n",
            "Epoch 69/100 loss: 0.2002\n",
            "Epoch 70/100 loss: 0.1994\n",
            "Epoch 71/100 loss: 0.2011\n",
            "Epoch 72/100 loss: 0.1996\n",
            "Epoch 73/100 loss: 0.2012\n",
            "Epoch 74/100 loss: 0.1988\n",
            "Epoch 75/100 loss: 0.1991\n",
            "Epoch 76/100 loss: 0.1986\n",
            "Epoch 77/100 loss: 0.1993\n",
            "Epoch 78/100 loss: 0.1992\n",
            "Epoch 79/100 loss: 0.1984\n",
            "Epoch 80/100 loss: 0.1992\n",
            "Epoch 81/100 loss: 0.1989\n",
            "Epoch 82/100 loss: 0.1984\n",
            "Epoch 83/100 loss: 0.1990\n",
            "Epoch 84/100 loss: 0.1985\n",
            "Epoch 85/100 loss: 0.1980\n",
            "Epoch 86/100 loss: 0.1979\n",
            "Epoch 87/100 loss: 0.1976\n",
            "Epoch 88/100 loss: 0.1979\n",
            "Epoch 89/100 loss: 0.1977\n",
            "Epoch 90/100 loss: 0.1986\n",
            "Epoch 91/100 loss: 0.1986\n",
            "Epoch 92/100 loss: 0.1981\n",
            "Epoch 93/100 loss: 0.1990\n",
            "Epoch 94/100 loss: 0.1986\n",
            "Epoch 95/100 loss: 0.1975\n",
            "Epoch 96/100 loss: 0.1983\n",
            "Epoch 97/100 loss: 0.1974\n",
            "Epoch 98/100 loss: 0.1980\n",
            "Epoch 99/100 loss: 0.1973\n",
            "Epoch 100/100 loss: 0.1974\n",
            "batch size: 64, learning rate: 0.1, num epochs: 200\n",
            "Epoch 1/200 loss: 1.0765\n",
            "Epoch 2/200 loss: 0.4421\n",
            "Epoch 3/200 loss: 0.3169\n",
            "Epoch 4/200 loss: 0.2765\n",
            "Epoch 5/200 loss: 0.2589\n",
            "Epoch 6/200 loss: 0.2478\n",
            "Epoch 7/200 loss: 0.2427\n",
            "Epoch 8/200 loss: 0.2385\n",
            "Epoch 9/200 loss: 0.2358\n",
            "Epoch 10/200 loss: 0.2330\n",
            "Epoch 11/200 loss: 0.2307\n",
            "Epoch 12/200 loss: 0.2281\n",
            "Epoch 13/200 loss: 0.2268\n",
            "Epoch 14/200 loss: 0.2254\n",
            "Epoch 15/200 loss: 0.2233\n",
            "Epoch 16/200 loss: 0.2225\n",
            "Epoch 17/200 loss: 0.2211\n",
            "Epoch 18/200 loss: 0.2217\n",
            "Epoch 19/200 loss: 0.2190\n",
            "Epoch 20/200 loss: 0.2184\n",
            "Epoch 21/200 loss: 0.2177\n",
            "Epoch 22/200 loss: 0.2185\n",
            "Epoch 23/200 loss: 0.2158\n",
            "Epoch 24/200 loss: 0.2149\n",
            "Epoch 25/200 loss: 0.2150\n",
            "Epoch 26/200 loss: 0.2136\n",
            "Epoch 27/200 loss: 0.2126\n",
            "Epoch 28/200 loss: 0.2132\n",
            "Epoch 29/200 loss: 0.2125\n",
            "Epoch 30/200 loss: 0.2113\n",
            "Epoch 31/200 loss: 0.2107\n",
            "Epoch 32/200 loss: 0.2103\n",
            "Epoch 33/200 loss: 0.2097\n",
            "Epoch 34/200 loss: 0.2097\n",
            "Epoch 35/200 loss: 0.2089\n",
            "Epoch 36/200 loss: 0.2088\n",
            "Epoch 37/200 loss: 0.2084\n",
            "Epoch 38/200 loss: 0.2078\n",
            "Epoch 39/200 loss: 0.2075\n",
            "Epoch 40/200 loss: 0.2088\n",
            "Epoch 41/200 loss: 0.2069\n",
            "Epoch 42/200 loss: 0.2068\n",
            "Epoch 43/200 loss: 0.2069\n",
            "Epoch 44/200 loss: 0.2057\n",
            "Epoch 45/200 loss: 0.2059\n",
            "Epoch 46/200 loss: 0.2064\n",
            "Epoch 47/200 loss: 0.2056\n",
            "Epoch 48/200 loss: 0.2049\n",
            "Epoch 49/200 loss: 0.2060\n",
            "Epoch 50/200 loss: 0.2062\n",
            "Epoch 51/200 loss: 0.2045\n",
            "Epoch 52/200 loss: 0.2041\n",
            "Epoch 53/200 loss: 0.2040\n",
            "Epoch 54/200 loss: 0.2035\n",
            "Epoch 55/200 loss: 0.2033\n",
            "Epoch 56/200 loss: 0.2036\n",
            "Epoch 57/200 loss: 0.2026\n",
            "Epoch 58/200 loss: 0.2025\n",
            "Epoch 59/200 loss: 0.2027\n",
            "Epoch 60/200 loss: 0.2022\n",
            "Epoch 61/200 loss: 0.2026\n",
            "Epoch 62/200 loss: 0.2034\n",
            "Epoch 63/200 loss: 0.2033\n",
            "Epoch 64/200 loss: 0.2050\n",
            "Epoch 65/200 loss: 0.2019\n",
            "Epoch 66/200 loss: 0.2012\n",
            "Epoch 67/200 loss: 0.2019\n",
            "Epoch 68/200 loss: 0.2014\n",
            "Epoch 69/200 loss: 0.2015\n",
            "Epoch 70/200 loss: 0.2017\n",
            "Epoch 71/200 loss: 0.2016\n",
            "Epoch 72/200 loss: 0.2006\n",
            "Epoch 73/200 loss: 0.2011\n",
            "Epoch 74/200 loss: 0.2016\n",
            "Epoch 75/200 loss: 0.1999\n",
            "Epoch 76/200 loss: 0.2005\n",
            "Epoch 77/200 loss: 0.1997\n",
            "Epoch 78/200 loss: 0.2005\n",
            "Epoch 79/200 loss: 0.1998\n",
            "Epoch 80/200 loss: 0.1999\n",
            "Epoch 81/200 loss: 0.1999\n",
            "Epoch 82/200 loss: 0.1990\n",
            "Epoch 83/200 loss: 0.1989\n",
            "Epoch 84/200 loss: 0.1997\n",
            "Epoch 85/200 loss: 0.1997\n",
            "Epoch 86/200 loss: 0.2005\n",
            "Epoch 87/200 loss: 0.1993\n",
            "Epoch 88/200 loss: 0.1993\n",
            "Epoch 89/200 loss: 0.1987\n",
            "Epoch 90/200 loss: 0.1989\n",
            "Epoch 91/200 loss: 0.1980\n",
            "Epoch 92/200 loss: 0.1994\n",
            "Epoch 93/200 loss: 0.1985\n",
            "Epoch 94/200 loss: 0.1978\n",
            "Epoch 95/200 loss: 0.1979\n",
            "Epoch 96/200 loss: 0.1996\n",
            "Epoch 97/200 loss: 0.1985\n",
            "Epoch 98/200 loss: 0.1981\n",
            "Epoch 99/200 loss: 0.1980\n",
            "Epoch 100/200 loss: 0.1982\n",
            "Epoch 101/200 loss: 0.1974\n",
            "Epoch 102/200 loss: 0.1976\n",
            "Epoch 103/200 loss: 0.1980\n",
            "Epoch 104/200 loss: 0.1973\n",
            "Epoch 105/200 loss: 0.1977\n",
            "Epoch 106/200 loss: 0.1975\n",
            "Epoch 107/200 loss: 0.1971\n",
            "Epoch 108/200 loss: 0.1971\n",
            "Epoch 109/200 loss: 0.1981\n",
            "Epoch 110/200 loss: 0.1968\n",
            "Epoch 111/200 loss: 0.1974\n",
            "Epoch 112/200 loss: 0.1971\n",
            "Epoch 113/200 loss: 0.1977\n",
            "Epoch 114/200 loss: 0.1964\n",
            "Epoch 115/200 loss: 0.1984\n",
            "Epoch 116/200 loss: 0.1976\n",
            "Epoch 117/200 loss: 0.1961\n",
            "Epoch 118/200 loss: 0.1965\n",
            "Epoch 119/200 loss: 0.1971\n",
            "Epoch 120/200 loss: 0.1966\n",
            "Epoch 121/200 loss: 0.1961\n",
            "Epoch 122/200 loss: 0.1966\n",
            "Epoch 123/200 loss: 0.1967\n",
            "Epoch 124/200 loss: 0.1957\n",
            "Epoch 125/200 loss: 0.1962\n",
            "Epoch 126/200 loss: 0.1972\n",
            "Epoch 127/200 loss: 0.1962\n",
            "Epoch 128/200 loss: 0.1964\n",
            "Epoch 129/200 loss: 0.1959\n",
            "Epoch 130/200 loss: 0.1973\n",
            "Epoch 131/200 loss: 0.1966\n",
            "Epoch 132/200 loss: 0.1969\n",
            "Epoch 133/200 loss: 0.1959\n",
            "Epoch 134/200 loss: 0.1956\n",
            "Epoch 135/200 loss: 0.1956\n",
            "Epoch 136/200 loss: 0.1953\n",
            "Epoch 137/200 loss: 0.1964\n",
            "Epoch 138/200 loss: 0.1960\n",
            "Epoch 139/200 loss: 0.1965\n",
            "Epoch 140/200 loss: 0.1962\n",
            "Epoch 141/200 loss: 0.1954\n",
            "Epoch 142/200 loss: 0.1961\n",
            "Epoch 143/200 loss: 0.1952\n",
            "Epoch 144/200 loss: 0.1964\n",
            "Epoch 145/200 loss: 0.1968\n",
            "Epoch 146/200 loss: 0.1964\n",
            "Epoch 147/200 loss: 0.1955\n",
            "Epoch 148/200 loss: 0.1954\n",
            "Epoch 149/200 loss: 0.1950\n",
            "Epoch 150/200 loss: 0.1954\n",
            "Epoch 151/200 loss: 0.1959\n",
            "Epoch 152/200 loss: 0.1950\n",
            "Epoch 153/200 loss: 0.1954\n",
            "Epoch 154/200 loss: 0.1956\n",
            "Epoch 155/200 loss: 0.1954\n",
            "Epoch 156/200 loss: 0.1950\n",
            "Epoch 157/200 loss: 0.1953\n",
            "Epoch 158/200 loss: 0.1959\n",
            "Epoch 159/200 loss: 0.1952\n",
            "Epoch 160/200 loss: 0.1946\n",
            "Epoch 161/200 loss: 0.1950\n",
            "Epoch 162/200 loss: 0.1951\n",
            "Epoch 163/200 loss: 0.1954\n",
            "Epoch 164/200 loss: 0.1943\n",
            "Epoch 165/200 loss: 0.1948\n",
            "Epoch 166/200 loss: 0.1950\n",
            "Epoch 167/200 loss: 0.1952\n",
            "Epoch 168/200 loss: 0.1946\n",
            "Epoch 169/200 loss: 0.1947\n",
            "Epoch 170/200 loss: 0.1944\n",
            "Epoch 171/200 loss: 0.1956\n",
            "Epoch 172/200 loss: 0.1945\n",
            "Epoch 173/200 loss: 0.1954\n",
            "Epoch 174/200 loss: 0.1942\n",
            "Epoch 175/200 loss: 0.1940\n",
            "Epoch 176/200 loss: 0.1944\n",
            "Epoch 177/200 loss: 0.1946\n",
            "Epoch 178/200 loss: 0.1946\n",
            "Epoch 179/200 loss: 0.1940\n",
            "Epoch 180/200 loss: 0.1944\n",
            "Epoch 181/200 loss: 0.1943\n",
            "Epoch 182/200 loss: 0.1949\n",
            "Epoch 183/200 loss: 0.1953\n",
            "Epoch 184/200 loss: 0.1949\n",
            "Epoch 185/200 loss: 0.1958\n",
            "Epoch 186/200 loss: 0.1942\n",
            "Epoch 187/200 loss: 0.1943\n",
            "Epoch 188/200 loss: 0.1942\n",
            "Epoch 189/200 loss: 0.1944\n",
            "Epoch 190/200 loss: 0.1953\n",
            "Epoch 191/200 loss: 0.1942\n",
            "Epoch 192/200 loss: 0.1941\n",
            "Epoch 193/200 loss: 0.1947\n",
            "Epoch 194/200 loss: 0.1940\n",
            "Epoch 195/200 loss: 0.1945\n",
            "Epoch 196/200 loss: 0.1937\n",
            "Epoch 197/200 loss: 0.1940\n",
            "Epoch 198/200 loss: 0.1942\n",
            "Epoch 199/200 loss: 0.1938\n",
            "Epoch 200/200 loss: 0.1936\n",
            "batch size: 64, learning rate: 0.05, num epochs: 20\n",
            "Epoch 1/20 loss: 1.4536\n",
            "Epoch 2/20 loss: 0.6495\n",
            "Epoch 3/20 loss: 0.4293\n",
            "Epoch 4/20 loss: 0.3487\n",
            "Epoch 5/20 loss: 0.3114\n",
            "Epoch 6/20 loss: 0.2898\n",
            "Epoch 7/20 loss: 0.2771\n",
            "Epoch 8/20 loss: 0.2678\n",
            "Epoch 9/20 loss: 0.2616\n",
            "Epoch 10/20 loss: 0.2564\n",
            "Epoch 11/20 loss: 0.2528\n",
            "Epoch 12/20 loss: 0.2493\n",
            "Epoch 13/20 loss: 0.2472\n",
            "Epoch 14/20 loss: 0.2455\n",
            "Epoch 15/20 loss: 0.2450\n",
            "Epoch 16/20 loss: 0.2417\n",
            "Epoch 17/20 loss: 0.2396\n",
            "Epoch 18/20 loss: 0.2382\n",
            "Epoch 19/20 loss: 0.2372\n",
            "Epoch 20/20 loss: 0.2363\n",
            "batch size: 64, learning rate: 0.05, num epochs: 50\n",
            "Epoch 1/50 loss: 0.8368\n",
            "Epoch 2/50 loss: 0.5094\n",
            "Epoch 3/50 loss: 0.3939\n",
            "Epoch 4/50 loss: 0.3406\n",
            "Epoch 5/50 loss: 0.3121\n",
            "Epoch 6/50 loss: 0.2938\n",
            "Epoch 7/50 loss: 0.2817\n",
            "Epoch 8/50 loss: 0.2742\n",
            "Epoch 9/50 loss: 0.2671\n",
            "Epoch 10/50 loss: 0.2624\n",
            "Epoch 11/50 loss: 0.2586\n",
            "Epoch 12/50 loss: 0.2547\n",
            "Epoch 13/50 loss: 0.2509\n",
            "Epoch 14/50 loss: 0.2505\n",
            "Epoch 15/50 loss: 0.2459\n",
            "Epoch 16/50 loss: 0.2444\n",
            "Epoch 17/50 loss: 0.2422\n",
            "Epoch 18/50 loss: 0.2403\n",
            "Epoch 19/50 loss: 0.2385\n",
            "Epoch 20/50 loss: 0.2372\n",
            "Epoch 21/50 loss: 0.2363\n",
            "Epoch 22/50 loss: 0.2349\n",
            "Epoch 23/50 loss: 0.2333\n",
            "Epoch 24/50 loss: 0.2331\n",
            "Epoch 25/50 loss: 0.2337\n",
            "Epoch 26/50 loss: 0.2310\n",
            "Epoch 27/50 loss: 0.2292\n",
            "Epoch 28/50 loss: 0.2288\n",
            "Epoch 29/50 loss: 0.2277\n",
            "Epoch 30/50 loss: 0.2273\n",
            "Epoch 31/50 loss: 0.2260\n",
            "Epoch 32/50 loss: 0.2261\n",
            "Epoch 33/50 loss: 0.2250\n",
            "Epoch 34/50 loss: 0.2238\n",
            "Epoch 35/50 loss: 0.2230\n",
            "Epoch 36/50 loss: 0.2224\n",
            "Epoch 37/50 loss: 0.2228\n",
            "Epoch 38/50 loss: 0.2228\n",
            "Epoch 39/50 loss: 0.2220\n",
            "Epoch 40/50 loss: 0.2206\n",
            "Epoch 41/50 loss: 0.2214\n",
            "Epoch 42/50 loss: 0.2195\n",
            "Epoch 43/50 loss: 0.2196\n",
            "Epoch 44/50 loss: 0.2222\n",
            "Epoch 45/50 loss: 0.2187\n",
            "Epoch 46/50 loss: 0.2182\n",
            "Epoch 47/50 loss: 0.2181\n",
            "Epoch 48/50 loss: 0.2166\n",
            "Epoch 49/50 loss: 0.2171\n",
            "Epoch 50/50 loss: 0.2174\n",
            "batch size: 64, learning rate: 0.05, num epochs: 100\n",
            "Epoch 1/100 loss: 1.0533\n",
            "Epoch 2/100 loss: 0.5874\n",
            "Epoch 3/100 loss: 0.4310\n",
            "Epoch 4/100 loss: 0.3627\n",
            "Epoch 5/100 loss: 0.3252\n",
            "Epoch 6/100 loss: 0.3030\n",
            "Epoch 7/100 loss: 0.2886\n",
            "Epoch 8/100 loss: 0.2793\n",
            "Epoch 9/100 loss: 0.2702\n",
            "Epoch 10/100 loss: 0.2646\n",
            "Epoch 11/100 loss: 0.2593\n",
            "Epoch 12/100 loss: 0.2553\n",
            "Epoch 13/100 loss: 0.2534\n",
            "Epoch 14/100 loss: 0.2488\n",
            "Epoch 15/100 loss: 0.2464\n",
            "Epoch 16/100 loss: 0.2445\n",
            "Epoch 17/100 loss: 0.2441\n",
            "Epoch 18/100 loss: 0.2410\n",
            "Epoch 19/100 loss: 0.2386\n",
            "Epoch 20/100 loss: 0.2376\n",
            "Epoch 21/100 loss: 0.2362\n",
            "Epoch 22/100 loss: 0.2351\n",
            "Epoch 23/100 loss: 0.2330\n",
            "Epoch 24/100 loss: 0.2326\n",
            "Epoch 25/100 loss: 0.2308\n",
            "Epoch 26/100 loss: 0.2299\n",
            "Epoch 27/100 loss: 0.2291\n",
            "Epoch 28/100 loss: 0.2287\n",
            "Epoch 29/100 loss: 0.2278\n",
            "Epoch 30/100 loss: 0.2262\n",
            "Epoch 31/100 loss: 0.2252\n",
            "Epoch 32/100 loss: 0.2268\n",
            "Epoch 33/100 loss: 0.2246\n",
            "Epoch 34/100 loss: 0.2241\n",
            "Epoch 35/100 loss: 0.2236\n",
            "Epoch 36/100 loss: 0.2227\n",
            "Epoch 37/100 loss: 0.2213\n",
            "Epoch 38/100 loss: 0.2209\n",
            "Epoch 39/100 loss: 0.2198\n",
            "Epoch 40/100 loss: 0.2206\n",
            "Epoch 41/100 loss: 0.2197\n",
            "Epoch 42/100 loss: 0.2189\n",
            "Epoch 43/100 loss: 0.2185\n",
            "Epoch 44/100 loss: 0.2181\n",
            "Epoch 45/100 loss: 0.2173\n",
            "Epoch 46/100 loss: 0.2188\n",
            "Epoch 47/100 loss: 0.2168\n",
            "Epoch 48/100 loss: 0.2157\n",
            "Epoch 49/100 loss: 0.2153\n",
            "Epoch 50/100 loss: 0.2152\n",
            "Epoch 51/100 loss: 0.2148\n",
            "Epoch 52/100 loss: 0.2146\n",
            "Epoch 53/100 loss: 0.2144\n",
            "Epoch 54/100 loss: 0.2149\n",
            "Epoch 55/100 loss: 0.2127\n",
            "Epoch 56/100 loss: 0.2125\n",
            "Epoch 57/100 loss: 0.2120\n",
            "Epoch 58/100 loss: 0.2120\n",
            "Epoch 59/100 loss: 0.2117\n",
            "Epoch 60/100 loss: 0.2113\n",
            "Epoch 61/100 loss: 0.2115\n",
            "Epoch 62/100 loss: 0.2114\n",
            "Epoch 63/100 loss: 0.2102\n",
            "Epoch 64/100 loss: 0.2100\n",
            "Epoch 65/100 loss: 0.2110\n",
            "Epoch 66/100 loss: 0.2097\n",
            "Epoch 67/100 loss: 0.2110\n",
            "Epoch 68/100 loss: 0.2092\n",
            "Epoch 69/100 loss: 0.2096\n",
            "Epoch 70/100 loss: 0.2087\n",
            "Epoch 71/100 loss: 0.2088\n",
            "Epoch 72/100 loss: 0.2080\n",
            "Epoch 73/100 loss: 0.2081\n",
            "Epoch 74/100 loss: 0.2076\n",
            "Epoch 75/100 loss: 0.2081\n",
            "Epoch 76/100 loss: 0.2080\n",
            "Epoch 77/100 loss: 0.2079\n",
            "Epoch 78/100 loss: 0.2077\n",
            "Epoch 79/100 loss: 0.2086\n",
            "Epoch 80/100 loss: 0.2067\n",
            "Epoch 81/100 loss: 0.2062\n",
            "Epoch 82/100 loss: 0.2068\n",
            "Epoch 83/100 loss: 0.2074\n",
            "Epoch 84/100 loss: 0.2056\n",
            "Epoch 85/100 loss: 0.2067\n",
            "Epoch 86/100 loss: 0.2056\n",
            "Epoch 87/100 loss: 0.2062\n",
            "Epoch 88/100 loss: 0.2066\n",
            "Epoch 89/100 loss: 0.2055\n",
            "Epoch 90/100 loss: 0.2060\n",
            "Epoch 91/100 loss: 0.2056\n",
            "Epoch 92/100 loss: 0.2050\n",
            "Epoch 93/100 loss: 0.2044\n",
            "Epoch 94/100 loss: 0.2042\n",
            "Epoch 95/100 loss: 0.2043\n",
            "Epoch 96/100 loss: 0.2041\n",
            "Epoch 97/100 loss: 0.2039\n",
            "Epoch 98/100 loss: 0.2061\n",
            "Epoch 99/100 loss: 0.2043\n",
            "Epoch 100/100 loss: 0.2036\n",
            "batch size: 64, learning rate: 0.05, num epochs: 200\n",
            "Epoch 1/200 loss: 1.0060\n",
            "Epoch 2/200 loss: 0.4638\n",
            "Epoch 3/200 loss: 0.3424\n",
            "Epoch 4/200 loss: 0.3004\n",
            "Epoch 5/200 loss: 0.2791\n",
            "Epoch 6/200 loss: 0.2680\n",
            "Epoch 7/200 loss: 0.2620\n",
            "Epoch 8/200 loss: 0.2555\n",
            "Epoch 9/200 loss: 0.2523\n",
            "Epoch 10/200 loss: 0.2486\n",
            "Epoch 11/200 loss: 0.2451\n",
            "Epoch 12/200 loss: 0.2450\n",
            "Epoch 13/200 loss: 0.2424\n",
            "Epoch 14/200 loss: 0.2392\n",
            "Epoch 15/200 loss: 0.2379\n",
            "Epoch 16/200 loss: 0.2359\n",
            "Epoch 17/200 loss: 0.2349\n",
            "Epoch 18/200 loss: 0.2339\n",
            "Epoch 19/200 loss: 0.2328\n",
            "Epoch 20/200 loss: 0.2312\n",
            "Epoch 21/200 loss: 0.2303\n",
            "Epoch 22/200 loss: 0.2290\n",
            "Epoch 23/200 loss: 0.2290\n",
            "Epoch 24/200 loss: 0.2288\n",
            "Epoch 25/200 loss: 0.2267\n",
            "Epoch 26/200 loss: 0.2262\n",
            "Epoch 27/200 loss: 0.2256\n",
            "Epoch 28/200 loss: 0.2246\n",
            "Epoch 29/200 loss: 0.2254\n",
            "Epoch 30/200 loss: 0.2227\n",
            "Epoch 31/200 loss: 0.2226\n",
            "Epoch 32/200 loss: 0.2240\n",
            "Epoch 33/200 loss: 0.2219\n",
            "Epoch 34/200 loss: 0.2205\n",
            "Epoch 35/200 loss: 0.2199\n",
            "Epoch 36/200 loss: 0.2195\n",
            "Epoch 37/200 loss: 0.2186\n",
            "Epoch 38/200 loss: 0.2187\n",
            "Epoch 39/200 loss: 0.2182\n",
            "Epoch 40/200 loss: 0.2176\n",
            "Epoch 41/200 loss: 0.2172\n",
            "Epoch 42/200 loss: 0.2168\n",
            "Epoch 43/200 loss: 0.2162\n",
            "Epoch 44/200 loss: 0.2172\n",
            "Epoch 45/200 loss: 0.2151\n",
            "Epoch 46/200 loss: 0.2152\n",
            "Epoch 47/200 loss: 0.2143\n",
            "Epoch 48/200 loss: 0.2150\n",
            "Epoch 49/200 loss: 0.2135\n",
            "Epoch 50/200 loss: 0.2137\n",
            "Epoch 51/200 loss: 0.2129\n",
            "Epoch 52/200 loss: 0.2126\n",
            "Epoch 53/200 loss: 0.2137\n",
            "Epoch 54/200 loss: 0.2124\n",
            "Epoch 55/200 loss: 0.2122\n",
            "Epoch 56/200 loss: 0.2114\n",
            "Epoch 57/200 loss: 0.2110\n",
            "Epoch 58/200 loss: 0.2113\n",
            "Epoch 59/200 loss: 0.2126\n",
            "Epoch 60/200 loss: 0.2106\n",
            "Epoch 61/200 loss: 0.2103\n",
            "Epoch 62/200 loss: 0.2103\n",
            "Epoch 63/200 loss: 0.2098\n",
            "Epoch 64/200 loss: 0.2091\n",
            "Epoch 65/200 loss: 0.2092\n",
            "Epoch 66/200 loss: 0.2095\n",
            "Epoch 67/200 loss: 0.2097\n",
            "Epoch 68/200 loss: 0.2080\n",
            "Epoch 69/200 loss: 0.2079\n",
            "Epoch 70/200 loss: 0.2094\n",
            "Epoch 71/200 loss: 0.2082\n",
            "Epoch 72/200 loss: 0.2081\n",
            "Epoch 73/200 loss: 0.2075\n",
            "Epoch 74/200 loss: 0.2076\n",
            "Epoch 75/200 loss: 0.2082\n",
            "Epoch 76/200 loss: 0.2065\n",
            "Epoch 77/200 loss: 0.2063\n",
            "Epoch 78/200 loss: 0.2079\n",
            "Epoch 79/200 loss: 0.2063\n",
            "Epoch 80/200 loss: 0.2066\n",
            "Epoch 81/200 loss: 0.2056\n",
            "Epoch 82/200 loss: 0.2054\n",
            "Epoch 83/200 loss: 0.2061\n",
            "Epoch 84/200 loss: 0.2074\n",
            "Epoch 85/200 loss: 0.2066\n",
            "Epoch 86/200 loss: 0.2047\n",
            "Epoch 87/200 loss: 0.2051\n",
            "Epoch 88/200 loss: 0.2059\n",
            "Epoch 89/200 loss: 0.2043\n",
            "Epoch 90/200 loss: 0.2050\n",
            "Epoch 91/200 loss: 0.2052\n",
            "Epoch 92/200 loss: 0.2038\n",
            "Epoch 93/200 loss: 0.2047\n",
            "Epoch 94/200 loss: 0.2043\n",
            "Epoch 95/200 loss: 0.2042\n",
            "Epoch 96/200 loss: 0.2036\n",
            "Epoch 97/200 loss: 0.2032\n",
            "Epoch 98/200 loss: 0.2031\n",
            "Epoch 99/200 loss: 0.2040\n",
            "Epoch 100/200 loss: 0.2037\n",
            "Epoch 101/200 loss: 0.2039\n",
            "Epoch 102/200 loss: 0.2033\n",
            "Epoch 103/200 loss: 0.2041\n",
            "Epoch 104/200 loss: 0.2028\n",
            "Epoch 105/200 loss: 0.2024\n",
            "Epoch 106/200 loss: 0.2029\n",
            "Epoch 107/200 loss: 0.2023\n",
            "Epoch 108/200 loss: 0.2023\n",
            "Epoch 109/200 loss: 0.2025\n",
            "Epoch 110/200 loss: 0.2024\n",
            "Epoch 111/200 loss: 0.2018\n",
            "Epoch 112/200 loss: 0.2016\n",
            "Epoch 113/200 loss: 0.2019\n",
            "Epoch 114/200 loss: 0.2025\n",
            "Epoch 115/200 loss: 0.2013\n",
            "Epoch 116/200 loss: 0.2010\n",
            "Epoch 117/200 loss: 0.2017\n",
            "Epoch 118/200 loss: 0.2029\n",
            "Epoch 119/200 loss: 0.2011\n",
            "Epoch 120/200 loss: 0.2009\n",
            "Epoch 121/200 loss: 0.2006\n",
            "Epoch 122/200 loss: 0.2014\n",
            "Epoch 123/200 loss: 0.2009\n",
            "Epoch 124/200 loss: 0.2004\n",
            "Epoch 125/200 loss: 0.2015\n",
            "Epoch 126/200 loss: 0.2009\n",
            "Epoch 127/200 loss: 0.2003\n",
            "Epoch 128/200 loss: 0.2001\n",
            "Epoch 129/200 loss: 0.2009\n",
            "Epoch 130/200 loss: 0.2002\n",
            "Epoch 131/200 loss: 0.1997\n",
            "Epoch 132/200 loss: 0.2005\n",
            "Epoch 133/200 loss: 0.2015\n",
            "Epoch 134/200 loss: 0.1997\n",
            "Epoch 135/200 loss: 0.2005\n",
            "Epoch 136/200 loss: 0.2000\n",
            "Epoch 137/200 loss: 0.2000\n",
            "Epoch 138/200 loss: 0.1996\n",
            "Epoch 139/200 loss: 0.1996\n",
            "Epoch 140/200 loss: 0.1999\n",
            "Epoch 141/200 loss: 0.2005\n",
            "Epoch 142/200 loss: 0.1992\n",
            "Epoch 143/200 loss: 0.1993\n",
            "Epoch 144/200 loss: 0.1992\n",
            "Epoch 145/200 loss: 0.1991\n",
            "Epoch 146/200 loss: 0.2004\n",
            "Epoch 147/200 loss: 0.1991\n",
            "Epoch 148/200 loss: 0.2003\n",
            "Epoch 149/200 loss: 0.1989\n",
            "Epoch 150/200 loss: 0.1990\n",
            "Epoch 151/200 loss: 0.1986\n",
            "Epoch 152/200 loss: 0.2002\n",
            "Epoch 153/200 loss: 0.2000\n",
            "Epoch 154/200 loss: 0.1994\n",
            "Epoch 155/200 loss: 0.2019\n",
            "Epoch 156/200 loss: 0.1982\n",
            "Epoch 157/200 loss: 0.1985\n",
            "Epoch 158/200 loss: 0.1985\n",
            "Epoch 159/200 loss: 0.1985\n",
            "Epoch 160/200 loss: 0.1986\n",
            "Epoch 161/200 loss: 0.1983\n",
            "Epoch 162/200 loss: 0.1981\n",
            "Epoch 163/200 loss: 0.1989\n",
            "Epoch 164/200 loss: 0.1985\n",
            "Epoch 165/200 loss: 0.1983\n",
            "Epoch 166/200 loss: 0.1987\n",
            "Epoch 167/200 loss: 0.1978\n",
            "Epoch 168/200 loss: 0.1992\n",
            "Epoch 169/200 loss: 0.1997\n",
            "Epoch 170/200 loss: 0.1992\n",
            "Epoch 171/200 loss: 0.1983\n",
            "Epoch 172/200 loss: 0.1985\n",
            "Epoch 173/200 loss: 0.1978\n",
            "Epoch 174/200 loss: 0.1985\n",
            "Epoch 175/200 loss: 0.1976\n",
            "Epoch 176/200 loss: 0.1989\n",
            "Epoch 177/200 loss: 0.1978\n",
            "Epoch 178/200 loss: 0.1975\n",
            "Epoch 179/200 loss: 0.1975\n",
            "Epoch 180/200 loss: 0.1978\n",
            "Epoch 181/200 loss: 0.1982\n",
            "Epoch 182/200 loss: 0.1973\n",
            "Epoch 183/200 loss: 0.1978\n",
            "Epoch 184/200 loss: 0.1974\n",
            "Epoch 185/200 loss: 0.1981\n",
            "Epoch 186/200 loss: 0.1983\n",
            "Epoch 187/200 loss: 0.1989\n",
            "Epoch 188/200 loss: 0.1971\n",
            "Epoch 189/200 loss: 0.1975\n",
            "Epoch 190/200 loss: 0.1977\n",
            "Epoch 191/200 loss: 0.1988\n",
            "Epoch 192/200 loss: 0.1976\n",
            "Epoch 193/200 loss: 0.1971\n",
            "Epoch 194/200 loss: 0.1968\n",
            "Epoch 195/200 loss: 0.1967\n",
            "Epoch 196/200 loss: 0.1978\n",
            "Epoch 197/200 loss: 0.1973\n",
            "Epoch 198/200 loss: 0.1964\n",
            "Epoch 199/200 loss: 0.1968\n",
            "Epoch 200/200 loss: 0.1971\n",
            "batch size: 64, learning rate: 0.01, num epochs: 20\n",
            "Epoch 1/20 loss: 0.9968\n",
            "Epoch 2/20 loss: 0.8591\n",
            "Epoch 3/20 loss: 0.7556\n",
            "Epoch 4/20 loss: 0.6753\n",
            "Epoch 5/20 loss: 0.6100\n",
            "Epoch 6/20 loss: 0.5616\n",
            "Epoch 7/20 loss: 0.5175\n",
            "Epoch 8/20 loss: 0.4837\n",
            "Epoch 9/20 loss: 0.4563\n",
            "Epoch 10/20 loss: 0.4325\n",
            "Epoch 11/20 loss: 0.4115\n",
            "Epoch 12/20 loss: 0.3952\n",
            "Epoch 13/20 loss: 0.3801\n",
            "Epoch 14/20 loss: 0.3679\n",
            "Epoch 15/20 loss: 0.3572\n",
            "Epoch 16/20 loss: 0.3470\n",
            "Epoch 17/20 loss: 0.3393\n",
            "Epoch 18/20 loss: 0.3325\n",
            "Epoch 19/20 loss: 0.3238\n",
            "Epoch 20/20 loss: 0.3176\n",
            "batch size: 64, learning rate: 0.01, num epochs: 50\n",
            "Epoch 1/50 loss: 1.9999\n",
            "Epoch 2/50 loss: 1.2754\n",
            "Epoch 3/50 loss: 1.0197\n",
            "Epoch 4/50 loss: 0.8522\n",
            "Epoch 5/50 loss: 0.7327\n",
            "Epoch 6/50 loss: 0.6459\n",
            "Epoch 7/50 loss: 0.5758\n",
            "Epoch 8/50 loss: 0.5231\n",
            "Epoch 9/50 loss: 0.4812\n",
            "Epoch 10/50 loss: 0.4487\n",
            "Epoch 11/50 loss: 0.4206\n",
            "Epoch 12/50 loss: 0.3998\n",
            "Epoch 13/50 loss: 0.3812\n",
            "Epoch 14/50 loss: 0.3651\n",
            "Epoch 15/50 loss: 0.3529\n",
            "Epoch 16/50 loss: 0.3420\n",
            "Epoch 17/50 loss: 0.3322\n",
            "Epoch 18/50 loss: 0.3252\n",
            "Epoch 19/50 loss: 0.3206\n",
            "Epoch 20/50 loss: 0.3105\n",
            "Epoch 21/50 loss: 0.3064\n",
            "Epoch 22/50 loss: 0.3000\n",
            "Epoch 23/50 loss: 0.2958\n",
            "Epoch 24/50 loss: 0.2935\n",
            "Epoch 25/50 loss: 0.2883\n",
            "Epoch 26/50 loss: 0.2845\n",
            "Epoch 27/50 loss: 0.2829\n",
            "Epoch 28/50 loss: 0.2796\n",
            "Epoch 29/50 loss: 0.2764\n",
            "Epoch 30/50 loss: 0.2754\n",
            "Epoch 31/50 loss: 0.2721\n",
            "Epoch 32/50 loss: 0.2704\n",
            "Epoch 33/50 loss: 0.2693\n",
            "Epoch 34/50 loss: 0.2673\n",
            "Epoch 35/50 loss: 0.2660\n",
            "Epoch 36/50 loss: 0.2647\n",
            "Epoch 37/50 loss: 0.2626\n",
            "Epoch 38/50 loss: 0.2610\n",
            "Epoch 39/50 loss: 0.2604\n",
            "Epoch 40/50 loss: 0.2586\n",
            "Epoch 41/50 loss: 0.2578\n",
            "Epoch 42/50 loss: 0.2576\n",
            "Epoch 43/50 loss: 0.2555\n",
            "Epoch 44/50 loss: 0.2545\n",
            "Epoch 45/50 loss: 0.2535\n",
            "Epoch 46/50 loss: 0.2537\n",
            "Epoch 47/50 loss: 0.2519\n",
            "Epoch 48/50 loss: 0.2514\n",
            "Epoch 49/50 loss: 0.2506\n",
            "Epoch 50/50 loss: 0.2497\n",
            "batch size: 64, learning rate: 0.01, num epochs: 100\n",
            "Epoch 1/100 loss: 1.6872\n",
            "Epoch 2/100 loss: 1.3662\n",
            "Epoch 3/100 loss: 1.1394\n",
            "Epoch 4/100 loss: 0.9688\n",
            "Epoch 5/100 loss: 0.8349\n",
            "Epoch 6/100 loss: 0.7329\n",
            "Epoch 7/100 loss: 0.6555\n",
            "Epoch 8/100 loss: 0.5927\n",
            "Epoch 9/100 loss: 0.5418\n",
            "Epoch 10/100 loss: 0.5026\n",
            "Epoch 11/100 loss: 0.4706\n",
            "Epoch 12/100 loss: 0.4427\n",
            "Epoch 13/100 loss: 0.4190\n",
            "Epoch 14/100 loss: 0.3998\n",
            "Epoch 15/100 loss: 0.3842\n",
            "Epoch 16/100 loss: 0.3693\n",
            "Epoch 17/100 loss: 0.3576\n",
            "Epoch 18/100 loss: 0.3464\n",
            "Epoch 19/100 loss: 0.3369\n",
            "Epoch 20/100 loss: 0.3293\n",
            "Epoch 21/100 loss: 0.3213\n",
            "Epoch 22/100 loss: 0.3157\n",
            "Epoch 23/100 loss: 0.3101\n",
            "Epoch 24/100 loss: 0.3044\n",
            "Epoch 25/100 loss: 0.2996\n",
            "Epoch 26/100 loss: 0.2947\n",
            "Epoch 27/100 loss: 0.2922\n",
            "Epoch 28/100 loss: 0.2893\n",
            "Epoch 29/100 loss: 0.2838\n",
            "Epoch 30/100 loss: 0.2809\n",
            "Epoch 31/100 loss: 0.2778\n",
            "Epoch 32/100 loss: 0.2760\n",
            "Epoch 33/100 loss: 0.2730\n",
            "Epoch 34/100 loss: 0.2716\n",
            "Epoch 35/100 loss: 0.2700\n",
            "Epoch 36/100 loss: 0.2674\n",
            "Epoch 37/100 loss: 0.2650\n",
            "Epoch 38/100 loss: 0.2637\n",
            "Epoch 39/100 loss: 0.2627\n",
            "Epoch 40/100 loss: 0.2608\n",
            "Epoch 41/100 loss: 0.2589\n",
            "Epoch 42/100 loss: 0.2578\n",
            "Epoch 43/100 loss: 0.2562\n",
            "Epoch 44/100 loss: 0.2566\n",
            "Epoch 45/100 loss: 0.2541\n",
            "Epoch 46/100 loss: 0.2541\n",
            "Epoch 47/100 loss: 0.2529\n",
            "Epoch 48/100 loss: 0.2531\n",
            "Epoch 49/100 loss: 0.2509\n",
            "Epoch 50/100 loss: 0.2502\n",
            "Epoch 51/100 loss: 0.2492\n",
            "Epoch 52/100 loss: 0.2485\n",
            "Epoch 53/100 loss: 0.2474\n",
            "Epoch 54/100 loss: 0.2462\n",
            "Epoch 55/100 loss: 0.2461\n",
            "Epoch 56/100 loss: 0.2446\n",
            "Epoch 57/100 loss: 0.2448\n",
            "Epoch 58/100 loss: 0.2451\n",
            "Epoch 59/100 loss: 0.2441\n",
            "Epoch 60/100 loss: 0.2424\n",
            "Epoch 61/100 loss: 0.2425\n",
            "Epoch 62/100 loss: 0.2417\n",
            "Epoch 63/100 loss: 0.2407\n",
            "Epoch 64/100 loss: 0.2404\n",
            "Epoch 65/100 loss: 0.2392\n",
            "Epoch 66/100 loss: 0.2392\n",
            "Epoch 67/100 loss: 0.2392\n",
            "Epoch 68/100 loss: 0.2385\n",
            "Epoch 69/100 loss: 0.2386\n",
            "Epoch 70/100 loss: 0.2372\n",
            "Epoch 71/100 loss: 0.2376\n",
            "Epoch 72/100 loss: 0.2363\n",
            "Epoch 73/100 loss: 0.2373\n",
            "Epoch 74/100 loss: 0.2369\n",
            "Epoch 75/100 loss: 0.2349\n",
            "Epoch 76/100 loss: 0.2359\n",
            "Epoch 77/100 loss: 0.2343\n",
            "Epoch 78/100 loss: 0.2354\n",
            "Epoch 79/100 loss: 0.2339\n",
            "Epoch 80/100 loss: 0.2334\n",
            "Epoch 81/100 loss: 0.2326\n",
            "Epoch 82/100 loss: 0.2339\n",
            "Epoch 83/100 loss: 0.2322\n",
            "Epoch 84/100 loss: 0.2332\n",
            "Epoch 85/100 loss: 0.2327\n",
            "Epoch 86/100 loss: 0.2312\n",
            "Epoch 87/100 loss: 0.2310\n",
            "Epoch 88/100 loss: 0.2317\n",
            "Epoch 89/100 loss: 0.2306\n",
            "Epoch 90/100 loss: 0.2305\n",
            "Epoch 91/100 loss: 0.2307\n",
            "Epoch 92/100 loss: 0.2298\n",
            "Epoch 93/100 loss: 0.2289\n",
            "Epoch 94/100 loss: 0.2312\n",
            "Epoch 95/100 loss: 0.2297\n",
            "Epoch 96/100 loss: 0.2299\n",
            "Epoch 97/100 loss: 0.2288\n",
            "Epoch 98/100 loss: 0.2277\n",
            "Epoch 99/100 loss: 0.2277\n",
            "Epoch 100/100 loss: 0.2275\n",
            "batch size: 64, learning rate: 0.01, num epochs: 200\n",
            "Epoch 1/200 loss: 1.4339\n",
            "Epoch 2/200 loss: 1.0625\n",
            "Epoch 3/200 loss: 0.8602\n",
            "Epoch 4/200 loss: 0.7334\n",
            "Epoch 5/200 loss: 0.6465\n",
            "Epoch 6/200 loss: 0.5875\n",
            "Epoch 7/200 loss: 0.5429\n",
            "Epoch 8/200 loss: 0.5058\n",
            "Epoch 9/200 loss: 0.4757\n",
            "Epoch 10/200 loss: 0.4521\n",
            "Epoch 11/200 loss: 0.4325\n",
            "Epoch 12/200 loss: 0.4177\n",
            "Epoch 13/200 loss: 0.4018\n",
            "Epoch 14/200 loss: 0.3893\n",
            "Epoch 15/200 loss: 0.3778\n",
            "Epoch 16/200 loss: 0.3681\n",
            "Epoch 17/200 loss: 0.3594\n",
            "Epoch 18/200 loss: 0.3521\n",
            "Epoch 19/200 loss: 0.3473\n",
            "Epoch 20/200 loss: 0.3394\n",
            "Epoch 21/200 loss: 0.3338\n",
            "Epoch 22/200 loss: 0.3292\n",
            "Epoch 23/200 loss: 0.3243\n",
            "Epoch 24/200 loss: 0.3206\n",
            "Epoch 25/200 loss: 0.3189\n",
            "Epoch 26/200 loss: 0.3132\n",
            "Epoch 27/200 loss: 0.3101\n",
            "Epoch 28/200 loss: 0.3079\n",
            "Epoch 29/200 loss: 0.3046\n",
            "Epoch 30/200 loss: 0.3033\n",
            "Epoch 31/200 loss: 0.2990\n",
            "Epoch 32/200 loss: 0.2972\n",
            "Epoch 33/200 loss: 0.2945\n",
            "Epoch 34/200 loss: 0.2923\n",
            "Epoch 35/200 loss: 0.2923\n",
            "Epoch 36/200 loss: 0.2897\n",
            "Epoch 37/200 loss: 0.2869\n",
            "Epoch 38/200 loss: 0.2852\n",
            "Epoch 39/200 loss: 0.2837\n",
            "Epoch 40/200 loss: 0.2820\n",
            "Epoch 41/200 loss: 0.2811\n",
            "Epoch 42/200 loss: 0.2802\n",
            "Epoch 43/200 loss: 0.2783\n",
            "Epoch 44/200 loss: 0.2766\n",
            "Epoch 45/200 loss: 0.2765\n",
            "Epoch 46/200 loss: 0.2746\n",
            "Epoch 47/200 loss: 0.2745\n",
            "Epoch 48/200 loss: 0.2724\n",
            "Epoch 49/200 loss: 0.2727\n",
            "Epoch 50/200 loss: 0.2697\n",
            "Epoch 51/200 loss: 0.2697\n",
            "Epoch 52/200 loss: 0.2688\n",
            "Epoch 53/200 loss: 0.2676\n",
            "Epoch 54/200 loss: 0.2680\n",
            "Epoch 55/200 loss: 0.2651\n",
            "Epoch 56/200 loss: 0.2642\n",
            "Epoch 57/200 loss: 0.2638\n",
            "Epoch 58/200 loss: 0.2652\n",
            "Epoch 59/200 loss: 0.2628\n",
            "Epoch 60/200 loss: 0.2619\n",
            "Epoch 61/200 loss: 0.2608\n",
            "Epoch 62/200 loss: 0.2596\n",
            "Epoch 63/200 loss: 0.2593\n",
            "Epoch 64/200 loss: 0.2582\n",
            "Epoch 65/200 loss: 0.2606\n",
            "Epoch 66/200 loss: 0.2586\n",
            "Epoch 67/200 loss: 0.2567\n",
            "Epoch 68/200 loss: 0.2557\n",
            "Epoch 69/200 loss: 0.2550\n",
            "Epoch 70/200 loss: 0.2542\n",
            "Epoch 71/200 loss: 0.2537\n",
            "Epoch 72/200 loss: 0.2550\n",
            "Epoch 73/200 loss: 0.2530\n",
            "Epoch 74/200 loss: 0.2520\n",
            "Epoch 75/200 loss: 0.2515\n",
            "Epoch 76/200 loss: 0.2520\n",
            "Epoch 77/200 loss: 0.2505\n",
            "Epoch 78/200 loss: 0.2498\n",
            "Epoch 79/200 loss: 0.2493\n",
            "Epoch 80/200 loss: 0.2495\n",
            "Epoch 81/200 loss: 0.2495\n",
            "Epoch 82/200 loss: 0.2479\n",
            "Epoch 83/200 loss: 0.2486\n",
            "Epoch 84/200 loss: 0.2471\n",
            "Epoch 85/200 loss: 0.2473\n",
            "Epoch 86/200 loss: 0.2468\n",
            "Epoch 87/200 loss: 0.2476\n",
            "Epoch 88/200 loss: 0.2453\n",
            "Epoch 89/200 loss: 0.2455\n",
            "Epoch 90/200 loss: 0.2455\n",
            "Epoch 91/200 loss: 0.2444\n",
            "Epoch 92/200 loss: 0.2447\n",
            "Epoch 93/200 loss: 0.2439\n",
            "Epoch 94/200 loss: 0.2433\n",
            "Epoch 95/200 loss: 0.2430\n",
            "Epoch 96/200 loss: 0.2437\n",
            "Epoch 97/200 loss: 0.2428\n",
            "Epoch 98/200 loss: 0.2414\n",
            "Epoch 99/200 loss: 0.2426\n",
            "Epoch 100/200 loss: 0.2408\n",
            "Epoch 101/200 loss: 0.2421\n",
            "Epoch 102/200 loss: 0.2400\n",
            "Epoch 103/200 loss: 0.2405\n",
            "Epoch 104/200 loss: 0.2394\n",
            "Epoch 105/200 loss: 0.2392\n",
            "Epoch 106/200 loss: 0.2391\n",
            "Epoch 107/200 loss: 0.2400\n",
            "Epoch 108/200 loss: 0.2383\n",
            "Epoch 109/200 loss: 0.2383\n",
            "Epoch 110/200 loss: 0.2385\n",
            "Epoch 111/200 loss: 0.2379\n",
            "Epoch 112/200 loss: 0.2375\n",
            "Epoch 113/200 loss: 0.2369\n",
            "Epoch 114/200 loss: 0.2367\n",
            "Epoch 115/200 loss: 0.2385\n",
            "Epoch 116/200 loss: 0.2370\n",
            "Epoch 117/200 loss: 0.2364\n",
            "Epoch 118/200 loss: 0.2358\n",
            "Epoch 119/200 loss: 0.2362\n",
            "Epoch 120/200 loss: 0.2358\n",
            "Epoch 121/200 loss: 0.2353\n",
            "Epoch 122/200 loss: 0.2367\n",
            "Epoch 123/200 loss: 0.2349\n",
            "Epoch 124/200 loss: 0.2343\n",
            "Epoch 125/200 loss: 0.2370\n",
            "Epoch 126/200 loss: 0.2345\n",
            "Epoch 127/200 loss: 0.2338\n",
            "Epoch 128/200 loss: 0.2336\n",
            "Epoch 129/200 loss: 0.2331\n",
            "Epoch 130/200 loss: 0.2340\n",
            "Epoch 131/200 loss: 0.2333\n",
            "Epoch 132/200 loss: 0.2338\n",
            "Epoch 133/200 loss: 0.2336\n",
            "Epoch 134/200 loss: 0.2320\n",
            "Epoch 135/200 loss: 0.2319\n",
            "Epoch 136/200 loss: 0.2325\n",
            "Epoch 137/200 loss: 0.2315\n",
            "Epoch 138/200 loss: 0.2310\n",
            "Epoch 139/200 loss: 0.2322\n",
            "Epoch 140/200 loss: 0.2312\n",
            "Epoch 141/200 loss: 0.2315\n",
            "Epoch 142/200 loss: 0.2310\n",
            "Epoch 143/200 loss: 0.2305\n",
            "Epoch 144/200 loss: 0.2300\n",
            "Epoch 145/200 loss: 0.2298\n",
            "Epoch 146/200 loss: 0.2299\n",
            "Epoch 147/200 loss: 0.2303\n",
            "Epoch 148/200 loss: 0.2299\n",
            "Epoch 149/200 loss: 0.2290\n",
            "Epoch 150/200 loss: 0.2293\n",
            "Epoch 151/200 loss: 0.2297\n",
            "Epoch 152/200 loss: 0.2294\n",
            "Epoch 153/200 loss: 0.2295\n",
            "Epoch 154/200 loss: 0.2285\n",
            "Epoch 155/200 loss: 0.2302\n",
            "Epoch 156/200 loss: 0.2289\n",
            "Epoch 157/200 loss: 0.2289\n",
            "Epoch 158/200 loss: 0.2279\n",
            "Epoch 159/200 loss: 0.2279\n",
            "Epoch 160/200 loss: 0.2274\n",
            "Epoch 161/200 loss: 0.2285\n",
            "Epoch 162/200 loss: 0.2270\n",
            "Epoch 163/200 loss: 0.2271\n",
            "Epoch 164/200 loss: 0.2269\n",
            "Epoch 165/200 loss: 0.2268\n",
            "Epoch 166/200 loss: 0.2262\n",
            "Epoch 167/200 loss: 0.2264\n",
            "Epoch 168/200 loss: 0.2268\n",
            "Epoch 169/200 loss: 0.2262\n",
            "Epoch 170/200 loss: 0.2263\n",
            "Epoch 171/200 loss: 0.2270\n",
            "Epoch 172/200 loss: 0.2255\n",
            "Epoch 173/200 loss: 0.2262\n",
            "Epoch 174/200 loss: 0.2259\n",
            "Epoch 175/200 loss: 0.2259\n",
            "Epoch 176/200 loss: 0.2248\n",
            "Epoch 177/200 loss: 0.2256\n",
            "Epoch 178/200 loss: 0.2248\n",
            "Epoch 179/200 loss: 0.2246\n",
            "Epoch 180/200 loss: 0.2254\n",
            "Epoch 181/200 loss: 0.2243\n",
            "Epoch 182/200 loss: 0.2243\n",
            "Epoch 183/200 loss: 0.2247\n",
            "Epoch 184/200 loss: 0.2241\n",
            "Epoch 185/200 loss: 0.2241\n",
            "Epoch 186/200 loss: 0.2247\n",
            "Epoch 187/200 loss: 0.2236\n",
            "Epoch 188/200 loss: 0.2232\n",
            "Epoch 189/200 loss: 0.2246\n",
            "Epoch 190/200 loss: 0.2235\n",
            "Epoch 191/200 loss: 0.2232\n",
            "Epoch 192/200 loss: 0.2233\n",
            "Epoch 193/200 loss: 0.2228\n",
            "Epoch 194/200 loss: 0.2228\n",
            "Epoch 195/200 loss: 0.2226\n",
            "Epoch 196/200 loss: 0.2231\n",
            "Epoch 197/200 loss: 0.2238\n",
            "Epoch 198/200 loss: 0.2222\n",
            "Epoch 199/200 loss: 0.2224\n",
            "Epoch 200/200 loss: 0.2240\n",
            "batch size: 64, learning rate: 0.001, num epochs: 20\n",
            "Epoch 1/20 loss: 2.4017\n",
            "Epoch 2/20 loss: 2.2476\n",
            "Epoch 3/20 loss: 2.1162\n",
            "Epoch 4/20 loss: 1.9963\n",
            "Epoch 5/20 loss: 1.8946\n",
            "Epoch 6/20 loss: 1.8036\n",
            "Epoch 7/20 loss: 1.7303\n",
            "Epoch 8/20 loss: 1.6592\n",
            "Epoch 9/20 loss: 1.5996\n",
            "Epoch 10/20 loss: 1.5485\n",
            "Epoch 11/20 loss: 1.4983\n",
            "Epoch 12/20 loss: 1.4568\n",
            "Epoch 13/20 loss: 1.4175\n",
            "Epoch 14/20 loss: 1.3813\n",
            "Epoch 15/20 loss: 1.3459\n",
            "Epoch 16/20 loss: 1.3140\n",
            "Epoch 17/20 loss: 1.2819\n",
            "Epoch 18/20 loss: 1.2511\n",
            "Epoch 19/20 loss: 1.2262\n",
            "Epoch 20/20 loss: 1.1953\n",
            "batch size: 64, learning rate: 0.001, num epochs: 50\n",
            "Epoch 1/50 loss: 1.6393\n",
            "Epoch 2/50 loss: 1.5639\n",
            "Epoch 3/50 loss: 1.4979\n",
            "Epoch 4/50 loss: 1.4443\n",
            "Epoch 5/50 loss: 1.3907\n",
            "Epoch 6/50 loss: 1.3519\n",
            "Epoch 7/50 loss: 1.3098\n",
            "Epoch 8/50 loss: 1.2728\n",
            "Epoch 9/50 loss: 1.2423\n",
            "Epoch 10/50 loss: 1.2146\n",
            "Epoch 11/50 loss: 1.1835\n",
            "Epoch 12/50 loss: 1.1603\n",
            "Epoch 13/50 loss: 1.1330\n",
            "Epoch 14/50 loss: 1.1111\n",
            "Epoch 15/50 loss: 1.0862\n",
            "Epoch 16/50 loss: 1.0650\n",
            "Epoch 17/50 loss: 1.0472\n",
            "Epoch 18/50 loss: 1.0231\n",
            "Epoch 19/50 loss: 1.0042\n",
            "Epoch 20/50 loss: 0.9874\n",
            "Epoch 21/50 loss: 0.9682\n",
            "Epoch 22/50 loss: 0.9506\n",
            "Epoch 23/50 loss: 0.9355\n",
            "Epoch 24/50 loss: 0.9186\n",
            "Epoch 25/50 loss: 0.9057\n",
            "Epoch 26/50 loss: 0.8905\n",
            "Epoch 27/50 loss: 0.8758\n",
            "Epoch 28/50 loss: 0.8609\n",
            "Epoch 29/50 loss: 0.8455\n",
            "Epoch 30/50 loss: 0.8362\n",
            "Epoch 31/50 loss: 0.8228\n",
            "Epoch 32/50 loss: 0.8084\n",
            "Epoch 33/50 loss: 0.7981\n",
            "Epoch 34/50 loss: 0.7873\n",
            "Epoch 35/50 loss: 0.7745\n",
            "Epoch 36/50 loss: 0.7642\n",
            "Epoch 37/50 loss: 0.7553\n",
            "Epoch 38/50 loss: 0.7459\n",
            "Epoch 39/50 loss: 0.7319\n",
            "Epoch 40/50 loss: 0.7226\n",
            "Epoch 41/50 loss: 0.7153\n",
            "Epoch 42/50 loss: 0.7047\n",
            "Epoch 43/50 loss: 0.6955\n",
            "Epoch 44/50 loss: 0.6886\n",
            "Epoch 45/50 loss: 0.6790\n",
            "Epoch 46/50 loss: 0.6702\n",
            "Epoch 47/50 loss: 0.6649\n",
            "Epoch 48/50 loss: 0.6586\n",
            "Epoch 49/50 loss: 0.6489\n",
            "Epoch 50/50 loss: 0.6404\n",
            "batch size: 64, learning rate: 0.001, num epochs: 100\n",
            "Epoch 1/100 loss: 2.3795\n",
            "Epoch 2/100 loss: 2.1496\n",
            "Epoch 3/100 loss: 1.9476\n",
            "Epoch 4/100 loss: 1.7688\n",
            "Epoch 5/100 loss: 1.6178\n",
            "Epoch 6/100 loss: 1.4993\n",
            "Epoch 7/100 loss: 1.3994\n",
            "Epoch 8/100 loss: 1.3203\n",
            "Epoch 9/100 loss: 1.2583\n",
            "Epoch 10/100 loss: 1.2112\n",
            "Epoch 11/100 loss: 1.1716\n",
            "Epoch 12/100 loss: 1.1365\n",
            "Epoch 13/100 loss: 1.1057\n",
            "Epoch 14/100 loss: 1.0809\n",
            "Epoch 15/100 loss: 1.0558\n",
            "Epoch 16/100 loss: 1.0362\n",
            "Epoch 17/100 loss: 1.0169\n",
            "Epoch 18/100 loss: 0.9969\n",
            "Epoch 19/100 loss: 0.9810\n",
            "Epoch 20/100 loss: 0.9634\n",
            "Epoch 21/100 loss: 0.9474\n",
            "Epoch 22/100 loss: 0.9321\n",
            "Epoch 23/100 loss: 0.9166\n",
            "Epoch 24/100 loss: 0.9041\n",
            "Epoch 25/100 loss: 0.8865\n",
            "Epoch 26/100 loss: 0.8772\n",
            "Epoch 27/100 loss: 0.8628\n",
            "Epoch 28/100 loss: 0.8477\n",
            "Epoch 29/100 loss: 0.8352\n",
            "Epoch 30/100 loss: 0.8259\n",
            "Epoch 31/100 loss: 0.8105\n",
            "Epoch 32/100 loss: 0.8022\n",
            "Epoch 33/100 loss: 0.7890\n",
            "Epoch 34/100 loss: 0.7770\n",
            "Epoch 35/100 loss: 0.7673\n",
            "Epoch 36/100 loss: 0.7567\n",
            "Epoch 37/100 loss: 0.7463\n",
            "Epoch 38/100 loss: 0.7368\n",
            "Epoch 39/100 loss: 0.7267\n",
            "Epoch 40/100 loss: 0.7182\n",
            "Epoch 41/100 loss: 0.7075\n",
            "Epoch 42/100 loss: 0.7002\n",
            "Epoch 43/100 loss: 0.6904\n",
            "Epoch 44/100 loss: 0.6830\n",
            "Epoch 45/100 loss: 0.6741\n",
            "Epoch 46/100 loss: 0.6648\n",
            "Epoch 47/100 loss: 0.6582\n",
            "Epoch 48/100 loss: 0.6491\n",
            "Epoch 49/100 loss: 0.6429\n",
            "Epoch 50/100 loss: 0.6348\n",
            "Epoch 51/100 loss: 0.6278\n",
            "Epoch 52/100 loss: 0.6207\n",
            "Epoch 53/100 loss: 0.6136\n",
            "Epoch 54/100 loss: 0.6084\n",
            "Epoch 55/100 loss: 0.6006\n",
            "Epoch 56/100 loss: 0.5954\n",
            "Epoch 57/100 loss: 0.5885\n",
            "Epoch 58/100 loss: 0.5826\n",
            "Epoch 59/100 loss: 0.5764\n",
            "Epoch 60/100 loss: 0.5710\n",
            "Epoch 61/100 loss: 0.5658\n",
            "Epoch 62/100 loss: 0.5629\n",
            "Epoch 63/100 loss: 0.5561\n",
            "Epoch 64/100 loss: 0.5503\n",
            "Epoch 65/100 loss: 0.5434\n",
            "Epoch 66/100 loss: 0.5405\n",
            "Epoch 67/100 loss: 0.5341\n",
            "Epoch 68/100 loss: 0.5302\n",
            "Epoch 69/100 loss: 0.5255\n",
            "Epoch 70/100 loss: 0.5216\n",
            "Epoch 71/100 loss: 0.5155\n",
            "Epoch 72/100 loss: 0.5126\n",
            "Epoch 73/100 loss: 0.5073\n",
            "Epoch 74/100 loss: 0.5027\n",
            "Epoch 75/100 loss: 0.4997\n",
            "Epoch 76/100 loss: 0.4949\n",
            "Epoch 77/100 loss: 0.4916\n",
            "Epoch 78/100 loss: 0.4879\n",
            "Epoch 79/100 loss: 0.4845\n",
            "Epoch 80/100 loss: 0.4815\n",
            "Epoch 81/100 loss: 0.4779\n",
            "Epoch 82/100 loss: 0.4745\n",
            "Epoch 83/100 loss: 0.4714\n",
            "Epoch 84/100 loss: 0.4673\n",
            "Epoch 85/100 loss: 0.4644\n",
            "Epoch 86/100 loss: 0.4614\n",
            "Epoch 87/100 loss: 0.4598\n",
            "Epoch 88/100 loss: 0.4542\n",
            "Epoch 89/100 loss: 0.4522\n",
            "Epoch 90/100 loss: 0.4488\n",
            "Epoch 91/100 loss: 0.4453\n",
            "Epoch 92/100 loss: 0.4444\n",
            "Epoch 93/100 loss: 0.4430\n",
            "Epoch 94/100 loss: 0.4380\n",
            "Epoch 95/100 loss: 0.4358\n",
            "Epoch 96/100 loss: 0.4330\n",
            "Epoch 97/100 loss: 0.4300\n",
            "Epoch 98/100 loss: 0.4294\n",
            "Epoch 99/100 loss: 0.4263\n",
            "Epoch 100/100 loss: 0.4246\n",
            "batch size: 64, learning rate: 0.001, num epochs: 200\n",
            "Epoch 1/200 loss: 1.7730\n",
            "Epoch 2/200 loss: 1.7331\n",
            "Epoch 3/200 loss: 1.6842\n",
            "Epoch 4/200 loss: 1.6439\n",
            "Epoch 5/200 loss: 1.6044\n",
            "Epoch 6/200 loss: 1.5616\n",
            "Epoch 7/200 loss: 1.5280\n",
            "Epoch 8/200 loss: 1.4915\n",
            "Epoch 9/200 loss: 1.4595\n",
            "Epoch 10/200 loss: 1.4236\n",
            "Epoch 11/200 loss: 1.3913\n",
            "Epoch 12/200 loss: 1.3671\n",
            "Epoch 13/200 loss: 1.3318\n",
            "Epoch 14/200 loss: 1.3016\n",
            "Epoch 15/200 loss: 1.2755\n",
            "Epoch 16/200 loss: 1.2472\n",
            "Epoch 17/200 loss: 1.2208\n",
            "Epoch 18/200 loss: 1.1949\n",
            "Epoch 19/200 loss: 1.1714\n",
            "Epoch 20/200 loss: 1.1481\n",
            "Epoch 21/200 loss: 1.1240\n",
            "Epoch 22/200 loss: 1.1030\n",
            "Epoch 23/200 loss: 1.0809\n",
            "Epoch 24/200 loss: 1.0600\n",
            "Epoch 25/200 loss: 1.0404\n",
            "Epoch 26/200 loss: 1.0214\n",
            "Epoch 27/200 loss: 1.0005\n",
            "Epoch 28/200 loss: 0.9820\n",
            "Epoch 29/200 loss: 0.9670\n",
            "Epoch 30/200 loss: 0.9479\n",
            "Epoch 31/200 loss: 0.9314\n",
            "Epoch 32/200 loss: 0.9180\n",
            "Epoch 33/200 loss: 0.9037\n",
            "Epoch 34/200 loss: 0.8898\n",
            "Epoch 35/200 loss: 0.8738\n",
            "Epoch 36/200 loss: 0.8624\n",
            "Epoch 37/200 loss: 0.8489\n",
            "Epoch 38/200 loss: 0.8344\n",
            "Epoch 39/200 loss: 0.8220\n",
            "Epoch 40/200 loss: 0.8114\n",
            "Epoch 41/200 loss: 0.8010\n",
            "Epoch 42/200 loss: 0.7891\n",
            "Epoch 43/200 loss: 0.7809\n",
            "Epoch 44/200 loss: 0.7705\n",
            "Epoch 45/200 loss: 0.7590\n",
            "Epoch 46/200 loss: 0.7502\n",
            "Epoch 47/200 loss: 0.7397\n",
            "Epoch 48/200 loss: 0.7313\n",
            "Epoch 49/200 loss: 0.7227\n",
            "Epoch 50/200 loss: 0.7120\n",
            "Epoch 51/200 loss: 0.7039\n",
            "Epoch 52/200 loss: 0.6954\n",
            "Epoch 53/200 loss: 0.6885\n",
            "Epoch 54/200 loss: 0.6802\n",
            "Epoch 55/200 loss: 0.6726\n",
            "Epoch 56/200 loss: 0.6656\n",
            "Epoch 57/200 loss: 0.6580\n",
            "Epoch 58/200 loss: 0.6500\n",
            "Epoch 59/200 loss: 0.6431\n",
            "Epoch 60/200 loss: 0.6357\n",
            "Epoch 61/200 loss: 0.6287\n",
            "Epoch 62/200 loss: 0.6227\n",
            "Epoch 63/200 loss: 0.6176\n",
            "Epoch 64/200 loss: 0.6114\n",
            "Epoch 65/200 loss: 0.6040\n",
            "Epoch 66/200 loss: 0.5980\n",
            "Epoch 67/200 loss: 0.5922\n",
            "Epoch 68/200 loss: 0.5875\n",
            "Epoch 69/200 loss: 0.5827\n",
            "Epoch 70/200 loss: 0.5769\n",
            "Epoch 71/200 loss: 0.5721\n",
            "Epoch 72/200 loss: 0.5665\n",
            "Epoch 73/200 loss: 0.5627\n",
            "Epoch 74/200 loss: 0.5576\n",
            "Epoch 75/200 loss: 0.5522\n",
            "Epoch 76/200 loss: 0.5467\n",
            "Epoch 77/200 loss: 0.5440\n",
            "Epoch 78/200 loss: 0.5389\n",
            "Epoch 79/200 loss: 0.5339\n",
            "Epoch 80/200 loss: 0.5318\n",
            "Epoch 81/200 loss: 0.5281\n",
            "Epoch 82/200 loss: 0.5220\n",
            "Epoch 83/200 loss: 0.5182\n",
            "Epoch 84/200 loss: 0.5134\n",
            "Epoch 85/200 loss: 0.5108\n",
            "Epoch 86/200 loss: 0.5072\n",
            "Epoch 87/200 loss: 0.5028\n",
            "Epoch 88/200 loss: 0.5020\n",
            "Epoch 89/200 loss: 0.4969\n",
            "Epoch 90/200 loss: 0.4929\n",
            "Epoch 91/200 loss: 0.4883\n",
            "Epoch 92/200 loss: 0.4845\n",
            "Epoch 93/200 loss: 0.4833\n",
            "Epoch 94/200 loss: 0.4785\n",
            "Epoch 95/200 loss: 0.4751\n",
            "Epoch 96/200 loss: 0.4720\n",
            "Epoch 97/200 loss: 0.4707\n",
            "Epoch 98/200 loss: 0.4673\n",
            "Epoch 99/200 loss: 0.4648\n",
            "Epoch 100/200 loss: 0.4615\n",
            "Epoch 101/200 loss: 0.4589\n",
            "Epoch 102/200 loss: 0.4562\n",
            "Epoch 103/200 loss: 0.4538\n",
            "Epoch 104/200 loss: 0.4507\n",
            "Epoch 105/200 loss: 0.4493\n",
            "Epoch 106/200 loss: 0.4463\n",
            "Epoch 107/200 loss: 0.4434\n",
            "Epoch 108/200 loss: 0.4422\n",
            "Epoch 109/200 loss: 0.4389\n",
            "Epoch 110/200 loss: 0.4370\n",
            "Epoch 111/200 loss: 0.4340\n",
            "Epoch 112/200 loss: 0.4323\n",
            "Epoch 113/200 loss: 0.4314\n",
            "Epoch 114/200 loss: 0.4281\n",
            "Epoch 115/200 loss: 0.4258\n",
            "Epoch 116/200 loss: 0.4230\n",
            "Epoch 117/200 loss: 0.4226\n",
            "Epoch 118/200 loss: 0.4205\n",
            "Epoch 119/200 loss: 0.4192\n",
            "Epoch 120/200 loss: 0.4166\n",
            "Epoch 121/200 loss: 0.4145\n",
            "Epoch 122/200 loss: 0.4125\n",
            "Epoch 123/200 loss: 0.4105\n",
            "Epoch 124/200 loss: 0.4094\n",
            "Epoch 125/200 loss: 0.4092\n",
            "Epoch 126/200 loss: 0.4052\n",
            "Epoch 127/200 loss: 0.4040\n",
            "Epoch 128/200 loss: 0.4030\n",
            "Epoch 129/200 loss: 0.4000\n",
            "Epoch 130/200 loss: 0.3993\n",
            "Epoch 131/200 loss: 0.3990\n",
            "Epoch 132/200 loss: 0.3956\n",
            "Epoch 133/200 loss: 0.3950\n",
            "Epoch 134/200 loss: 0.3926\n",
            "Epoch 135/200 loss: 0.3926\n",
            "Epoch 136/200 loss: 0.3892\n",
            "Epoch 137/200 loss: 0.3894\n",
            "Epoch 138/200 loss: 0.3876\n",
            "Epoch 139/200 loss: 0.3864\n",
            "Epoch 140/200 loss: 0.3863\n",
            "Epoch 141/200 loss: 0.3832\n",
            "Epoch 142/200 loss: 0.3818\n",
            "Epoch 143/200 loss: 0.3825\n",
            "Epoch 144/200 loss: 0.3792\n",
            "Epoch 145/200 loss: 0.3797\n",
            "Epoch 146/200 loss: 0.3763\n",
            "Epoch 147/200 loss: 0.3768\n",
            "Epoch 148/200 loss: 0.3745\n",
            "Epoch 149/200 loss: 0.3736\n",
            "Epoch 150/200 loss: 0.3721\n",
            "Epoch 151/200 loss: 0.3717\n",
            "Epoch 152/200 loss: 0.3703\n",
            "Epoch 153/200 loss: 0.3689\n",
            "Epoch 154/200 loss: 0.3668\n",
            "Epoch 155/200 loss: 0.3666\n",
            "Epoch 156/200 loss: 0.3656\n",
            "Epoch 157/200 loss: 0.3644\n",
            "Epoch 158/200 loss: 0.3629\n",
            "Epoch 159/200 loss: 0.3648\n",
            "Epoch 160/200 loss: 0.3617\n",
            "Epoch 161/200 loss: 0.3597\n",
            "Epoch 162/200 loss: 0.3596\n",
            "Epoch 163/200 loss: 0.3599\n",
            "Epoch 164/200 loss: 0.3577\n",
            "Epoch 165/200 loss: 0.3572\n",
            "Epoch 166/200 loss: 0.3557\n",
            "Epoch 167/200 loss: 0.3551\n",
            "Epoch 168/200 loss: 0.3550\n",
            "Epoch 169/200 loss: 0.3532\n",
            "Epoch 170/200 loss: 0.3517\n",
            "Epoch 171/200 loss: 0.3534\n",
            "Epoch 172/200 loss: 0.3503\n",
            "Epoch 173/200 loss: 0.3492\n",
            "Epoch 174/200 loss: 0.3487\n",
            "Epoch 175/200 loss: 0.3497\n",
            "Epoch 176/200 loss: 0.3474\n",
            "Epoch 177/200 loss: 0.3472\n",
            "Epoch 178/200 loss: 0.3453\n",
            "Epoch 179/200 loss: 0.3441\n",
            "Epoch 180/200 loss: 0.3438\n",
            "Epoch 181/200 loss: 0.3448\n",
            "Epoch 182/200 loss: 0.3432\n",
            "Epoch 183/200 loss: 0.3433\n",
            "Epoch 184/200 loss: 0.3406\n",
            "Epoch 185/200 loss: 0.3403\n",
            "Epoch 186/200 loss: 0.3402\n",
            "Epoch 187/200 loss: 0.3389\n",
            "Epoch 188/200 loss: 0.3384\n",
            "Epoch 189/200 loss: 0.3411\n",
            "Epoch 190/200 loss: 0.3366\n",
            "Epoch 191/200 loss: 0.3378\n",
            "Epoch 192/200 loss: 0.3360\n",
            "Epoch 193/200 loss: 0.3356\n",
            "Epoch 194/200 loss: 0.3342\n",
            "Epoch 195/200 loss: 0.3332\n",
            "Epoch 196/200 loss: 0.3349\n",
            "Epoch 197/200 loss: 0.3318\n",
            "Epoch 198/200 loss: 0.3339\n",
            "Epoch 199/200 loss: 0.3323\n",
            "Epoch 200/200 loss: 0.3308\n",
            "batch size: 128, learning rate: 0.1, num epochs: 20\n",
            "Epoch 1/20 loss: 0.6761\n",
            "Epoch 2/20 loss: 0.4164\n",
            "Epoch 3/20 loss: 0.3450\n",
            "Epoch 4/20 loss: 0.3085\n",
            "Epoch 5/20 loss: 0.2951\n",
            "Epoch 6/20 loss: 0.2816\n",
            "Epoch 7/20 loss: 0.2792\n",
            "Epoch 8/20 loss: 0.2676\n",
            "Epoch 9/20 loss: 0.2605\n",
            "Epoch 10/20 loss: 0.2596\n",
            "Epoch 11/20 loss: 0.2546\n",
            "Epoch 12/20 loss: 0.2499\n",
            "Epoch 13/20 loss: 0.2501\n",
            "Epoch 14/20 loss: 0.2462\n",
            "Epoch 15/20 loss: 0.2436\n",
            "Epoch 16/20 loss: 0.2420\n",
            "Epoch 17/20 loss: 0.2413\n",
            "Epoch 18/20 loss: 0.2390\n",
            "Epoch 19/20 loss: 0.2371\n",
            "Epoch 20/20 loss: 0.2361\n",
            "batch size: 128, learning rate: 0.1, num epochs: 50\n",
            "Epoch 1/50 loss: 0.8409\n",
            "Epoch 2/50 loss: 0.4797\n",
            "Epoch 3/50 loss: 0.3705\n",
            "Epoch 4/50 loss: 0.3221\n",
            "Epoch 5/50 loss: 0.2983\n",
            "Epoch 6/50 loss: 0.2810\n",
            "Epoch 7/50 loss: 0.2724\n",
            "Epoch 8/50 loss: 0.2630\n",
            "Epoch 9/50 loss: 0.2585\n",
            "Epoch 10/50 loss: 0.2524\n",
            "Epoch 11/50 loss: 0.2501\n",
            "Epoch 12/50 loss: 0.2446\n",
            "Epoch 13/50 loss: 0.2442\n",
            "Epoch 14/50 loss: 0.2389\n",
            "Epoch 15/50 loss: 0.2379\n",
            "Epoch 16/50 loss: 0.2352\n",
            "Epoch 17/50 loss: 0.2348\n",
            "Epoch 18/50 loss: 0.2371\n",
            "Epoch 19/50 loss: 0.2318\n",
            "Epoch 20/50 loss: 0.2291\n",
            "Epoch 21/50 loss: 0.2317\n",
            "Epoch 22/50 loss: 0.2298\n",
            "Epoch 23/50 loss: 0.2289\n",
            "Epoch 24/50 loss: 0.2265\n",
            "Epoch 25/50 loss: 0.2247\n",
            "Epoch 26/50 loss: 0.2243\n",
            "Epoch 27/50 loss: 0.2257\n",
            "Epoch 28/50 loss: 0.2219\n",
            "Epoch 29/50 loss: 0.2212\n",
            "Epoch 30/50 loss: 0.2236\n",
            "Epoch 31/50 loss: 0.2190\n",
            "Epoch 32/50 loss: 0.2203\n",
            "Epoch 33/50 loss: 0.2186\n",
            "Epoch 34/50 loss: 0.2199\n",
            "Epoch 35/50 loss: 0.2210\n",
            "Epoch 36/50 loss: 0.2181\n",
            "Epoch 37/50 loss: 0.2154\n",
            "Epoch 38/50 loss: 0.2151\n",
            "Epoch 39/50 loss: 0.2185\n",
            "Epoch 40/50 loss: 0.2151\n",
            "Epoch 41/50 loss: 0.2147\n",
            "Epoch 42/50 loss: 0.2141\n",
            "Epoch 43/50 loss: 0.2135\n",
            "Epoch 44/50 loss: 0.2145\n",
            "Epoch 45/50 loss: 0.2131\n",
            "Epoch 46/50 loss: 0.2120\n",
            "Epoch 47/50 loss: 0.2125\n",
            "Epoch 48/50 loss: 0.2115\n",
            "Epoch 49/50 loss: 0.2114\n",
            "Epoch 50/50 loss: 0.2122\n",
            "batch size: 128, learning rate: 0.1, num epochs: 100\n",
            "Epoch 1/100 loss: 0.9909\n",
            "Epoch 2/100 loss: 0.4956\n",
            "Epoch 3/100 loss: 0.3689\n",
            "Epoch 4/100 loss: 0.3190\n",
            "Epoch 5/100 loss: 0.2951\n",
            "Epoch 6/100 loss: 0.2793\n",
            "Epoch 7/100 loss: 0.2657\n",
            "Epoch 8/100 loss: 0.2606\n",
            "Epoch 9/100 loss: 0.2545\n",
            "Epoch 10/100 loss: 0.2486\n",
            "Epoch 11/100 loss: 0.2475\n",
            "Epoch 12/100 loss: 0.2466\n",
            "Epoch 13/100 loss: 0.2409\n",
            "Epoch 14/100 loss: 0.2383\n",
            "Epoch 15/100 loss: 0.2390\n",
            "Epoch 16/100 loss: 0.2339\n",
            "Epoch 17/100 loss: 0.2328\n",
            "Epoch 18/100 loss: 0.2308\n",
            "Epoch 19/100 loss: 0.2324\n",
            "Epoch 20/100 loss: 0.2286\n",
            "Epoch 21/100 loss: 0.2273\n",
            "Epoch 22/100 loss: 0.2263\n",
            "Epoch 23/100 loss: 0.2288\n",
            "Epoch 24/100 loss: 0.2293\n",
            "Epoch 25/100 loss: 0.2240\n",
            "Epoch 26/100 loss: 0.2230\n",
            "Epoch 27/100 loss: 0.2225\n",
            "Epoch 28/100 loss: 0.2236\n",
            "Epoch 29/100 loss: 0.2237\n",
            "Epoch 30/100 loss: 0.2212\n",
            "Epoch 31/100 loss: 0.2210\n",
            "Epoch 32/100 loss: 0.2206\n",
            "Epoch 33/100 loss: 0.2219\n",
            "Epoch 34/100 loss: 0.2196\n",
            "Epoch 35/100 loss: 0.2187\n",
            "Epoch 36/100 loss: 0.2178\n",
            "Epoch 37/100 loss: 0.2185\n",
            "Epoch 38/100 loss: 0.2200\n",
            "Epoch 39/100 loss: 0.2190\n",
            "Epoch 40/100 loss: 0.2156\n",
            "Epoch 41/100 loss: 0.2150\n",
            "Epoch 42/100 loss: 0.2194\n",
            "Epoch 43/100 loss: 0.2152\n",
            "Epoch 44/100 loss: 0.2139\n",
            "Epoch 45/100 loss: 0.2180\n",
            "Epoch 46/100 loss: 0.2134\n",
            "Epoch 47/100 loss: 0.2157\n",
            "Epoch 48/100 loss: 0.2125\n",
            "Epoch 49/100 loss: 0.2119\n",
            "Epoch 50/100 loss: 0.2137\n",
            "Epoch 51/100 loss: 0.2119\n",
            "Epoch 52/100 loss: 0.2127\n",
            "Epoch 53/100 loss: 0.2142\n",
            "Epoch 54/100 loss: 0.2123\n",
            "Epoch 55/100 loss: 0.2104\n",
            "Epoch 56/100 loss: 0.2103\n",
            "Epoch 57/100 loss: 0.2105\n",
            "Epoch 58/100 loss: 0.2140\n",
            "Epoch 59/100 loss: 0.2092\n",
            "Epoch 60/100 loss: 0.2110\n",
            "Epoch 61/100 loss: 0.2090\n",
            "Epoch 62/100 loss: 0.2095\n",
            "Epoch 63/100 loss: 0.2096\n",
            "Epoch 64/100 loss: 0.2097\n",
            "Epoch 65/100 loss: 0.2088\n",
            "Epoch 66/100 loss: 0.2075\n",
            "Epoch 67/100 loss: 0.2079\n",
            "Epoch 68/100 loss: 0.2102\n",
            "Epoch 69/100 loss: 0.2070\n",
            "Epoch 70/100 loss: 0.2069\n",
            "Epoch 71/100 loss: 0.2075\n",
            "Epoch 72/100 loss: 0.2074\n",
            "Epoch 73/100 loss: 0.2086\n",
            "Epoch 74/100 loss: 0.2083\n",
            "Epoch 75/100 loss: 0.2076\n",
            "Epoch 76/100 loss: 0.2082\n",
            "Epoch 77/100 loss: 0.2059\n",
            "Epoch 78/100 loss: 0.2088\n",
            "Epoch 79/100 loss: 0.2067\n",
            "Epoch 80/100 loss: 0.2056\n",
            "Epoch 81/100 loss: 0.2065\n",
            "Epoch 82/100 loss: 0.2049\n",
            "Epoch 83/100 loss: 0.2085\n",
            "Epoch 84/100 loss: 0.2043\n",
            "Epoch 85/100 loss: 0.2045\n",
            "Epoch 86/100 loss: 0.2051\n",
            "Epoch 87/100 loss: 0.2095\n",
            "Epoch 88/100 loss: 0.2049\n",
            "Epoch 89/100 loss: 0.2061\n",
            "Epoch 90/100 loss: 0.2060\n",
            "Epoch 91/100 loss: 0.2068\n",
            "Epoch 92/100 loss: 0.2033\n",
            "Epoch 93/100 loss: 0.2039\n",
            "Epoch 94/100 loss: 0.2037\n",
            "Epoch 95/100 loss: 0.2030\n",
            "Epoch 96/100 loss: 0.2027\n",
            "Epoch 97/100 loss: 0.2037\n",
            "Epoch 98/100 loss: 0.2089\n",
            "Epoch 99/100 loss: 0.2045\n",
            "Epoch 100/100 loss: 0.2022\n",
            "batch size: 128, learning rate: 0.1, num epochs: 200\n",
            "Epoch 1/200 loss: 0.8797\n",
            "Epoch 2/200 loss: 0.4940\n",
            "Epoch 3/200 loss: 0.3751\n",
            "Epoch 4/200 loss: 0.3259\n",
            "Epoch 5/200 loss: 0.2998\n",
            "Epoch 6/200 loss: 0.2862\n",
            "Epoch 7/200 loss: 0.2755\n",
            "Epoch 8/200 loss: 0.2660\n",
            "Epoch 9/200 loss: 0.2613\n",
            "Epoch 10/200 loss: 0.2594\n",
            "Epoch 11/200 loss: 0.2540\n",
            "Epoch 12/200 loss: 0.2517\n",
            "Epoch 13/200 loss: 0.2480\n",
            "Epoch 14/200 loss: 0.2496\n",
            "Epoch 15/200 loss: 0.2521\n",
            "Epoch 16/200 loss: 0.2421\n",
            "Epoch 17/200 loss: 0.2405\n",
            "Epoch 18/200 loss: 0.2409\n",
            "Epoch 19/200 loss: 0.2380\n",
            "Epoch 20/200 loss: 0.2370\n",
            "Epoch 21/200 loss: 0.2351\n",
            "Epoch 22/200 loss: 0.2346\n",
            "Epoch 23/200 loss: 0.2344\n",
            "Epoch 24/200 loss: 0.2327\n",
            "Epoch 25/200 loss: 0.2304\n",
            "Epoch 26/200 loss: 0.2332\n",
            "Epoch 27/200 loss: 0.2305\n",
            "Epoch 28/200 loss: 0.2283\n",
            "Epoch 29/200 loss: 0.2316\n",
            "Epoch 30/200 loss: 0.2284\n",
            "Epoch 31/200 loss: 0.2284\n",
            "Epoch 32/200 loss: 0.2275\n",
            "Epoch 33/200 loss: 0.2252\n",
            "Epoch 34/200 loss: 0.2255\n",
            "Epoch 35/200 loss: 0.2254\n",
            "Epoch 36/200 loss: 0.2227\n",
            "Epoch 37/200 loss: 0.2232\n",
            "Epoch 38/200 loss: 0.2240\n",
            "Epoch 39/200 loss: 0.2292\n",
            "Epoch 40/200 loss: 0.2206\n",
            "Epoch 41/200 loss: 0.2223\n",
            "Epoch 42/200 loss: 0.2209\n",
            "Epoch 43/200 loss: 0.2194\n",
            "Epoch 44/200 loss: 0.2198\n",
            "Epoch 45/200 loss: 0.2215\n",
            "Epoch 46/200 loss: 0.2195\n",
            "Epoch 47/200 loss: 0.2178\n",
            "Epoch 48/200 loss: 0.2170\n",
            "Epoch 49/200 loss: 0.2183\n",
            "Epoch 50/200 loss: 0.2162\n",
            "Epoch 51/200 loss: 0.2202\n",
            "Epoch 52/200 loss: 0.2195\n",
            "Epoch 53/200 loss: 0.2147\n",
            "Epoch 54/200 loss: 0.2155\n",
            "Epoch 55/200 loss: 0.2169\n",
            "Epoch 56/200 loss: 0.2173\n",
            "Epoch 57/200 loss: 0.2168\n",
            "Epoch 58/200 loss: 0.2153\n",
            "Epoch 59/200 loss: 0.2133\n",
            "Epoch 60/200 loss: 0.2144\n",
            "Epoch 61/200 loss: 0.2125\n",
            "Epoch 62/200 loss: 0.2170\n",
            "Epoch 63/200 loss: 0.2117\n",
            "Epoch 64/200 loss: 0.2124\n",
            "Epoch 65/200 loss: 0.2125\n",
            "Epoch 66/200 loss: 0.2150\n",
            "Epoch 67/200 loss: 0.2138\n",
            "Epoch 68/200 loss: 0.2111\n",
            "Epoch 69/200 loss: 0.2124\n",
            "Epoch 70/200 loss: 0.2098\n",
            "Epoch 71/200 loss: 0.2115\n",
            "Epoch 72/200 loss: 0.2112\n",
            "Epoch 73/200 loss: 0.2106\n",
            "Epoch 74/200 loss: 0.2104\n",
            "Epoch 75/200 loss: 0.2092\n",
            "Epoch 76/200 loss: 0.2103\n",
            "Epoch 77/200 loss: 0.2103\n",
            "Epoch 78/200 loss: 0.2096\n",
            "Epoch 79/200 loss: 0.2087\n",
            "Epoch 80/200 loss: 0.2089\n",
            "Epoch 81/200 loss: 0.2117\n",
            "Epoch 82/200 loss: 0.2092\n",
            "Epoch 83/200 loss: 0.2093\n",
            "Epoch 84/200 loss: 0.2075\n",
            "Epoch 85/200 loss: 0.2089\n",
            "Epoch 86/200 loss: 0.2097\n",
            "Epoch 87/200 loss: 0.2118\n",
            "Epoch 88/200 loss: 0.2079\n",
            "Epoch 89/200 loss: 0.2085\n",
            "Epoch 90/200 loss: 0.2082\n",
            "Epoch 91/200 loss: 0.2096\n",
            "Epoch 92/200 loss: 0.2072\n",
            "Epoch 93/200 loss: 0.2060\n",
            "Epoch 94/200 loss: 0.2055\n",
            "Epoch 95/200 loss: 0.2053\n",
            "Epoch 96/200 loss: 0.2051\n",
            "Epoch 97/200 loss: 0.2081\n",
            "Epoch 98/200 loss: 0.2051\n",
            "Epoch 99/200 loss: 0.2063\n",
            "Epoch 100/200 loss: 0.2050\n",
            "Epoch 101/200 loss: 0.2048\n",
            "Epoch 102/200 loss: 0.2075\n",
            "Epoch 103/200 loss: 0.2043\n",
            "Epoch 104/200 loss: 0.2051\n",
            "Epoch 105/200 loss: 0.2063\n",
            "Epoch 106/200 loss: 0.2047\n",
            "Epoch 107/200 loss: 0.2049\n",
            "Epoch 108/200 loss: 0.2072\n",
            "Epoch 109/200 loss: 0.2041\n",
            "Epoch 110/200 loss: 0.2061\n",
            "Epoch 111/200 loss: 0.2038\n",
            "Epoch 112/200 loss: 0.2050\n",
            "Epoch 113/200 loss: 0.2068\n",
            "Epoch 114/200 loss: 0.2042\n",
            "Epoch 115/200 loss: 0.2028\n",
            "Epoch 116/200 loss: 0.2026\n",
            "Epoch 117/200 loss: 0.2040\n",
            "Epoch 118/200 loss: 0.2030\n",
            "Epoch 119/200 loss: 0.2045\n",
            "Epoch 120/200 loss: 0.2033\n",
            "Epoch 121/200 loss: 0.2040\n",
            "Epoch 122/200 loss: 0.2022\n",
            "Epoch 123/200 loss: 0.2047\n",
            "Epoch 124/200 loss: 0.2046\n",
            "Epoch 125/200 loss: 0.2061\n",
            "Epoch 126/200 loss: 0.2019\n",
            "Epoch 127/200 loss: 0.2048\n",
            "Epoch 128/200 loss: 0.2044\n",
            "Epoch 129/200 loss: 0.2024\n",
            "Epoch 130/200 loss: 0.2026\n",
            "Epoch 131/200 loss: 0.2032\n",
            "Epoch 132/200 loss: 0.2043\n",
            "Epoch 133/200 loss: 0.2042\n",
            "Epoch 134/200 loss: 0.2027\n",
            "Epoch 135/200 loss: 0.2017\n",
            "Epoch 136/200 loss: 0.2026\n",
            "Epoch 137/200 loss: 0.2022\n",
            "Epoch 138/200 loss: 0.2017\n",
            "Epoch 139/200 loss: 0.2007\n",
            "Epoch 140/200 loss: 0.2024\n",
            "Epoch 141/200 loss: 0.2012\n",
            "Epoch 142/200 loss: 0.2013\n",
            "Epoch 143/200 loss: 0.2009\n",
            "Epoch 144/200 loss: 0.2028\n",
            "Epoch 145/200 loss: 0.2003\n",
            "Epoch 146/200 loss: 0.2001\n",
            "Epoch 147/200 loss: 0.2001\n",
            "Epoch 148/200 loss: 0.1998\n",
            "Epoch 149/200 loss: 0.2042\n",
            "Epoch 150/200 loss: 0.2002\n",
            "Epoch 151/200 loss: 0.2005\n",
            "Epoch 152/200 loss: 0.2032\n",
            "Epoch 153/200 loss: 0.2042\n",
            "Epoch 154/200 loss: 0.2010\n",
            "Epoch 155/200 loss: 0.1998\n",
            "Epoch 156/200 loss: 0.2041\n",
            "Epoch 157/200 loss: 0.1998\n",
            "Epoch 158/200 loss: 0.2009\n",
            "Epoch 159/200 loss: 0.2004\n",
            "Epoch 160/200 loss: 0.2014\n",
            "Epoch 161/200 loss: 0.2008\n",
            "Epoch 162/200 loss: 0.1989\n",
            "Epoch 163/200 loss: 0.2004\n",
            "Epoch 164/200 loss: 0.2026\n",
            "Epoch 165/200 loss: 0.1990\n",
            "Epoch 166/200 loss: 0.2005\n",
            "Epoch 167/200 loss: 0.1989\n",
            "Epoch 168/200 loss: 0.1994\n",
            "Epoch 169/200 loss: 0.1999\n",
            "Epoch 170/200 loss: 0.2037\n",
            "Epoch 171/200 loss: 0.2000\n",
            "Epoch 172/200 loss: 0.1988\n",
            "Epoch 173/200 loss: 0.1989\n",
            "Epoch 174/200 loss: 0.1993\n",
            "Epoch 175/200 loss: 0.1996\n",
            "Epoch 176/200 loss: 0.1987\n",
            "Epoch 177/200 loss: 0.1994\n",
            "Epoch 178/200 loss: 0.2010\n",
            "Epoch 179/200 loss: 0.1980\n",
            "Epoch 180/200 loss: 0.1985\n",
            "Epoch 181/200 loss: 0.2008\n",
            "Epoch 182/200 loss: 0.1997\n",
            "Epoch 183/200 loss: 0.1979\n",
            "Epoch 184/200 loss: 0.2011\n",
            "Epoch 185/200 loss: 0.2000\n",
            "Epoch 186/200 loss: 0.1992\n",
            "Epoch 187/200 loss: 0.1998\n",
            "Epoch 188/200 loss: 0.1985\n",
            "Epoch 189/200 loss: 0.1976\n",
            "Epoch 190/200 loss: 0.1974\n",
            "Epoch 191/200 loss: 0.2002\n",
            "Epoch 192/200 loss: 0.1986\n",
            "Epoch 193/200 loss: 0.1977\n",
            "Epoch 194/200 loss: 0.1983\n",
            "Epoch 195/200 loss: 0.1973\n",
            "Epoch 196/200 loss: 0.1982\n",
            "Epoch 197/200 loss: 0.1984\n",
            "Epoch 198/200 loss: 0.1990\n",
            "Epoch 199/200 loss: 0.1991\n",
            "Epoch 200/200 loss: 0.1992\n",
            "batch size: 128, learning rate: 0.05, num epochs: 20\n",
            "Epoch 1/20 loss: 1.2604\n",
            "Epoch 2/20 loss: 0.8192\n",
            "Epoch 3/20 loss: 0.6085\n",
            "Epoch 4/20 loss: 0.4910\n",
            "Epoch 5/20 loss: 0.4225\n",
            "Epoch 6/20 loss: 0.3823\n",
            "Epoch 7/20 loss: 0.3515\n",
            "Epoch 8/20 loss: 0.3349\n",
            "Epoch 9/20 loss: 0.3180\n",
            "Epoch 10/20 loss: 0.3053\n",
            "Epoch 11/20 loss: 0.2966\n",
            "Epoch 12/20 loss: 0.2888\n",
            "Epoch 13/20 loss: 0.2867\n",
            "Epoch 14/20 loss: 0.2810\n",
            "Epoch 15/20 loss: 0.2761\n",
            "Epoch 16/20 loss: 0.2710\n",
            "Epoch 17/20 loss: 0.2693\n",
            "Epoch 18/20 loss: 0.2643\n",
            "Epoch 19/20 loss: 0.2656\n",
            "Epoch 20/20 loss: 0.2654\n",
            "batch size: 128, learning rate: 0.05, num epochs: 50\n",
            "Epoch 1/50 loss: 1.4654\n",
            "Epoch 2/50 loss: 0.8968\n",
            "Epoch 3/50 loss: 0.6279\n",
            "Epoch 4/50 loss: 0.4905\n",
            "Epoch 5/50 loss: 0.4205\n",
            "Epoch 6/50 loss: 0.3731\n",
            "Epoch 7/50 loss: 0.3489\n",
            "Epoch 8/50 loss: 0.3299\n",
            "Epoch 9/50 loss: 0.3145\n",
            "Epoch 10/50 loss: 0.3034\n",
            "Epoch 11/50 loss: 0.2975\n",
            "Epoch 12/50 loss: 0.2890\n",
            "Epoch 13/50 loss: 0.2870\n",
            "Epoch 14/50 loss: 0.2828\n",
            "Epoch 15/50 loss: 0.2779\n",
            "Epoch 16/50 loss: 0.2726\n",
            "Epoch 17/50 loss: 0.2710\n",
            "Epoch 18/50 loss: 0.2662\n",
            "Epoch 19/50 loss: 0.2635\n",
            "Epoch 20/50 loss: 0.2592\n",
            "Epoch 21/50 loss: 0.2600\n",
            "Epoch 22/50 loss: 0.2554\n",
            "Epoch 23/50 loss: 0.2556\n",
            "Epoch 24/50 loss: 0.2535\n",
            "Epoch 25/50 loss: 0.2513\n",
            "Epoch 26/50 loss: 0.2480\n",
            "Epoch 27/50 loss: 0.2492\n",
            "Epoch 28/50 loss: 0.2462\n",
            "Epoch 29/50 loss: 0.2443\n",
            "Epoch 30/50 loss: 0.2487\n",
            "Epoch 31/50 loss: 0.2448\n",
            "Epoch 32/50 loss: 0.2433\n",
            "Epoch 33/50 loss: 0.2412\n",
            "Epoch 34/50 loss: 0.2398\n",
            "Epoch 35/50 loss: 0.2392\n",
            "Epoch 36/50 loss: 0.2364\n",
            "Epoch 37/50 loss: 0.2381\n",
            "Epoch 38/50 loss: 0.2353\n",
            "Epoch 39/50 loss: 0.2354\n",
            "Epoch 40/50 loss: 0.2330\n",
            "Epoch 41/50 loss: 0.2358\n",
            "Epoch 42/50 loss: 0.2335\n",
            "Epoch 43/50 loss: 0.2353\n",
            "Epoch 44/50 loss: 0.2320\n",
            "Epoch 45/50 loss: 0.2361\n",
            "Epoch 46/50 loss: 0.2328\n",
            "Epoch 47/50 loss: 0.2306\n",
            "Epoch 48/50 loss: 0.2319\n",
            "Epoch 49/50 loss: 0.2279\n",
            "Epoch 50/50 loss: 0.2280\n",
            "batch size: 128, learning rate: 0.05, num epochs: 100\n",
            "Epoch 1/100 loss: 1.2774\n",
            "Epoch 2/100 loss: 0.8218\n",
            "Epoch 3/100 loss: 0.5806\n",
            "Epoch 4/100 loss: 0.4655\n",
            "Epoch 5/100 loss: 0.3915\n",
            "Epoch 6/100 loss: 0.3519\n",
            "Epoch 7/100 loss: 0.3247\n",
            "Epoch 8/100 loss: 0.3094\n",
            "Epoch 9/100 loss: 0.2962\n",
            "Epoch 10/100 loss: 0.2906\n",
            "Epoch 11/100 loss: 0.2807\n",
            "Epoch 12/100 loss: 0.2750\n",
            "Epoch 13/100 loss: 0.2696\n",
            "Epoch 14/100 loss: 0.2672\n",
            "Epoch 15/100 loss: 0.2657\n",
            "Epoch 16/100 loss: 0.2615\n",
            "Epoch 17/100 loss: 0.2588\n",
            "Epoch 18/100 loss: 0.2570\n",
            "Epoch 19/100 loss: 0.2545\n",
            "Epoch 20/100 loss: 0.2546\n",
            "Epoch 21/100 loss: 0.2524\n",
            "Epoch 22/100 loss: 0.2510\n",
            "Epoch 23/100 loss: 0.2496\n",
            "Epoch 24/100 loss: 0.2468\n",
            "Epoch 25/100 loss: 0.2468\n",
            "Epoch 26/100 loss: 0.2465\n",
            "Epoch 27/100 loss: 0.2473\n",
            "Epoch 28/100 loss: 0.2428\n",
            "Epoch 29/100 loss: 0.2475\n",
            "Epoch 30/100 loss: 0.2408\n",
            "Epoch 31/100 loss: 0.2402\n",
            "Epoch 32/100 loss: 0.2405\n",
            "Epoch 33/100 loss: 0.2381\n",
            "Epoch 34/100 loss: 0.2414\n",
            "Epoch 35/100 loss: 0.2378\n",
            "Epoch 36/100 loss: 0.2360\n",
            "Epoch 37/100 loss: 0.2353\n",
            "Epoch 38/100 loss: 0.2373\n",
            "Epoch 39/100 loss: 0.2340\n",
            "Epoch 40/100 loss: 0.2357\n",
            "Epoch 41/100 loss: 0.2334\n",
            "Epoch 42/100 loss: 0.2337\n",
            "Epoch 43/100 loss: 0.2340\n",
            "Epoch 44/100 loss: 0.2357\n",
            "Epoch 45/100 loss: 0.2321\n",
            "Epoch 46/100 loss: 0.2305\n",
            "Epoch 47/100 loss: 0.2307\n",
            "Epoch 48/100 loss: 0.2296\n",
            "Epoch 49/100 loss: 0.2344\n",
            "Epoch 50/100 loss: 0.2302\n",
            "Epoch 51/100 loss: 0.2302\n",
            "Epoch 52/100 loss: 0.2296\n",
            "Epoch 53/100 loss: 0.2273\n",
            "Epoch 54/100 loss: 0.2270\n",
            "Epoch 55/100 loss: 0.2308\n",
            "Epoch 56/100 loss: 0.2272\n",
            "Epoch 57/100 loss: 0.2256\n",
            "Epoch 58/100 loss: 0.2256\n",
            "Epoch 59/100 loss: 0.2254\n",
            "Epoch 60/100 loss: 0.2250\n",
            "Epoch 61/100 loss: 0.2301\n",
            "Epoch 62/100 loss: 0.2248\n",
            "Epoch 63/100 loss: 0.2239\n",
            "Epoch 64/100 loss: 0.2238\n",
            "Epoch 65/100 loss: 0.2243\n",
            "Epoch 66/100 loss: 0.2253\n",
            "Epoch 67/100 loss: 0.2228\n",
            "Epoch 68/100 loss: 0.2254\n",
            "Epoch 69/100 loss: 0.2227\n",
            "Epoch 70/100 loss: 0.2236\n",
            "Epoch 71/100 loss: 0.2213\n",
            "Epoch 72/100 loss: 0.2226\n",
            "Epoch 73/100 loss: 0.2249\n",
            "Epoch 74/100 loss: 0.2221\n",
            "Epoch 75/100 loss: 0.2222\n",
            "Epoch 76/100 loss: 0.2208\n",
            "Epoch 77/100 loss: 0.2211\n",
            "Epoch 78/100 loss: 0.2227\n",
            "Epoch 79/100 loss: 0.2207\n",
            "Epoch 80/100 loss: 0.2208\n",
            "Epoch 81/100 loss: 0.2205\n",
            "Epoch 82/100 loss: 0.2202\n",
            "Epoch 83/100 loss: 0.2195\n",
            "Epoch 84/100 loss: 0.2209\n",
            "Epoch 85/100 loss: 0.2191\n",
            "Epoch 86/100 loss: 0.2224\n",
            "Epoch 87/100 loss: 0.2202\n",
            "Epoch 88/100 loss: 0.2183\n",
            "Epoch 89/100 loss: 0.2180\n",
            "Epoch 90/100 loss: 0.2186\n",
            "Epoch 91/100 loss: 0.2169\n",
            "Epoch 92/100 loss: 0.2177\n",
            "Epoch 93/100 loss: 0.2163\n",
            "Epoch 94/100 loss: 0.2190\n",
            "Epoch 95/100 loss: 0.2162\n",
            "Epoch 96/100 loss: 0.2167\n",
            "Epoch 97/100 loss: 0.2159\n",
            "Epoch 98/100 loss: 0.2175\n",
            "Epoch 99/100 loss: 0.2162\n",
            "Epoch 100/100 loss: 0.2161\n",
            "batch size: 128, learning rate: 0.05, num epochs: 200\n",
            "Epoch 1/200 loss: 1.0129\n",
            "Epoch 2/200 loss: 0.6917\n",
            "Epoch 3/200 loss: 0.5361\n",
            "Epoch 4/200 loss: 0.4476\n",
            "Epoch 5/200 loss: 0.3950\n",
            "Epoch 6/200 loss: 0.3691\n",
            "Epoch 7/200 loss: 0.3361\n",
            "Epoch 8/200 loss: 0.3205\n",
            "Epoch 9/200 loss: 0.3083\n",
            "Epoch 10/200 loss: 0.2996\n",
            "Epoch 11/200 loss: 0.2909\n",
            "Epoch 12/200 loss: 0.2838\n",
            "Epoch 13/200 loss: 0.2825\n",
            "Epoch 14/200 loss: 0.2784\n",
            "Epoch 15/200 loss: 0.2744\n",
            "Epoch 16/200 loss: 0.2676\n",
            "Epoch 17/200 loss: 0.2650\n",
            "Epoch 18/200 loss: 0.2632\n",
            "Epoch 19/200 loss: 0.2600\n",
            "Epoch 20/200 loss: 0.2607\n",
            "Epoch 21/200 loss: 0.2561\n",
            "Epoch 22/200 loss: 0.2550\n",
            "Epoch 23/200 loss: 0.2537\n",
            "Epoch 24/200 loss: 0.2528\n",
            "Epoch 25/200 loss: 0.2502\n",
            "Epoch 26/200 loss: 0.2514\n",
            "Epoch 27/200 loss: 0.2469\n",
            "Epoch 28/200 loss: 0.2486\n",
            "Epoch 29/200 loss: 0.2484\n",
            "Epoch 30/200 loss: 0.2494\n",
            "Epoch 31/200 loss: 0.2435\n",
            "Epoch 32/200 loss: 0.2423\n",
            "Epoch 33/200 loss: 0.2423\n",
            "Epoch 34/200 loss: 0.2430\n",
            "Epoch 35/200 loss: 0.2409\n",
            "Epoch 36/200 loss: 0.2413\n",
            "Epoch 37/200 loss: 0.2399\n",
            "Epoch 38/200 loss: 0.2390\n",
            "Epoch 39/200 loss: 0.2362\n",
            "Epoch 40/200 loss: 0.2371\n",
            "Epoch 41/200 loss: 0.2348\n",
            "Epoch 42/200 loss: 0.2344\n",
            "Epoch 43/200 loss: 0.2344\n",
            "Epoch 44/200 loss: 0.2340\n",
            "Epoch 45/200 loss: 0.2388\n",
            "Epoch 46/200 loss: 0.2341\n",
            "Epoch 47/200 loss: 0.2311\n",
            "Epoch 48/200 loss: 0.2307\n",
            "Epoch 49/200 loss: 0.2319\n",
            "Epoch 50/200 loss: 0.2324\n",
            "Epoch 51/200 loss: 0.2325\n",
            "Epoch 52/200 loss: 0.2285\n",
            "Epoch 53/200 loss: 0.2320\n",
            "Epoch 54/200 loss: 0.2282\n",
            "Epoch 55/200 loss: 0.2271\n",
            "Epoch 56/200 loss: 0.2273\n",
            "Epoch 57/200 loss: 0.2330\n",
            "Epoch 58/200 loss: 0.2270\n",
            "Epoch 59/200 loss: 0.2289\n",
            "Epoch 60/200 loss: 0.2272\n",
            "Epoch 61/200 loss: 0.2279\n",
            "Epoch 62/200 loss: 0.2253\n",
            "Epoch 63/200 loss: 0.2298\n",
            "Epoch 64/200 loss: 0.2244\n",
            "Epoch 65/200 loss: 0.2246\n",
            "Epoch 66/200 loss: 0.2232\n",
            "Epoch 67/200 loss: 0.2279\n",
            "Epoch 68/200 loss: 0.2253\n",
            "Epoch 69/200 loss: 0.2242\n",
            "Epoch 70/200 loss: 0.2226\n",
            "Epoch 71/200 loss: 0.2241\n",
            "Epoch 72/200 loss: 0.2219\n",
            "Epoch 73/200 loss: 0.2228\n",
            "Epoch 74/200 loss: 0.2275\n",
            "Epoch 75/200 loss: 0.2223\n",
            "Epoch 76/200 loss: 0.2204\n",
            "Epoch 77/200 loss: 0.2208\n",
            "Epoch 78/200 loss: 0.2195\n",
            "Epoch 79/200 loss: 0.2216\n",
            "Epoch 80/200 loss: 0.2201\n",
            "Epoch 81/200 loss: 0.2198\n",
            "Epoch 82/200 loss: 0.2219\n",
            "Epoch 83/200 loss: 0.2201\n",
            "Epoch 84/200 loss: 0.2206\n",
            "Epoch 85/200 loss: 0.2182\n",
            "Epoch 86/200 loss: 0.2181\n",
            "Epoch 87/200 loss: 0.2182\n",
            "Epoch 88/200 loss: 0.2172\n",
            "Epoch 89/200 loss: 0.2192\n",
            "Epoch 90/200 loss: 0.2172\n",
            "Epoch 91/200 loss: 0.2177\n",
            "Epoch 92/200 loss: 0.2199\n",
            "Epoch 93/200 loss: 0.2163\n",
            "Epoch 94/200 loss: 0.2195\n",
            "Epoch 95/200 loss: 0.2163\n",
            "Epoch 96/200 loss: 0.2174\n",
            "Epoch 97/200 loss: 0.2166\n",
            "Epoch 98/200 loss: 0.2180\n",
            "Epoch 99/200 loss: 0.2169\n",
            "Epoch 100/200 loss: 0.2150\n",
            "Epoch 101/200 loss: 0.2159\n",
            "Epoch 102/200 loss: 0.2153\n",
            "Epoch 103/200 loss: 0.2166\n",
            "Epoch 104/200 loss: 0.2147\n",
            "Epoch 105/200 loss: 0.2149\n",
            "Epoch 106/200 loss: 0.2170\n",
            "Epoch 107/200 loss: 0.2167\n",
            "Epoch 108/200 loss: 0.2155\n",
            "Epoch 109/200 loss: 0.2160\n",
            "Epoch 110/200 loss: 0.2140\n",
            "Epoch 111/200 loss: 0.2163\n",
            "Epoch 112/200 loss: 0.2147\n",
            "Epoch 113/200 loss: 0.2130\n",
            "Epoch 114/200 loss: 0.2129\n",
            "Epoch 115/200 loss: 0.2137\n",
            "Epoch 116/200 loss: 0.2124\n",
            "Epoch 117/200 loss: 0.2127\n",
            "Epoch 118/200 loss: 0.2129\n",
            "Epoch 119/200 loss: 0.2158\n",
            "Epoch 120/200 loss: 0.2114\n",
            "Epoch 121/200 loss: 0.2126\n",
            "Epoch 122/200 loss: 0.2146\n",
            "Epoch 123/200 loss: 0.2116\n",
            "Epoch 124/200 loss: 0.2122\n",
            "Epoch 125/200 loss: 0.2131\n",
            "Epoch 126/200 loss: 0.2108\n",
            "Epoch 127/200 loss: 0.2126\n",
            "Epoch 128/200 loss: 0.2117\n",
            "Epoch 129/200 loss: 0.2102\n",
            "Epoch 130/200 loss: 0.2115\n",
            "Epoch 131/200 loss: 0.2099\n",
            "Epoch 132/200 loss: 0.2127\n",
            "Epoch 133/200 loss: 0.2110\n",
            "Epoch 134/200 loss: 0.2116\n",
            "Epoch 135/200 loss: 0.2098\n",
            "Epoch 136/200 loss: 0.2101\n",
            "Epoch 137/200 loss: 0.2105\n",
            "Epoch 138/200 loss: 0.2115\n",
            "Epoch 139/200 loss: 0.2091\n",
            "Epoch 140/200 loss: 0.2111\n",
            "Epoch 141/200 loss: 0.2118\n",
            "Epoch 142/200 loss: 0.2086\n",
            "Epoch 143/200 loss: 0.2102\n",
            "Epoch 144/200 loss: 0.2116\n",
            "Epoch 145/200 loss: 0.2092\n",
            "Epoch 146/200 loss: 0.2093\n",
            "Epoch 147/200 loss: 0.2085\n",
            "Epoch 148/200 loss: 0.2095\n",
            "Epoch 149/200 loss: 0.2104\n",
            "Epoch 150/200 loss: 0.2081\n",
            "Epoch 151/200 loss: 0.2090\n",
            "Epoch 152/200 loss: 0.2079\n",
            "Epoch 153/200 loss: 0.2077\n",
            "Epoch 154/200 loss: 0.2085\n",
            "Epoch 155/200 loss: 0.2087\n",
            "Epoch 156/200 loss: 0.2090\n",
            "Epoch 157/200 loss: 0.2104\n",
            "Epoch 158/200 loss: 0.2074\n",
            "Epoch 159/200 loss: 0.2080\n",
            "Epoch 160/200 loss: 0.2071\n",
            "Epoch 161/200 loss: 0.2078\n",
            "Epoch 162/200 loss: 0.2071\n",
            "Epoch 163/200 loss: 0.2091\n",
            "Epoch 164/200 loss: 0.2088\n",
            "Epoch 165/200 loss: 0.2094\n",
            "Epoch 166/200 loss: 0.2064\n",
            "Epoch 167/200 loss: 0.2080\n",
            "Epoch 168/200 loss: 0.2058\n",
            "Epoch 169/200 loss: 0.2083\n",
            "Epoch 170/200 loss: 0.2087\n",
            "Epoch 171/200 loss: 0.2060\n",
            "Epoch 172/200 loss: 0.2062\n",
            "Epoch 173/200 loss: 0.2056\n",
            "Epoch 174/200 loss: 0.2058\n",
            "Epoch 175/200 loss: 0.2067\n",
            "Epoch 176/200 loss: 0.2056\n",
            "Epoch 177/200 loss: 0.2058\n",
            "Epoch 178/200 loss: 0.2094\n",
            "Epoch 179/200 loss: 0.2088\n",
            "Epoch 180/200 loss: 0.2057\n",
            "Epoch 181/200 loss: 0.2056\n",
            "Epoch 182/200 loss: 0.2055\n",
            "Epoch 183/200 loss: 0.2065\n",
            "Epoch 184/200 loss: 0.2049\n",
            "Epoch 185/200 loss: 0.2056\n",
            "Epoch 186/200 loss: 0.2054\n",
            "Epoch 187/200 loss: 0.2070\n",
            "Epoch 188/200 loss: 0.2077\n",
            "Epoch 189/200 loss: 0.2095\n",
            "Epoch 190/200 loss: 0.2051\n",
            "Epoch 191/200 loss: 0.2053\n",
            "Epoch 192/200 loss: 0.2067\n",
            "Epoch 193/200 loss: 0.2045\n",
            "Epoch 194/200 loss: 0.2044\n",
            "Epoch 195/200 loss: 0.2044\n",
            "Epoch 196/200 loss: 0.2043\n",
            "Epoch 197/200 loss: 0.2047\n",
            "Epoch 198/200 loss: 0.2059\n",
            "Epoch 199/200 loss: 0.2047\n",
            "Epoch 200/200 loss: 0.2054\n",
            "batch size: 128, learning rate: 0.01, num epochs: 20\n",
            "Epoch 1/20 loss: 1.4936\n",
            "Epoch 2/20 loss: 1.3330\n",
            "Epoch 3/20 loss: 1.1967\n",
            "Epoch 4/20 loss: 1.0909\n",
            "Epoch 5/20 loss: 0.9950\n",
            "Epoch 6/20 loss: 0.9087\n",
            "Epoch 7/20 loss: 0.8366\n",
            "Epoch 8/20 loss: 0.7732\n",
            "Epoch 9/20 loss: 0.7222\n",
            "Epoch 10/20 loss: 0.6621\n",
            "Epoch 11/20 loss: 0.6212\n",
            "Epoch 12/20 loss: 0.5816\n",
            "Epoch 13/20 loss: 0.5492\n",
            "Epoch 14/20 loss: 0.5181\n",
            "Epoch 15/20 loss: 0.4946\n",
            "Epoch 16/20 loss: 0.4701\n",
            "Epoch 17/20 loss: 0.4505\n",
            "Epoch 18/20 loss: 0.4342\n",
            "Epoch 19/20 loss: 0.4197\n",
            "Epoch 20/20 loss: 0.4047\n",
            "batch size: 128, learning rate: 0.01, num epochs: 50\n",
            "Epoch 1/50 loss: 0.9657\n",
            "Epoch 2/50 loss: 0.8834\n",
            "Epoch 3/50 loss: 0.8082\n",
            "Epoch 4/50 loss: 0.7467\n",
            "Epoch 5/50 loss: 0.6945\n",
            "Epoch 6/50 loss: 0.6518\n",
            "Epoch 7/50 loss: 0.6125\n",
            "Epoch 8/50 loss: 0.5778\n",
            "Epoch 9/50 loss: 0.5500\n",
            "Epoch 10/50 loss: 0.5284\n",
            "Epoch 11/50 loss: 0.5116\n",
            "Epoch 12/50 loss: 0.4941\n",
            "Epoch 13/50 loss: 0.4716\n",
            "Epoch 14/50 loss: 0.4592\n",
            "Epoch 15/50 loss: 0.4443\n",
            "Epoch 16/50 loss: 0.4335\n",
            "Epoch 17/50 loss: 0.4213\n",
            "Epoch 18/50 loss: 0.4134\n",
            "Epoch 19/50 loss: 0.4052\n",
            "Epoch 20/50 loss: 0.3957\n",
            "Epoch 21/50 loss: 0.3900\n",
            "Epoch 22/50 loss: 0.3797\n",
            "Epoch 23/50 loss: 0.3725\n",
            "Epoch 24/50 loss: 0.3664\n",
            "Epoch 25/50 loss: 0.3600\n",
            "Epoch 26/50 loss: 0.3572\n",
            "Epoch 27/50 loss: 0.3514\n",
            "Epoch 28/50 loss: 0.3461\n",
            "Epoch 29/50 loss: 0.3407\n",
            "Epoch 30/50 loss: 0.3372\n",
            "Epoch 31/50 loss: 0.3335\n",
            "Epoch 32/50 loss: 0.3336\n",
            "Epoch 33/50 loss: 0.3273\n",
            "Epoch 34/50 loss: 0.3277\n",
            "Epoch 35/50 loss: 0.3205\n",
            "Epoch 36/50 loss: 0.3196\n",
            "Epoch 37/50 loss: 0.3208\n",
            "Epoch 38/50 loss: 0.3173\n",
            "Epoch 39/50 loss: 0.3135\n",
            "Epoch 40/50 loss: 0.3097\n",
            "Epoch 41/50 loss: 0.3074\n",
            "Epoch 42/50 loss: 0.3074\n",
            "Epoch 43/50 loss: 0.3034\n",
            "Epoch 44/50 loss: 0.3051\n",
            "Epoch 45/50 loss: 0.3002\n",
            "Epoch 46/50 loss: 0.2990\n",
            "Epoch 47/50 loss: 0.2981\n",
            "Epoch 48/50 loss: 0.2941\n",
            "Epoch 49/50 loss: 0.2936\n",
            "Epoch 50/50 loss: 0.2941\n",
            "batch size: 128, learning rate: 0.01, num epochs: 100\n",
            "Epoch 1/100 loss: 1.8105\n",
            "Epoch 2/100 loss: 1.3470\n",
            "Epoch 3/100 loss: 1.1508\n",
            "Epoch 4/100 loss: 1.0296\n",
            "Epoch 5/100 loss: 0.9225\n",
            "Epoch 6/100 loss: 0.8357\n",
            "Epoch 7/100 loss: 0.7645\n",
            "Epoch 8/100 loss: 0.7062\n",
            "Epoch 9/100 loss: 0.6529\n",
            "Epoch 10/100 loss: 0.6086\n",
            "Epoch 11/100 loss: 0.5736\n",
            "Epoch 12/100 loss: 0.5398\n",
            "Epoch 13/100 loss: 0.5121\n",
            "Epoch 14/100 loss: 0.4909\n",
            "Epoch 15/100 loss: 0.4689\n",
            "Epoch 16/100 loss: 0.4539\n",
            "Epoch 17/100 loss: 0.4344\n",
            "Epoch 18/100 loss: 0.4217\n",
            "Epoch 19/100 loss: 0.4112\n",
            "Epoch 20/100 loss: 0.3966\n",
            "Epoch 21/100 loss: 0.3861\n",
            "Epoch 22/100 loss: 0.3777\n",
            "Epoch 23/100 loss: 0.3728\n",
            "Epoch 24/100 loss: 0.3653\n",
            "Epoch 25/100 loss: 0.3609\n",
            "Epoch 26/100 loss: 0.3528\n",
            "Epoch 27/100 loss: 0.3450\n",
            "Epoch 28/100 loss: 0.3399\n",
            "Epoch 29/100 loss: 0.3384\n",
            "Epoch 30/100 loss: 0.3333\n",
            "Epoch 31/100 loss: 0.3310\n",
            "Epoch 32/100 loss: 0.3231\n",
            "Epoch 33/100 loss: 0.3225\n",
            "Epoch 34/100 loss: 0.3175\n",
            "Epoch 35/100 loss: 0.3134\n",
            "Epoch 36/100 loss: 0.3134\n",
            "Epoch 37/100 loss: 0.3086\n",
            "Epoch 38/100 loss: 0.3084\n",
            "Epoch 39/100 loss: 0.3085\n",
            "Epoch 40/100 loss: 0.3059\n",
            "Epoch 41/100 loss: 0.3045\n",
            "Epoch 42/100 loss: 0.2992\n",
            "Epoch 43/100 loss: 0.2967\n",
            "Epoch 44/100 loss: 0.2955\n",
            "Epoch 45/100 loss: 0.2926\n",
            "Epoch 46/100 loss: 0.2922\n",
            "Epoch 47/100 loss: 0.2916\n",
            "Epoch 48/100 loss: 0.2875\n",
            "Epoch 49/100 loss: 0.2900\n",
            "Epoch 50/100 loss: 0.2859\n",
            "Epoch 51/100 loss: 0.2874\n",
            "Epoch 52/100 loss: 0.2840\n",
            "Epoch 53/100 loss: 0.2822\n",
            "Epoch 54/100 loss: 0.2825\n",
            "Epoch 55/100 loss: 0.2807\n",
            "Epoch 56/100 loss: 0.2799\n",
            "Epoch 57/100 loss: 0.2786\n",
            "Epoch 58/100 loss: 0.2841\n",
            "Epoch 59/100 loss: 0.2771\n",
            "Epoch 60/100 loss: 0.2757\n",
            "Epoch 61/100 loss: 0.2752\n",
            "Epoch 62/100 loss: 0.2771\n",
            "Epoch 63/100 loss: 0.2732\n",
            "Epoch 64/100 loss: 0.2727\n",
            "Epoch 65/100 loss: 0.2747\n",
            "Epoch 66/100 loss: 0.2741\n",
            "Epoch 67/100 loss: 0.2699\n",
            "Epoch 68/100 loss: 0.2702\n",
            "Epoch 69/100 loss: 0.2697\n",
            "Epoch 70/100 loss: 0.2704\n",
            "Epoch 71/100 loss: 0.2683\n",
            "Epoch 72/100 loss: 0.2675\n",
            "Epoch 73/100 loss: 0.2663\n",
            "Epoch 74/100 loss: 0.2690\n",
            "Epoch 75/100 loss: 0.2666\n",
            "Epoch 76/100 loss: 0.2651\n",
            "Epoch 77/100 loss: 0.2656\n",
            "Epoch 78/100 loss: 0.2637\n",
            "Epoch 79/100 loss: 0.2643\n",
            "Epoch 80/100 loss: 0.2654\n",
            "Epoch 81/100 loss: 0.2638\n",
            "Epoch 82/100 loss: 0.2625\n",
            "Epoch 83/100 loss: 0.2624\n",
            "Epoch 84/100 loss: 0.2611\n",
            "Epoch 85/100 loss: 0.2601\n",
            "Epoch 86/100 loss: 0.2653\n",
            "Epoch 87/100 loss: 0.2666\n",
            "Epoch 88/100 loss: 0.2593\n",
            "Epoch 89/100 loss: 0.2590\n",
            "Epoch 90/100 loss: 0.2611\n",
            "Epoch 91/100 loss: 0.2583\n",
            "Epoch 92/100 loss: 0.2609\n",
            "Epoch 93/100 loss: 0.2581\n",
            "Epoch 94/100 loss: 0.2586\n",
            "Epoch 95/100 loss: 0.2567\n",
            "Epoch 96/100 loss: 0.2574\n",
            "Epoch 97/100 loss: 0.2591\n",
            "Epoch 98/100 loss: 0.2554\n",
            "Epoch 99/100 loss: 0.2599\n",
            "Epoch 100/100 loss: 0.2567\n",
            "batch size: 128, learning rate: 0.01, num epochs: 200\n",
            "Epoch 1/200 loss: 2.2336\n",
            "Epoch 2/200 loss: 1.9866\n",
            "Epoch 3/200 loss: 1.7913\n",
            "Epoch 4/200 loss: 1.6096\n",
            "Epoch 5/200 loss: 1.4557\n",
            "Epoch 6/200 loss: 1.3095\n",
            "Epoch 7/200 loss: 1.1918\n",
            "Epoch 8/200 loss: 1.0828\n",
            "Epoch 9/200 loss: 0.9952\n",
            "Epoch 10/200 loss: 0.9124\n",
            "Epoch 11/200 loss: 0.8379\n",
            "Epoch 12/200 loss: 0.7805\n",
            "Epoch 13/200 loss: 0.7250\n",
            "Epoch 14/200 loss: 0.6822\n",
            "Epoch 15/200 loss: 0.6425\n",
            "Epoch 16/200 loss: 0.6071\n",
            "Epoch 17/200 loss: 0.5802\n",
            "Epoch 18/200 loss: 0.5491\n",
            "Epoch 19/200 loss: 0.5261\n",
            "Epoch 20/200 loss: 0.5062\n",
            "Epoch 21/200 loss: 0.4873\n",
            "Epoch 22/200 loss: 0.4704\n",
            "Epoch 23/200 loss: 0.4587\n",
            "Epoch 24/200 loss: 0.4444\n",
            "Epoch 25/200 loss: 0.4317\n",
            "Epoch 26/200 loss: 0.4215\n",
            "Epoch 27/200 loss: 0.4082\n",
            "Epoch 28/200 loss: 0.4027\n",
            "Epoch 29/200 loss: 0.3958\n",
            "Epoch 30/200 loss: 0.3887\n",
            "Epoch 31/200 loss: 0.3811\n",
            "Epoch 32/200 loss: 0.3713\n",
            "Epoch 33/200 loss: 0.3686\n",
            "Epoch 34/200 loss: 0.3641\n",
            "Epoch 35/200 loss: 0.3637\n",
            "Epoch 36/200 loss: 0.3518\n",
            "Epoch 37/200 loss: 0.3458\n",
            "Epoch 38/200 loss: 0.3456\n",
            "Epoch 39/200 loss: 0.3425\n",
            "Epoch 40/200 loss: 0.3376\n",
            "Epoch 41/200 loss: 0.3335\n",
            "Epoch 42/200 loss: 0.3309\n",
            "Epoch 43/200 loss: 0.3321\n",
            "Epoch 44/200 loss: 0.3239\n",
            "Epoch 45/200 loss: 0.3202\n",
            "Epoch 46/200 loss: 0.3187\n",
            "Epoch 47/200 loss: 0.3199\n",
            "Epoch 48/200 loss: 0.3133\n",
            "Epoch 49/200 loss: 0.3134\n",
            "Epoch 50/200 loss: 0.3135\n",
            "Epoch 51/200 loss: 0.3124\n",
            "Epoch 52/200 loss: 0.3075\n",
            "Epoch 53/200 loss: 0.3050\n",
            "Epoch 54/200 loss: 0.3042\n",
            "Epoch 55/200 loss: 0.3024\n",
            "Epoch 56/200 loss: 0.3032\n",
            "Epoch 57/200 loss: 0.2985\n",
            "Epoch 58/200 loss: 0.2983\n",
            "Epoch 59/200 loss: 0.2958\n",
            "Epoch 60/200 loss: 0.2950\n",
            "Epoch 61/200 loss: 0.2923\n",
            "Epoch 62/200 loss: 0.2953\n",
            "Epoch 63/200 loss: 0.2906\n",
            "Epoch 64/200 loss: 0.2895\n",
            "Epoch 65/200 loss: 0.2895\n",
            "Epoch 66/200 loss: 0.2865\n",
            "Epoch 67/200 loss: 0.2883\n",
            "Epoch 68/200 loss: 0.2854\n",
            "Epoch 69/200 loss: 0.2909\n",
            "Epoch 70/200 loss: 0.2839\n",
            "Epoch 71/200 loss: 0.2827\n",
            "Epoch 72/200 loss: 0.2832\n",
            "Epoch 73/200 loss: 0.2821\n",
            "Epoch 74/200 loss: 0.2815\n",
            "Epoch 75/200 loss: 0.2795\n",
            "Epoch 76/200 loss: 0.2786\n",
            "Epoch 77/200 loss: 0.2825\n",
            "Epoch 78/200 loss: 0.2790\n",
            "Epoch 79/200 loss: 0.2767\n",
            "Epoch 80/200 loss: 0.2777\n",
            "Epoch 81/200 loss: 0.2760\n",
            "Epoch 82/200 loss: 0.2768\n",
            "Epoch 83/200 loss: 0.2753\n",
            "Epoch 84/200 loss: 0.2742\n",
            "Epoch 85/200 loss: 0.2754\n",
            "Epoch 86/200 loss: 0.2737\n",
            "Epoch 87/200 loss: 0.2721\n",
            "Epoch 88/200 loss: 0.2752\n",
            "Epoch 89/200 loss: 0.2717\n",
            "Epoch 90/200 loss: 0.2717\n",
            "Epoch 91/200 loss: 0.2724\n",
            "Epoch 92/200 loss: 0.2721\n",
            "Epoch 93/200 loss: 0.2701\n",
            "Epoch 94/200 loss: 0.2729\n",
            "Epoch 95/200 loss: 0.2708\n",
            "Epoch 96/200 loss: 0.2702\n",
            "Epoch 97/200 loss: 0.2691\n",
            "Epoch 98/200 loss: 0.2687\n",
            "Epoch 99/200 loss: 0.2697\n",
            "Epoch 100/200 loss: 0.2681\n",
            "Epoch 101/200 loss: 0.2722\n",
            "Epoch 102/200 loss: 0.2675\n",
            "Epoch 103/200 loss: 0.2670\n",
            "Epoch 104/200 loss: 0.2658\n",
            "Epoch 105/200 loss: 0.2658\n",
            "Epoch 106/200 loss: 0.2662\n",
            "Epoch 107/200 loss: 0.2667\n",
            "Epoch 108/200 loss: 0.2638\n",
            "Epoch 109/200 loss: 0.2648\n",
            "Epoch 110/200 loss: 0.2650\n",
            "Epoch 111/200 loss: 0.2646\n",
            "Epoch 112/200 loss: 0.2645\n",
            "Epoch 113/200 loss: 0.2619\n",
            "Epoch 114/200 loss: 0.2676\n",
            "Epoch 115/200 loss: 0.2619\n",
            "Epoch 116/200 loss: 0.2616\n",
            "Epoch 117/200 loss: 0.2623\n",
            "Epoch 118/200 loss: 0.2643\n",
            "Epoch 119/200 loss: 0.2617\n",
            "Epoch 120/200 loss: 0.2606\n",
            "Epoch 121/200 loss: 0.2643\n",
            "Epoch 122/200 loss: 0.2631\n",
            "Epoch 123/200 loss: 0.2601\n",
            "Epoch 124/200 loss: 0.2590\n",
            "Epoch 125/200 loss: 0.2625\n",
            "Epoch 126/200 loss: 0.2584\n",
            "Epoch 127/200 loss: 0.2599\n",
            "Epoch 128/200 loss: 0.2582\n",
            "Epoch 129/200 loss: 0.2572\n",
            "Epoch 130/200 loss: 0.2580\n",
            "Epoch 131/200 loss: 0.2567\n",
            "Epoch 132/200 loss: 0.2645\n",
            "Epoch 133/200 loss: 0.2591\n",
            "Epoch 134/200 loss: 0.2585\n",
            "Epoch 135/200 loss: 0.2596\n",
            "Epoch 136/200 loss: 0.2584\n",
            "Epoch 137/200 loss: 0.2604\n",
            "Epoch 138/200 loss: 0.2566\n",
            "Epoch 139/200 loss: 0.2560\n",
            "Epoch 140/200 loss: 0.2571\n",
            "Epoch 141/200 loss: 0.2550\n",
            "Epoch 142/200 loss: 0.2565\n",
            "Epoch 143/200 loss: 0.2543\n",
            "Epoch 144/200 loss: 0.2535\n",
            "Epoch 145/200 loss: 0.2543\n",
            "Epoch 146/200 loss: 0.2544\n",
            "Epoch 147/200 loss: 0.2538\n",
            "Epoch 148/200 loss: 0.2535\n",
            "Epoch 149/200 loss: 0.2539\n",
            "Epoch 150/200 loss: 0.2535\n",
            "Epoch 151/200 loss: 0.2521\n",
            "Epoch 152/200 loss: 0.2537\n",
            "Epoch 153/200 loss: 0.2549\n",
            "Epoch 154/200 loss: 0.2518\n",
            "Epoch 155/200 loss: 0.2524\n",
            "Epoch 156/200 loss: 0.2541\n",
            "Epoch 157/200 loss: 0.2506\n",
            "Epoch 158/200 loss: 0.2512\n",
            "Epoch 159/200 loss: 0.2520\n",
            "Epoch 160/200 loss: 0.2508\n",
            "Epoch 161/200 loss: 0.2530\n",
            "Epoch 162/200 loss: 0.2510\n",
            "Epoch 163/200 loss: 0.2533\n",
            "Epoch 164/200 loss: 0.2495\n",
            "Epoch 165/200 loss: 0.2499\n",
            "Epoch 166/200 loss: 0.2494\n",
            "Epoch 167/200 loss: 0.2490\n",
            "Epoch 168/200 loss: 0.2516\n",
            "Epoch 169/200 loss: 0.2509\n",
            "Epoch 170/200 loss: 0.2480\n",
            "Epoch 171/200 loss: 0.2544\n",
            "Epoch 172/200 loss: 0.2519\n",
            "Epoch 173/200 loss: 0.2501\n",
            "Epoch 174/200 loss: 0.2506\n",
            "Epoch 175/200 loss: 0.2484\n",
            "Epoch 176/200 loss: 0.2532\n",
            "Epoch 177/200 loss: 0.2518\n",
            "Epoch 178/200 loss: 0.2493\n",
            "Epoch 179/200 loss: 0.2475\n",
            "Epoch 180/200 loss: 0.2478\n",
            "Epoch 181/200 loss: 0.2471\n",
            "Epoch 182/200 loss: 0.2529\n",
            "Epoch 183/200 loss: 0.2477\n",
            "Epoch 184/200 loss: 0.2487\n",
            "Epoch 185/200 loss: 0.2468\n",
            "Epoch 186/200 loss: 0.2458\n",
            "Epoch 187/200 loss: 0.2488\n",
            "Epoch 188/200 loss: 0.2484\n",
            "Epoch 189/200 loss: 0.2451\n",
            "Epoch 190/200 loss: 0.2456\n",
            "Epoch 191/200 loss: 0.2449\n",
            "Epoch 192/200 loss: 0.2453\n",
            "Epoch 193/200 loss: 0.2447\n",
            "Epoch 194/200 loss: 0.2473\n",
            "Epoch 195/200 loss: 0.2479\n",
            "Epoch 196/200 loss: 0.2447\n",
            "Epoch 197/200 loss: 0.2441\n",
            "Epoch 198/200 loss: 0.2445\n",
            "Epoch 199/200 loss: 0.2468\n",
            "Epoch 200/200 loss: 0.2477\n",
            "batch size: 128, learning rate: 0.001, num epochs: 20\n",
            "Epoch 1/20 loss: 2.4383\n",
            "Epoch 2/20 loss: 2.3415\n",
            "Epoch 3/20 loss: 2.2649\n",
            "Epoch 4/20 loss: 2.1822\n",
            "Epoch 5/20 loss: 2.1096\n",
            "Epoch 6/20 loss: 2.0386\n",
            "Epoch 7/20 loss: 1.9771\n",
            "Epoch 8/20 loss: 1.9156\n",
            "Epoch 9/20 loss: 1.8660\n",
            "Epoch 10/20 loss: 1.8098\n",
            "Epoch 11/20 loss: 1.7731\n",
            "Epoch 12/20 loss: 1.7243\n",
            "Epoch 13/20 loss: 1.6765\n",
            "Epoch 14/20 loss: 1.6382\n",
            "Epoch 15/20 loss: 1.6007\n",
            "Epoch 16/20 loss: 1.5665\n",
            "Epoch 17/20 loss: 1.5358\n",
            "Epoch 18/20 loss: 1.5095\n",
            "Epoch 19/20 loss: 1.4796\n",
            "Epoch 20/20 loss: 1.4506\n",
            "batch size: 128, learning rate: 0.001, num epochs: 50\n",
            "Epoch 1/50 loss: 1.7017\n",
            "Epoch 2/50 loss: 1.6772\n",
            "Epoch 3/50 loss: 1.6506\n",
            "Epoch 4/50 loss: 1.6324\n",
            "Epoch 5/50 loss: 1.6147\n",
            "Epoch 6/50 loss: 1.5961\n",
            "Epoch 7/50 loss: 1.5780\n",
            "Epoch 8/50 loss: 1.5714\n",
            "Epoch 9/50 loss: 1.5459\n",
            "Epoch 10/50 loss: 1.5384\n",
            "Epoch 11/50 loss: 1.5170\n",
            "Epoch 12/50 loss: 1.4989\n",
            "Epoch 13/50 loss: 1.4843\n",
            "Epoch 14/50 loss: 1.4742\n",
            "Epoch 15/50 loss: 1.4611\n",
            "Epoch 16/50 loss: 1.4463\n",
            "Epoch 17/50 loss: 1.4311\n",
            "Epoch 18/50 loss: 1.4191\n",
            "Epoch 19/50 loss: 1.4055\n",
            "Epoch 20/50 loss: 1.3989\n",
            "Epoch 21/50 loss: 1.3854\n",
            "Epoch 22/50 loss: 1.3697\n",
            "Epoch 23/50 loss: 1.3571\n",
            "Epoch 24/50 loss: 1.3503\n",
            "Epoch 25/50 loss: 1.3374\n",
            "Epoch 26/50 loss: 1.3251\n",
            "Epoch 27/50 loss: 1.3188\n",
            "Epoch 28/50 loss: 1.2990\n",
            "Epoch 29/50 loss: 1.2980\n",
            "Epoch 30/50 loss: 1.2832\n",
            "Epoch 31/50 loss: 1.2719\n",
            "Epoch 32/50 loss: 1.2598\n",
            "Epoch 33/50 loss: 1.2487\n",
            "Epoch 34/50 loss: 1.2401\n",
            "Epoch 35/50 loss: 1.2299\n",
            "Epoch 36/50 loss: 1.2146\n",
            "Epoch 37/50 loss: 1.2169\n",
            "Epoch 38/50 loss: 1.2049\n",
            "Epoch 39/50 loss: 1.2027\n",
            "Epoch 40/50 loss: 1.1824\n",
            "Epoch 41/50 loss: 1.1722\n",
            "Epoch 42/50 loss: 1.1631\n",
            "Epoch 43/50 loss: 1.1548\n",
            "Epoch 44/50 loss: 1.1506\n",
            "Epoch 45/50 loss: 1.1439\n",
            "Epoch 46/50 loss: 1.1326\n",
            "Epoch 47/50 loss: 1.1211\n",
            "Epoch 48/50 loss: 1.1105\n",
            "Epoch 49/50 loss: 1.1047\n",
            "Epoch 50/50 loss: 1.0951\n",
            "batch size: 128, learning rate: 0.001, num epochs: 100\n",
            "Epoch 1/100 loss: 0.8348\n",
            "Epoch 2/100 loss: 0.8197\n",
            "Epoch 3/100 loss: 0.8219\n",
            "Epoch 4/100 loss: 0.8060\n",
            "Epoch 5/100 loss: 0.7976\n",
            "Epoch 6/100 loss: 0.7979\n",
            "Epoch 7/100 loss: 0.7813\n",
            "Epoch 8/100 loss: 0.7774\n",
            "Epoch 9/100 loss: 0.7710\n",
            "Epoch 10/100 loss: 0.7661\n",
            "Epoch 11/100 loss: 0.7624\n",
            "Epoch 12/100 loss: 0.7519\n",
            "Epoch 13/100 loss: 0.7470\n",
            "Epoch 14/100 loss: 0.7433\n",
            "Epoch 15/100 loss: 0.7442\n",
            "Epoch 16/100 loss: 0.7317\n",
            "Epoch 17/100 loss: 0.7312\n",
            "Epoch 18/100 loss: 0.7341\n",
            "Epoch 19/100 loss: 0.7238\n",
            "Epoch 20/100 loss: 0.7194\n",
            "Epoch 21/100 loss: 0.7117\n",
            "Epoch 22/100 loss: 0.7112\n",
            "Epoch 23/100 loss: 0.7147\n",
            "Epoch 24/100 loss: 0.7090\n",
            "Epoch 25/100 loss: 0.7027\n",
            "Epoch 26/100 loss: 0.6979\n",
            "Epoch 27/100 loss: 0.6959\n",
            "Epoch 28/100 loss: 0.6900\n",
            "Epoch 29/100 loss: 0.6882\n",
            "Epoch 30/100 loss: 0.6831\n",
            "Epoch 31/100 loss: 0.6784\n",
            "Epoch 32/100 loss: 0.6758\n",
            "Epoch 33/100 loss: 0.6760\n",
            "Epoch 34/100 loss: 0.6708\n",
            "Epoch 35/100 loss: 0.6683\n",
            "Epoch 36/100 loss: 0.6634\n",
            "Epoch 37/100 loss: 0.6571\n",
            "Epoch 38/100 loss: 0.6559\n",
            "Epoch 39/100 loss: 0.6555\n",
            "Epoch 40/100 loss: 0.6501\n",
            "Epoch 41/100 loss: 0.6499\n",
            "Epoch 42/100 loss: 0.6439\n",
            "Epoch 43/100 loss: 0.6414\n",
            "Epoch 44/100 loss: 0.6456\n",
            "Epoch 45/100 loss: 0.6344\n",
            "Epoch 46/100 loss: 0.6339\n",
            "Epoch 47/100 loss: 0.6331\n",
            "Epoch 48/100 loss: 0.6244\n",
            "Epoch 49/100 loss: 0.6285\n",
            "Epoch 50/100 loss: 0.6223\n",
            "Epoch 51/100 loss: 0.6189\n",
            "Epoch 52/100 loss: 0.6146\n",
            "Epoch 53/100 loss: 0.6155\n",
            "Epoch 54/100 loss: 0.6143\n",
            "Epoch 55/100 loss: 0.6100\n",
            "Epoch 56/100 loss: 0.6083\n",
            "Epoch 57/100 loss: 0.6045\n",
            "Epoch 58/100 loss: 0.6017\n",
            "Epoch 59/100 loss: 0.5983\n",
            "Epoch 60/100 loss: 0.5997\n",
            "Epoch 61/100 loss: 0.5929\n",
            "Epoch 62/100 loss: 0.5887\n",
            "Epoch 63/100 loss: 0.5913\n",
            "Epoch 64/100 loss: 0.5864\n",
            "Epoch 65/100 loss: 0.5873\n",
            "Epoch 66/100 loss: 0.5811\n",
            "Epoch 67/100 loss: 0.5790\n",
            "Epoch 68/100 loss: 0.5765\n",
            "Epoch 69/100 loss: 0.5759\n",
            "Epoch 70/100 loss: 0.5738\n",
            "Epoch 71/100 loss: 0.5723\n",
            "Epoch 72/100 loss: 0.5731\n",
            "Epoch 73/100 loss: 0.5678\n",
            "Epoch 74/100 loss: 0.5680\n",
            "Epoch 75/100 loss: 0.5686\n",
            "Epoch 76/100 loss: 0.5632\n",
            "Epoch 77/100 loss: 0.5591\n",
            "Epoch 78/100 loss: 0.5545\n",
            "Epoch 79/100 loss: 0.5543\n",
            "Epoch 80/100 loss: 0.5549\n",
            "Epoch 81/100 loss: 0.5509\n",
            "Epoch 82/100 loss: 0.5475\n",
            "Epoch 83/100 loss: 0.5471\n",
            "Epoch 84/100 loss: 0.5444\n",
            "Epoch 85/100 loss: 0.5491\n",
            "Epoch 86/100 loss: 0.5390\n",
            "Epoch 87/100 loss: 0.5388\n",
            "Epoch 88/100 loss: 0.5399\n",
            "Epoch 89/100 loss: 0.5362\n",
            "Epoch 90/100 loss: 0.5328\n",
            "Epoch 91/100 loss: 0.5340\n",
            "Epoch 92/100 loss: 0.5283\n",
            "Epoch 93/100 loss: 0.5297\n",
            "Epoch 94/100 loss: 0.5295\n",
            "Epoch 95/100 loss: 0.5231\n",
            "Epoch 96/100 loss: 0.5215\n",
            "Epoch 97/100 loss: 0.5284\n",
            "Epoch 98/100 loss: 0.5205\n",
            "Epoch 99/100 loss: 0.5167\n",
            "Epoch 100/100 loss: 0.5203\n",
            "batch size: 128, learning rate: 0.001, num epochs: 200\n",
            "Epoch 1/200 loss: 1.6722\n",
            "Epoch 2/200 loss: 1.5912\n",
            "Epoch 3/200 loss: 1.5327\n",
            "Epoch 4/200 loss: 1.4705\n",
            "Epoch 5/200 loss: 1.4029\n",
            "Epoch 6/200 loss: 1.3575\n",
            "Epoch 7/200 loss: 1.3050\n",
            "Epoch 8/200 loss: 1.2605\n",
            "Epoch 9/200 loss: 1.2207\n",
            "Epoch 10/200 loss: 1.1763\n",
            "Epoch 11/200 loss: 1.1439\n",
            "Epoch 12/200 loss: 1.1125\n",
            "Epoch 13/200 loss: 1.0917\n",
            "Epoch 14/200 loss: 1.0584\n",
            "Epoch 15/200 loss: 1.0395\n",
            "Epoch 16/200 loss: 1.0189\n",
            "Epoch 17/200 loss: 1.0014\n",
            "Epoch 18/200 loss: 0.9852\n",
            "Epoch 19/200 loss: 0.9767\n",
            "Epoch 20/200 loss: 0.9517\n",
            "Epoch 21/200 loss: 0.9413\n",
            "Epoch 22/200 loss: 0.9320\n",
            "Epoch 23/200 loss: 0.9315\n",
            "Epoch 24/200 loss: 0.9137\n",
            "Epoch 25/200 loss: 0.9011\n",
            "Epoch 26/200 loss: 0.8970\n",
            "Epoch 27/200 loss: 0.8849\n",
            "Epoch 28/200 loss: 0.8815\n",
            "Epoch 29/200 loss: 0.8704\n",
            "Epoch 30/200 loss: 0.8631\n",
            "Epoch 31/200 loss: 0.8566\n",
            "Epoch 32/200 loss: 0.8510\n",
            "Epoch 33/200 loss: 0.8452\n",
            "Epoch 34/200 loss: 0.8356\n",
            "Epoch 35/200 loss: 0.8353\n",
            "Epoch 36/200 loss: 0.8266\n",
            "Epoch 37/200 loss: 0.8220\n",
            "Epoch 38/200 loss: 0.8152\n",
            "Epoch 39/200 loss: 0.8109\n",
            "Epoch 40/200 loss: 0.8071\n",
            "Epoch 41/200 loss: 0.8032\n",
            "Epoch 42/200 loss: 0.7967\n",
            "Epoch 43/200 loss: 0.7978\n",
            "Epoch 44/200 loss: 0.7824\n",
            "Epoch 45/200 loss: 0.7859\n",
            "Epoch 46/200 loss: 0.7838\n",
            "Epoch 47/200 loss: 0.7761\n",
            "Epoch 48/200 loss: 0.7663\n",
            "Epoch 49/200 loss: 0.7667\n",
            "Epoch 50/200 loss: 0.7562\n",
            "Epoch 51/200 loss: 0.7554\n",
            "Epoch 52/200 loss: 0.7511\n",
            "Epoch 53/200 loss: 0.7504\n",
            "Epoch 54/200 loss: 0.7432\n",
            "Epoch 55/200 loss: 0.7456\n",
            "Epoch 56/200 loss: 0.7406\n",
            "Epoch 57/200 loss: 0.7279\n",
            "Epoch 58/200 loss: 0.7256\n",
            "Epoch 59/200 loss: 0.7211\n",
            "Epoch 60/200 loss: 0.7205\n",
            "Epoch 61/200 loss: 0.7195\n",
            "Epoch 62/200 loss: 0.7123\n",
            "Epoch 63/200 loss: 0.7064\n",
            "Epoch 64/200 loss: 0.7042\n",
            "Epoch 65/200 loss: 0.7025\n",
            "Epoch 66/200 loss: 0.6965\n",
            "Epoch 67/200 loss: 0.6998\n",
            "Epoch 68/200 loss: 0.6928\n",
            "Epoch 69/200 loss: 0.6896\n",
            "Epoch 70/200 loss: 0.6858\n",
            "Epoch 71/200 loss: 0.6846\n",
            "Epoch 72/200 loss: 0.6765\n",
            "Epoch 73/200 loss: 0.6788\n",
            "Epoch 74/200 loss: 0.6711\n",
            "Epoch 75/200 loss: 0.6711\n",
            "Epoch 76/200 loss: 0.6644\n",
            "Epoch 77/200 loss: 0.6690\n",
            "Epoch 78/200 loss: 0.6667\n",
            "Epoch 79/200 loss: 0.6567\n",
            "Epoch 80/200 loss: 0.6591\n",
            "Epoch 81/200 loss: 0.6480\n",
            "Epoch 82/200 loss: 0.6478\n",
            "Epoch 83/200 loss: 0.6439\n",
            "Epoch 84/200 loss: 0.6440\n",
            "Epoch 85/200 loss: 0.6393\n",
            "Epoch 86/200 loss: 0.6408\n",
            "Epoch 87/200 loss: 0.6369\n",
            "Epoch 88/200 loss: 0.6302\n",
            "Epoch 89/200 loss: 0.6263\n",
            "Epoch 90/200 loss: 0.6320\n",
            "Epoch 91/200 loss: 0.6217\n",
            "Epoch 92/200 loss: 0.6218\n",
            "Epoch 93/200 loss: 0.6180\n",
            "Epoch 94/200 loss: 0.6198\n",
            "Epoch 95/200 loss: 0.6143\n",
            "Epoch 96/200 loss: 0.6120\n",
            "Epoch 97/200 loss: 0.6101\n",
            "Epoch 98/200 loss: 0.6039\n",
            "Epoch 99/200 loss: 0.6055\n",
            "Epoch 100/200 loss: 0.6009\n",
            "Epoch 101/200 loss: 0.5985\n",
            "Epoch 102/200 loss: 0.5966\n",
            "Epoch 103/200 loss: 0.5933\n",
            "Epoch 104/200 loss: 0.5952\n",
            "Epoch 105/200 loss: 0.5889\n",
            "Epoch 106/200 loss: 0.5845\n",
            "Epoch 107/200 loss: 0.5852\n",
            "Epoch 108/200 loss: 0.5803\n",
            "Epoch 109/200 loss: 0.5818\n",
            "Epoch 110/200 loss: 0.5782\n",
            "Epoch 111/200 loss: 0.5808\n",
            "Epoch 112/200 loss: 0.5786\n",
            "Epoch 113/200 loss: 0.5711\n",
            "Epoch 114/200 loss: 0.5750\n",
            "Epoch 115/200 loss: 0.5692\n",
            "Epoch 116/200 loss: 0.5660\n",
            "Epoch 117/200 loss: 0.5619\n",
            "Epoch 118/200 loss: 0.5636\n",
            "Epoch 119/200 loss: 0.5633\n",
            "Epoch 120/200 loss: 0.5584\n",
            "Epoch 121/200 loss: 0.5568\n",
            "Epoch 122/200 loss: 0.5526\n",
            "Epoch 123/200 loss: 0.5513\n",
            "Epoch 124/200 loss: 0.5502\n",
            "Epoch 125/200 loss: 0.5487\n",
            "Epoch 126/200 loss: 0.5492\n",
            "Epoch 127/200 loss: 0.5457\n",
            "Epoch 128/200 loss: 0.5413\n",
            "Epoch 129/200 loss: 0.5387\n",
            "Epoch 130/200 loss: 0.5357\n",
            "Epoch 131/200 loss: 0.5440\n",
            "Epoch 132/200 loss: 0.5406\n",
            "Epoch 133/200 loss: 0.5386\n",
            "Epoch 134/200 loss: 0.5316\n",
            "Epoch 135/200 loss: 0.5325\n",
            "Epoch 136/200 loss: 0.5278\n",
            "Epoch 137/200 loss: 0.5258\n",
            "Epoch 138/200 loss: 0.5241\n",
            "Epoch 139/200 loss: 0.5237\n",
            "Epoch 140/200 loss: 0.5209\n",
            "Epoch 141/200 loss: 0.5199\n",
            "Epoch 142/200 loss: 0.5209\n",
            "Epoch 143/200 loss: 0.5159\n",
            "Epoch 144/200 loss: 0.5142\n",
            "Epoch 145/200 loss: 0.5121\n",
            "Epoch 146/200 loss: 0.5106\n",
            "Epoch 147/200 loss: 0.5101\n",
            "Epoch 148/200 loss: 0.5094\n",
            "Epoch 149/200 loss: 0.5085\n",
            "Epoch 150/200 loss: 0.5139\n",
            "Epoch 151/200 loss: 0.5072\n",
            "Epoch 152/200 loss: 0.5037\n",
            "Epoch 153/200 loss: 0.5018\n",
            "Epoch 154/200 loss: 0.5015\n",
            "Epoch 155/200 loss: 0.4959\n",
            "Epoch 156/200 loss: 0.5002\n",
            "Epoch 157/200 loss: 0.4946\n",
            "Epoch 158/200 loss: 0.4959\n",
            "Epoch 159/200 loss: 0.4981\n",
            "Epoch 160/200 loss: 0.4906\n",
            "Epoch 161/200 loss: 0.4881\n",
            "Epoch 162/200 loss: 0.4895\n",
            "Epoch 163/200 loss: 0.4903\n",
            "Epoch 164/200 loss: 0.4900\n",
            "Epoch 165/200 loss: 0.4867\n",
            "Epoch 166/200 loss: 0.4845\n",
            "Epoch 167/200 loss: 0.4811\n",
            "Epoch 168/200 loss: 0.4825\n",
            "Epoch 169/200 loss: 0.4854\n",
            "Epoch 170/200 loss: 0.4767\n",
            "Epoch 171/200 loss: 0.4757\n",
            "Epoch 172/200 loss: 0.4744\n",
            "Epoch 173/200 loss: 0.4766\n",
            "Epoch 174/200 loss: 0.4738\n",
            "Epoch 175/200 loss: 0.4718\n",
            "Epoch 176/200 loss: 0.4719\n",
            "Epoch 177/200 loss: 0.4723\n",
            "Epoch 178/200 loss: 0.4692\n",
            "Epoch 179/200 loss: 0.4685\n",
            "Epoch 180/200 loss: 0.4670\n",
            "Epoch 181/200 loss: 0.4666\n",
            "Epoch 182/200 loss: 0.4675\n",
            "Epoch 183/200 loss: 0.4655\n",
            "Epoch 184/200 loss: 0.4636\n",
            "Epoch 185/200 loss: 0.4637\n",
            "Epoch 186/200 loss: 0.4604\n",
            "Epoch 187/200 loss: 0.4580\n",
            "Epoch 188/200 loss: 0.4590\n",
            "Epoch 189/200 loss: 0.4572\n",
            "Epoch 190/200 loss: 0.4553\n",
            "Epoch 191/200 loss: 0.4627\n",
            "Epoch 192/200 loss: 0.4544\n",
            "Epoch 193/200 loss: 0.4508\n",
            "Epoch 194/200 loss: 0.4539\n",
            "Epoch 195/200 loss: 0.4491\n",
            "Epoch 196/200 loss: 0.4495\n",
            "Epoch 197/200 loss: 0.4506\n",
            "Epoch 198/200 loss: 0.4533\n",
            "Epoch 199/200 loss: 0.4503\n",
            "Epoch 200/200 loss: 0.4463\n",
            "batch size: 256, learning rate: 0.1, num epochs: 20\n",
            "Epoch 1/20 loss: 1.1014\n",
            "Epoch 2/20 loss: 0.7204\n",
            "Epoch 3/20 loss: 0.5464\n",
            "Epoch 4/20 loss: 0.4549\n",
            "Epoch 5/20 loss: 0.4024\n",
            "Epoch 6/20 loss: 0.3684\n",
            "Epoch 7/20 loss: 0.3456\n",
            "Epoch 8/20 loss: 0.3290\n",
            "Epoch 9/20 loss: 0.3163\n",
            "Epoch 10/20 loss: 0.3071\n",
            "Epoch 11/20 loss: 0.2982\n",
            "Epoch 12/20 loss: 0.2904\n",
            "Epoch 13/20 loss: 0.2857\n",
            "Epoch 14/20 loss: 0.2809\n",
            "Epoch 15/20 loss: 0.2768\n",
            "Epoch 16/20 loss: 0.2725\n",
            "Epoch 17/20 loss: 0.2695\n",
            "Epoch 18/20 loss: 0.2656\n",
            "Epoch 19/20 loss: 0.2636\n",
            "Epoch 20/20 loss: 0.2613\n",
            "batch size: 256, learning rate: 0.1, num epochs: 50\n",
            "Epoch 1/50 loss: 1.0138\n",
            "Epoch 2/50 loss: 0.7001\n",
            "Epoch 3/50 loss: 0.5475\n",
            "Epoch 4/50 loss: 0.4624\n",
            "Epoch 5/50 loss: 0.4107\n",
            "Epoch 6/50 loss: 0.3795\n",
            "Epoch 7/50 loss: 0.3595\n",
            "Epoch 8/50 loss: 0.3429\n",
            "Epoch 9/50 loss: 0.3303\n",
            "Epoch 10/50 loss: 0.3214\n",
            "Epoch 11/50 loss: 0.3136\n",
            "Epoch 12/50 loss: 0.3086\n",
            "Epoch 13/50 loss: 0.3014\n",
            "Epoch 14/50 loss: 0.2969\n",
            "Epoch 15/50 loss: 0.2923\n",
            "Epoch 16/50 loss: 0.2885\n",
            "Epoch 17/50 loss: 0.2856\n",
            "Epoch 18/50 loss: 0.2823\n",
            "Epoch 19/50 loss: 0.2799\n",
            "Epoch 20/50 loss: 0.2758\n",
            "Epoch 21/50 loss: 0.2722\n",
            "Epoch 22/50 loss: 0.2712\n",
            "Epoch 23/50 loss: 0.2674\n",
            "Epoch 24/50 loss: 0.2670\n",
            "Epoch 25/50 loss: 0.2646\n",
            "Epoch 26/50 loss: 0.2634\n",
            "Epoch 27/50 loss: 0.2601\n",
            "Epoch 28/50 loss: 0.2589\n",
            "Epoch 29/50 loss: 0.2574\n",
            "Epoch 30/50 loss: 0.2545\n",
            "Epoch 31/50 loss: 0.2532\n",
            "Epoch 32/50 loss: 0.2525\n",
            "Epoch 33/50 loss: 0.2500\n",
            "Epoch 34/50 loss: 0.2489\n",
            "Epoch 35/50 loss: 0.2481\n",
            "Epoch 36/50 loss: 0.2463\n",
            "Epoch 37/50 loss: 0.2458\n",
            "Epoch 38/50 loss: 0.2442\n",
            "Epoch 39/50 loss: 0.2435\n",
            "Epoch 40/50 loss: 0.2428\n",
            "Epoch 41/50 loss: 0.2405\n",
            "Epoch 42/50 loss: 0.2391\n",
            "Epoch 43/50 loss: 0.2395\n",
            "Epoch 44/50 loss: 0.2386\n",
            "Epoch 45/50 loss: 0.2375\n",
            "Epoch 46/50 loss: 0.2368\n",
            "Epoch 47/50 loss: 0.2363\n",
            "Epoch 48/50 loss: 0.2351\n",
            "Epoch 49/50 loss: 0.2352\n",
            "Epoch 50/50 loss: 0.2341\n",
            "batch size: 256, learning rate: 0.1, num epochs: 100\n",
            "Epoch 1/100 loss: 0.9461\n",
            "Epoch 2/100 loss: 0.6958\n",
            "Epoch 3/100 loss: 0.5587\n",
            "Epoch 4/100 loss: 0.4744\n",
            "Epoch 5/100 loss: 0.4201\n",
            "Epoch 6/100 loss: 0.3802\n",
            "Epoch 7/100 loss: 0.3527\n",
            "Epoch 8/100 loss: 0.3325\n",
            "Epoch 9/100 loss: 0.3152\n",
            "Epoch 10/100 loss: 0.3039\n",
            "Epoch 11/100 loss: 0.2958\n",
            "Epoch 12/100 loss: 0.2863\n",
            "Epoch 13/100 loss: 0.2782\n",
            "Epoch 14/100 loss: 0.2741\n",
            "Epoch 15/100 loss: 0.2685\n",
            "Epoch 16/100 loss: 0.2655\n",
            "Epoch 17/100 loss: 0.2622\n",
            "Epoch 18/100 loss: 0.2582\n",
            "Epoch 19/100 loss: 0.2554\n",
            "Epoch 20/100 loss: 0.2534\n",
            "Epoch 21/100 loss: 0.2519\n",
            "Epoch 22/100 loss: 0.2496\n",
            "Epoch 23/100 loss: 0.2481\n",
            "Epoch 24/100 loss: 0.2461\n",
            "Epoch 25/100 loss: 0.2448\n",
            "Epoch 26/100 loss: 0.2435\n",
            "Epoch 27/100 loss: 0.2438\n",
            "Epoch 28/100 loss: 0.2407\n",
            "Epoch 29/100 loss: 0.2405\n",
            "Epoch 30/100 loss: 0.2407\n",
            "Epoch 31/100 loss: 0.2390\n",
            "Epoch 32/100 loss: 0.2382\n",
            "Epoch 33/100 loss: 0.2379\n",
            "Epoch 34/100 loss: 0.2359\n",
            "Epoch 35/100 loss: 0.2359\n",
            "Epoch 36/100 loss: 0.2344\n",
            "Epoch 37/100 loss: 0.2344\n",
            "Epoch 38/100 loss: 0.2337\n",
            "Epoch 39/100 loss: 0.2333\n",
            "Epoch 40/100 loss: 0.2320\n",
            "Epoch 41/100 loss: 0.2322\n",
            "Epoch 42/100 loss: 0.2318\n",
            "Epoch 43/100 loss: 0.2312\n",
            "Epoch 44/100 loss: 0.2313\n",
            "Epoch 45/100 loss: 0.2309\n",
            "Epoch 46/100 loss: 0.2294\n",
            "Epoch 47/100 loss: 0.2293\n",
            "Epoch 48/100 loss: 0.2305\n",
            "Epoch 49/100 loss: 0.2290\n",
            "Epoch 50/100 loss: 0.2280\n",
            "Epoch 51/100 loss: 0.2274\n",
            "Epoch 52/100 loss: 0.2269\n",
            "Epoch 53/100 loss: 0.2282\n",
            "Epoch 54/100 loss: 0.2258\n",
            "Epoch 55/100 loss: 0.2269\n",
            "Epoch 56/100 loss: 0.2254\n",
            "Epoch 57/100 loss: 0.2255\n",
            "Epoch 58/100 loss: 0.2252\n",
            "Epoch 59/100 loss: 0.2245\n",
            "Epoch 60/100 loss: 0.2238\n",
            "Epoch 61/100 loss: 0.2237\n",
            "Epoch 62/100 loss: 0.2243\n",
            "Epoch 63/100 loss: 0.2241\n",
            "Epoch 64/100 loss: 0.2236\n",
            "Epoch 65/100 loss: 0.2239\n",
            "Epoch 66/100 loss: 0.2227\n",
            "Epoch 67/100 loss: 0.2227\n",
            "Epoch 68/100 loss: 0.2222\n",
            "Epoch 69/100 loss: 0.2222\n",
            "Epoch 70/100 loss: 0.2218\n",
            "Epoch 71/100 loss: 0.2214\n",
            "Epoch 72/100 loss: 0.2201\n",
            "Epoch 73/100 loss: 0.2211\n",
            "Epoch 74/100 loss: 0.2207\n",
            "Epoch 75/100 loss: 0.2209\n",
            "Epoch 76/100 loss: 0.2208\n",
            "Epoch 77/100 loss: 0.2206\n",
            "Epoch 78/100 loss: 0.2204\n",
            "Epoch 79/100 loss: 0.2197\n",
            "Epoch 80/100 loss: 0.2192\n",
            "Epoch 81/100 loss: 0.2186\n",
            "Epoch 82/100 loss: 0.2197\n",
            "Epoch 83/100 loss: 0.2193\n",
            "Epoch 84/100 loss: 0.2187\n",
            "Epoch 85/100 loss: 0.2185\n",
            "Epoch 86/100 loss: 0.2179\n",
            "Epoch 87/100 loss: 0.2178\n",
            "Epoch 88/100 loss: 0.2172\n",
            "Epoch 89/100 loss: 0.2178\n",
            "Epoch 90/100 loss: 0.2170\n",
            "Epoch 91/100 loss: 0.2177\n",
            "Epoch 92/100 loss: 0.2170\n",
            "Epoch 93/100 loss: 0.2171\n",
            "Epoch 94/100 loss: 0.2168\n",
            "Epoch 95/100 loss: 0.2166\n",
            "Epoch 96/100 loss: 0.2164\n",
            "Epoch 97/100 loss: 0.2159\n",
            "Epoch 98/100 loss: 0.2161\n",
            "Epoch 99/100 loss: 0.2163\n",
            "Epoch 100/100 loss: 0.2157\n",
            "batch size: 256, learning rate: 0.1, num epochs: 200\n",
            "Epoch 1/200 loss: 1.2652\n",
            "Epoch 2/200 loss: 0.7876\n",
            "Epoch 3/200 loss: 0.5823\n",
            "Epoch 4/200 loss: 0.4759\n",
            "Epoch 5/200 loss: 0.4100\n",
            "Epoch 6/200 loss: 0.3686\n",
            "Epoch 7/200 loss: 0.3398\n",
            "Epoch 8/200 loss: 0.3191\n",
            "Epoch 9/200 loss: 0.3051\n",
            "Epoch 10/200 loss: 0.2940\n",
            "Epoch 11/200 loss: 0.2863\n",
            "Epoch 12/200 loss: 0.2788\n",
            "Epoch 13/200 loss: 0.2719\n",
            "Epoch 14/200 loss: 0.2674\n",
            "Epoch 15/200 loss: 0.2631\n",
            "Epoch 16/200 loss: 0.2606\n",
            "Epoch 17/200 loss: 0.2567\n",
            "Epoch 18/200 loss: 0.2536\n",
            "Epoch 19/200 loss: 0.2520\n",
            "Epoch 20/200 loss: 0.2500\n",
            "Epoch 21/200 loss: 0.2480\n",
            "Epoch 22/200 loss: 0.2464\n",
            "Epoch 23/200 loss: 0.2441\n",
            "Epoch 24/200 loss: 0.2429\n",
            "Epoch 25/200 loss: 0.2415\n",
            "Epoch 26/200 loss: 0.2401\n",
            "Epoch 27/200 loss: 0.2398\n",
            "Epoch 28/200 loss: 0.2378\n",
            "Epoch 29/200 loss: 0.2366\n",
            "Epoch 30/200 loss: 0.2352\n",
            "Epoch 31/200 loss: 0.2346\n",
            "Epoch 32/200 loss: 0.2329\n",
            "Epoch 33/200 loss: 0.2335\n",
            "Epoch 34/200 loss: 0.2324\n",
            "Epoch 35/200 loss: 0.2302\n",
            "Epoch 36/200 loss: 0.2301\n",
            "Epoch 37/200 loss: 0.2299\n",
            "Epoch 38/200 loss: 0.2293\n",
            "Epoch 39/200 loss: 0.2294\n",
            "Epoch 40/200 loss: 0.2278\n",
            "Epoch 41/200 loss: 0.2275\n",
            "Epoch 42/200 loss: 0.2270\n",
            "Epoch 43/200 loss: 0.2265\n",
            "Epoch 44/200 loss: 0.2262\n",
            "Epoch 45/200 loss: 0.2255\n",
            "Epoch 46/200 loss: 0.2249\n",
            "Epoch 47/200 loss: 0.2249\n",
            "Epoch 48/200 loss: 0.2236\n",
            "Epoch 49/200 loss: 0.2235\n",
            "Epoch 50/200 loss: 0.2232\n",
            "Epoch 51/200 loss: 0.2235\n",
            "Epoch 52/200 loss: 0.2220\n",
            "Epoch 53/200 loss: 0.2216\n",
            "Epoch 54/200 loss: 0.2209\n",
            "Epoch 55/200 loss: 0.2202\n",
            "Epoch 56/200 loss: 0.2207\n",
            "Epoch 57/200 loss: 0.2206\n",
            "Epoch 58/200 loss: 0.2204\n",
            "Epoch 59/200 loss: 0.2204\n",
            "Epoch 60/200 loss: 0.2193\n",
            "Epoch 61/200 loss: 0.2186\n",
            "Epoch 62/200 loss: 0.2192\n",
            "Epoch 63/200 loss: 0.2188\n",
            "Epoch 64/200 loss: 0.2188\n",
            "Epoch 65/200 loss: 0.2181\n",
            "Epoch 66/200 loss: 0.2175\n",
            "Epoch 67/200 loss: 0.2175\n",
            "Epoch 68/200 loss: 0.2170\n",
            "Epoch 69/200 loss: 0.2165\n",
            "Epoch 70/200 loss: 0.2155\n",
            "Epoch 71/200 loss: 0.2159\n",
            "Epoch 72/200 loss: 0.2160\n",
            "Epoch 73/200 loss: 0.2167\n",
            "Epoch 74/200 loss: 0.2165\n",
            "Epoch 75/200 loss: 0.2166\n",
            "Epoch 76/200 loss: 0.2154\n",
            "Epoch 77/200 loss: 0.2149\n",
            "Epoch 78/200 loss: 0.2137\n",
            "Epoch 79/200 loss: 0.2145\n",
            "Epoch 80/200 loss: 0.2146\n",
            "Epoch 81/200 loss: 0.2134\n",
            "Epoch 82/200 loss: 0.2140\n",
            "Epoch 83/200 loss: 0.2140\n",
            "Epoch 84/200 loss: 0.2132\n",
            "Epoch 85/200 loss: 0.2130\n",
            "Epoch 86/200 loss: 0.2128\n",
            "Epoch 87/200 loss: 0.2132\n",
            "Epoch 88/200 loss: 0.2129\n",
            "Epoch 89/200 loss: 0.2127\n",
            "Epoch 90/200 loss: 0.2123\n",
            "Epoch 91/200 loss: 0.2116\n",
            "Epoch 92/200 loss: 0.2112\n",
            "Epoch 93/200 loss: 0.2115\n",
            "Epoch 94/200 loss: 0.2114\n",
            "Epoch 95/200 loss: 0.2119\n",
            "Epoch 96/200 loss: 0.2114\n",
            "Epoch 97/200 loss: 0.2109\n",
            "Epoch 98/200 loss: 0.2116\n",
            "Epoch 99/200 loss: 0.2112\n",
            "Epoch 100/200 loss: 0.2104\n",
            "Epoch 101/200 loss: 0.2099\n",
            "Epoch 102/200 loss: 0.2104\n",
            "Epoch 103/200 loss: 0.2106\n",
            "Epoch 104/200 loss: 0.2096\n",
            "Epoch 105/200 loss: 0.2094\n",
            "Epoch 106/200 loss: 0.2101\n",
            "Epoch 107/200 loss: 0.2096\n",
            "Epoch 108/200 loss: 0.2092\n",
            "Epoch 109/200 loss: 0.2093\n",
            "Epoch 110/200 loss: 0.2088\n",
            "Epoch 111/200 loss: 0.2095\n",
            "Epoch 112/200 loss: 0.2080\n",
            "Epoch 113/200 loss: 0.2083\n",
            "Epoch 114/200 loss: 0.2086\n",
            "Epoch 115/200 loss: 0.2089\n",
            "Epoch 116/200 loss: 0.2084\n",
            "Epoch 117/200 loss: 0.2076\n",
            "Epoch 118/200 loss: 0.2074\n",
            "Epoch 119/200 loss: 0.2083\n",
            "Epoch 120/200 loss: 0.2077\n",
            "Epoch 121/200 loss: 0.2083\n",
            "Epoch 122/200 loss: 0.2082\n",
            "Epoch 123/200 loss: 0.2078\n",
            "Epoch 124/200 loss: 0.2076\n",
            "Epoch 125/200 loss: 0.2079\n",
            "Epoch 126/200 loss: 0.2069\n",
            "Epoch 127/200 loss: 0.2079\n",
            "Epoch 128/200 loss: 0.2077\n",
            "Epoch 129/200 loss: 0.2063\n",
            "Epoch 130/200 loss: 0.2074\n",
            "Epoch 131/200 loss: 0.2062\n",
            "Epoch 132/200 loss: 0.2066\n",
            "Epoch 133/200 loss: 0.2058\n",
            "Epoch 134/200 loss: 0.2065\n",
            "Epoch 135/200 loss: 0.2064\n",
            "Epoch 136/200 loss: 0.2056\n",
            "Epoch 137/200 loss: 0.2060\n",
            "Epoch 138/200 loss: 0.2056\n",
            "Epoch 139/200 loss: 0.2057\n",
            "Epoch 140/200 loss: 0.2054\n",
            "Epoch 141/200 loss: 0.2056\n",
            "Epoch 142/200 loss: 0.2061\n",
            "Epoch 143/200 loss: 0.2055\n",
            "Epoch 144/200 loss: 0.2053\n",
            "Epoch 145/200 loss: 0.2053\n",
            "Epoch 146/200 loss: 0.2051\n",
            "Epoch 147/200 loss: 0.2049\n",
            "Epoch 148/200 loss: 0.2050\n",
            "Epoch 149/200 loss: 0.2054\n",
            "Epoch 150/200 loss: 0.2054\n",
            "Epoch 151/200 loss: 0.2045\n",
            "Epoch 152/200 loss: 0.2048\n",
            "Epoch 153/200 loss: 0.2051\n",
            "Epoch 154/200 loss: 0.2042\n",
            "Epoch 155/200 loss: 0.2041\n",
            "Epoch 156/200 loss: 0.2056\n",
            "Epoch 157/200 loss: 0.2041\n",
            "Epoch 158/200 loss: 0.2041\n",
            "Epoch 159/200 loss: 0.2037\n",
            "Epoch 160/200 loss: 0.2037\n",
            "Epoch 161/200 loss: 0.2047\n",
            "Epoch 162/200 loss: 0.2043\n",
            "Epoch 163/200 loss: 0.2041\n",
            "Epoch 164/200 loss: 0.2050\n",
            "Epoch 165/200 loss: 0.2032\n",
            "Epoch 166/200 loss: 0.2028\n",
            "Epoch 167/200 loss: 0.2039\n",
            "Epoch 168/200 loss: 0.2032\n",
            "Epoch 169/200 loss: 0.2040\n",
            "Epoch 170/200 loss: 0.2046\n",
            "Epoch 171/200 loss: 0.2034\n",
            "Epoch 172/200 loss: 0.2037\n",
            "Epoch 173/200 loss: 0.2027\n",
            "Epoch 174/200 loss: 0.2031\n",
            "Epoch 175/200 loss: 0.2027\n",
            "Epoch 176/200 loss: 0.2022\n",
            "Epoch 177/200 loss: 0.2033\n",
            "Epoch 178/200 loss: 0.2029\n",
            "Epoch 179/200 loss: 0.2033\n",
            "Epoch 180/200 loss: 0.2029\n",
            "Epoch 181/200 loss: 0.2027\n",
            "Epoch 182/200 loss: 0.2024\n",
            "Epoch 183/200 loss: 0.2024\n",
            "Epoch 184/200 loss: 0.2018\n",
            "Epoch 185/200 loss: 0.2020\n",
            "Epoch 186/200 loss: 0.2021\n",
            "Epoch 187/200 loss: 0.2023\n",
            "Epoch 188/200 loss: 0.2018\n",
            "Epoch 189/200 loss: 0.2022\n",
            "Epoch 190/200 loss: 0.2020\n",
            "Epoch 191/200 loss: 0.2017\n",
            "Epoch 192/200 loss: 0.2020\n",
            "Epoch 193/200 loss: 0.2018\n",
            "Epoch 194/200 loss: 0.2020\n",
            "Epoch 195/200 loss: 0.2020\n",
            "Epoch 196/200 loss: 0.2015\n",
            "Epoch 197/200 loss: 0.2016\n",
            "Epoch 198/200 loss: 0.2014\n",
            "Epoch 199/200 loss: 0.2028\n",
            "Epoch 200/200 loss: 0.2017\n",
            "batch size: 256, learning rate: 0.05, num epochs: 20\n",
            "Epoch 1/20 loss: 1.1155\n",
            "Epoch 2/20 loss: 0.8671\n",
            "Epoch 3/20 loss: 0.6880\n",
            "Epoch 4/20 loss: 0.5748\n",
            "Epoch 5/20 loss: 0.5021\n",
            "Epoch 6/20 loss: 0.4624\n",
            "Epoch 7/20 loss: 0.4300\n",
            "Epoch 8/20 loss: 0.4098\n",
            "Epoch 9/20 loss: 0.3930\n",
            "Epoch 10/20 loss: 0.3781\n",
            "Epoch 11/20 loss: 0.3655\n",
            "Epoch 12/20 loss: 0.3562\n",
            "Epoch 13/20 loss: 0.3460\n",
            "Epoch 14/20 loss: 0.3386\n",
            "Epoch 15/20 loss: 0.3310\n",
            "Epoch 16/20 loss: 0.3239\n",
            "Epoch 17/20 loss: 0.3198\n",
            "Epoch 18/20 loss: 0.3140\n",
            "Epoch 19/20 loss: 0.3103\n",
            "Epoch 20/20 loss: 0.3058\n",
            "batch size: 256, learning rate: 0.05, num epochs: 50\n",
            "Epoch 1/50 loss: 1.7208\n",
            "Epoch 2/50 loss: 1.3115\n",
            "Epoch 3/50 loss: 1.0352\n",
            "Epoch 4/50 loss: 0.8444\n",
            "Epoch 5/50 loss: 0.7185\n",
            "Epoch 6/50 loss: 0.6267\n",
            "Epoch 7/50 loss: 0.5598\n",
            "Epoch 8/50 loss: 0.5068\n",
            "Epoch 9/50 loss: 0.4679\n",
            "Epoch 10/50 loss: 0.4390\n",
            "Epoch 11/50 loss: 0.4131\n",
            "Epoch 12/50 loss: 0.3944\n",
            "Epoch 13/50 loss: 0.3772\n",
            "Epoch 14/50 loss: 0.3645\n",
            "Epoch 15/50 loss: 0.3519\n",
            "Epoch 16/50 loss: 0.3434\n",
            "Epoch 17/50 loss: 0.3340\n",
            "Epoch 18/50 loss: 0.3282\n",
            "Epoch 19/50 loss: 0.3227\n",
            "Epoch 20/50 loss: 0.3165\n",
            "Epoch 21/50 loss: 0.3112\n",
            "Epoch 22/50 loss: 0.3077\n",
            "Epoch 23/50 loss: 0.3027\n",
            "Epoch 24/50 loss: 0.2995\n",
            "Epoch 25/50 loss: 0.2969\n",
            "Epoch 26/50 loss: 0.2925\n",
            "Epoch 27/50 loss: 0.2916\n",
            "Epoch 28/50 loss: 0.2882\n",
            "Epoch 29/50 loss: 0.2852\n",
            "Epoch 30/50 loss: 0.2839\n",
            "Epoch 31/50 loss: 0.2825\n",
            "Epoch 32/50 loss: 0.2802\n",
            "Epoch 33/50 loss: 0.2778\n",
            "Epoch 34/50 loss: 0.2760\n",
            "Epoch 35/50 loss: 0.2764\n",
            "Epoch 36/50 loss: 0.2733\n",
            "Epoch 37/50 loss: 0.2719\n",
            "Epoch 38/50 loss: 0.2700\n",
            "Epoch 39/50 loss: 0.2687\n",
            "Epoch 40/50 loss: 0.2682\n",
            "Epoch 41/50 loss: 0.2662\n",
            "Epoch 42/50 loss: 0.2654\n",
            "Epoch 43/50 loss: 0.2645\n",
            "Epoch 44/50 loss: 0.2633\n",
            "Epoch 45/50 loss: 0.2620\n",
            "Epoch 46/50 loss: 0.2611\n",
            "Epoch 47/50 loss: 0.2606\n",
            "Epoch 48/50 loss: 0.2596\n",
            "Epoch 49/50 loss: 0.2605\n",
            "Epoch 50/50 loss: 0.2583\n",
            "batch size: 256, learning rate: 0.05, num epochs: 100\n",
            "Epoch 1/100 loss: 1.3656\n",
            "Epoch 2/100 loss: 1.0798\n",
            "Epoch 3/100 loss: 0.8934\n",
            "Epoch 4/100 loss: 0.7607\n",
            "Epoch 5/100 loss: 0.6603\n",
            "Epoch 6/100 loss: 0.5824\n",
            "Epoch 7/100 loss: 0.5257\n",
            "Epoch 8/100 loss: 0.4791\n",
            "Epoch 9/100 loss: 0.4456\n",
            "Epoch 10/100 loss: 0.4183\n",
            "Epoch 11/100 loss: 0.3947\n",
            "Epoch 12/100 loss: 0.3764\n",
            "Epoch 13/100 loss: 0.3598\n",
            "Epoch 14/100 loss: 0.3479\n",
            "Epoch 15/100 loss: 0.3376\n",
            "Epoch 16/100 loss: 0.3293\n",
            "Epoch 17/100 loss: 0.3197\n",
            "Epoch 18/100 loss: 0.3148\n",
            "Epoch 19/100 loss: 0.3109\n",
            "Epoch 20/100 loss: 0.3033\n",
            "Epoch 21/100 loss: 0.2990\n",
            "Epoch 22/100 loss: 0.2960\n",
            "Epoch 23/100 loss: 0.2929\n",
            "Epoch 24/100 loss: 0.2885\n",
            "Epoch 25/100 loss: 0.2856\n",
            "Epoch 26/100 loss: 0.2822\n",
            "Epoch 27/100 loss: 0.2811\n",
            "Epoch 28/100 loss: 0.2779\n",
            "Epoch 29/100 loss: 0.2773\n",
            "Epoch 30/100 loss: 0.2749\n",
            "Epoch 31/100 loss: 0.2732\n",
            "Epoch 32/100 loss: 0.2709\n",
            "Epoch 33/100 loss: 0.2692\n",
            "Epoch 34/100 loss: 0.2692\n",
            "Epoch 35/100 loss: 0.2673\n",
            "Epoch 36/100 loss: 0.2656\n",
            "Epoch 37/100 loss: 0.2638\n",
            "Epoch 38/100 loss: 0.2632\n",
            "Epoch 39/100 loss: 0.2619\n",
            "Epoch 40/100 loss: 0.2626\n",
            "Epoch 41/100 loss: 0.2612\n",
            "Epoch 42/100 loss: 0.2591\n",
            "Epoch 43/100 loss: 0.2577\n",
            "Epoch 44/100 loss: 0.2578\n",
            "Epoch 45/100 loss: 0.2573\n",
            "Epoch 46/100 loss: 0.2559\n",
            "Epoch 47/100 loss: 0.2553\n",
            "Epoch 48/100 loss: 0.2538\n",
            "Epoch 49/100 loss: 0.2543\n",
            "Epoch 50/100 loss: 0.2529\n",
            "Epoch 51/100 loss: 0.2539\n",
            "Epoch 52/100 loss: 0.2511\n",
            "Epoch 53/100 loss: 0.2519\n",
            "Epoch 54/100 loss: 0.2506\n",
            "Epoch 55/100 loss: 0.2505\n",
            "Epoch 56/100 loss: 0.2504\n",
            "Epoch 57/100 loss: 0.2485\n",
            "Epoch 58/100 loss: 0.2482\n",
            "Epoch 59/100 loss: 0.2476\n",
            "Epoch 60/100 loss: 0.2468\n",
            "Epoch 61/100 loss: 0.2467\n",
            "Epoch 62/100 loss: 0.2458\n",
            "Epoch 63/100 loss: 0.2465\n",
            "Epoch 64/100 loss: 0.2452\n",
            "Epoch 65/100 loss: 0.2451\n",
            "Epoch 66/100 loss: 0.2442\n",
            "Epoch 67/100 loss: 0.2450\n",
            "Epoch 68/100 loss: 0.2445\n",
            "Epoch 69/100 loss: 0.2422\n",
            "Epoch 70/100 loss: 0.2442\n",
            "Epoch 71/100 loss: 0.2422\n",
            "Epoch 72/100 loss: 0.2433\n",
            "Epoch 73/100 loss: 0.2419\n",
            "Epoch 74/100 loss: 0.2426\n",
            "Epoch 75/100 loss: 0.2405\n",
            "Epoch 76/100 loss: 0.2411\n",
            "Epoch 77/100 loss: 0.2411\n",
            "Epoch 78/100 loss: 0.2393\n",
            "Epoch 79/100 loss: 0.2404\n",
            "Epoch 80/100 loss: 0.2406\n",
            "Epoch 81/100 loss: 0.2400\n",
            "Epoch 82/100 loss: 0.2380\n",
            "Epoch 83/100 loss: 0.2379\n",
            "Epoch 84/100 loss: 0.2376\n",
            "Epoch 85/100 loss: 0.2366\n",
            "Epoch 86/100 loss: 0.2377\n",
            "Epoch 87/100 loss: 0.2375\n",
            "Epoch 88/100 loss: 0.2362\n",
            "Epoch 89/100 loss: 0.2362\n",
            "Epoch 90/100 loss: 0.2352\n",
            "Epoch 91/100 loss: 0.2355\n",
            "Epoch 92/100 loss: 0.2354\n",
            "Epoch 93/100 loss: 0.2353\n",
            "Epoch 94/100 loss: 0.2358\n",
            "Epoch 95/100 loss: 0.2345\n",
            "Epoch 96/100 loss: 0.2352\n",
            "Epoch 97/100 loss: 0.2356\n",
            "Epoch 98/100 loss: 0.2337\n",
            "Epoch 99/100 loss: 0.2327\n",
            "Epoch 100/100 loss: 0.2339\n",
            "batch size: 256, learning rate: 0.05, num epochs: 200\n",
            "Epoch 1/200 loss: 1.4698\n",
            "Epoch 2/200 loss: 1.0619\n",
            "Epoch 3/200 loss: 0.8329\n",
            "Epoch 4/200 loss: 0.6934\n",
            "Epoch 5/200 loss: 0.5983\n",
            "Epoch 6/200 loss: 0.5358\n",
            "Epoch 7/200 loss: 0.4865\n",
            "Epoch 8/200 loss: 0.4507\n",
            "Epoch 9/200 loss: 0.4207\n",
            "Epoch 10/200 loss: 0.3978\n",
            "Epoch 11/200 loss: 0.3790\n",
            "Epoch 12/200 loss: 0.3630\n",
            "Epoch 13/200 loss: 0.3501\n",
            "Epoch 14/200 loss: 0.3388\n",
            "Epoch 15/200 loss: 0.3285\n",
            "Epoch 16/200 loss: 0.3201\n",
            "Epoch 17/200 loss: 0.3145\n",
            "Epoch 18/200 loss: 0.3071\n",
            "Epoch 19/200 loss: 0.3017\n",
            "Epoch 20/200 loss: 0.2979\n",
            "Epoch 21/200 loss: 0.2933\n",
            "Epoch 22/200 loss: 0.2894\n",
            "Epoch 23/200 loss: 0.2851\n",
            "Epoch 24/200 loss: 0.2824\n",
            "Epoch 25/200 loss: 0.2787\n",
            "Epoch 26/200 loss: 0.2767\n",
            "Epoch 27/200 loss: 0.2734\n",
            "Epoch 28/200 loss: 0.2720\n",
            "Epoch 29/200 loss: 0.2689\n",
            "Epoch 30/200 loss: 0.2674\n",
            "Epoch 31/200 loss: 0.2669\n",
            "Epoch 32/200 loss: 0.2640\n",
            "Epoch 33/200 loss: 0.2633\n",
            "Epoch 34/200 loss: 0.2612\n",
            "Epoch 35/200 loss: 0.2607\n",
            "Epoch 36/200 loss: 0.2583\n",
            "Epoch 37/200 loss: 0.2570\n",
            "Epoch 38/200 loss: 0.2564\n",
            "Epoch 39/200 loss: 0.2556\n",
            "Epoch 40/200 loss: 0.2550\n",
            "Epoch 41/200 loss: 0.2537\n",
            "Epoch 42/200 loss: 0.2513\n",
            "Epoch 43/200 loss: 0.2515\n",
            "Epoch 44/200 loss: 0.2500\n",
            "Epoch 45/200 loss: 0.2488\n",
            "Epoch 46/200 loss: 0.2491\n",
            "Epoch 47/200 loss: 0.2482\n",
            "Epoch 48/200 loss: 0.2468\n",
            "Epoch 49/200 loss: 0.2478\n",
            "Epoch 50/200 loss: 0.2469\n",
            "Epoch 51/200 loss: 0.2452\n",
            "Epoch 52/200 loss: 0.2445\n",
            "Epoch 53/200 loss: 0.2443\n",
            "Epoch 54/200 loss: 0.2433\n",
            "Epoch 55/200 loss: 0.2424\n",
            "Epoch 56/200 loss: 0.2423\n",
            "Epoch 57/200 loss: 0.2416\n",
            "Epoch 58/200 loss: 0.2418\n",
            "Epoch 59/200 loss: 0.2409\n",
            "Epoch 60/200 loss: 0.2410\n",
            "Epoch 61/200 loss: 0.2398\n",
            "Epoch 62/200 loss: 0.2387\n",
            "Epoch 63/200 loss: 0.2395\n",
            "Epoch 64/200 loss: 0.2383\n",
            "Epoch 65/200 loss: 0.2376\n",
            "Epoch 66/200 loss: 0.2374\n",
            "Epoch 67/200 loss: 0.2366\n",
            "Epoch 68/200 loss: 0.2370\n",
            "Epoch 69/200 loss: 0.2366\n",
            "Epoch 70/200 loss: 0.2364\n",
            "Epoch 71/200 loss: 0.2360\n",
            "Epoch 72/200 loss: 0.2357\n",
            "Epoch 73/200 loss: 0.2350\n",
            "Epoch 74/200 loss: 0.2349\n",
            "Epoch 75/200 loss: 0.2355\n",
            "Epoch 76/200 loss: 0.2352\n",
            "Epoch 77/200 loss: 0.2333\n",
            "Epoch 78/200 loss: 0.2344\n",
            "Epoch 79/200 loss: 0.2338\n",
            "Epoch 80/200 loss: 0.2327\n",
            "Epoch 81/200 loss: 0.2326\n",
            "Epoch 82/200 loss: 0.2322\n",
            "Epoch 83/200 loss: 0.2319\n",
            "Epoch 84/200 loss: 0.2323\n",
            "Epoch 85/200 loss: 0.2311\n",
            "Epoch 86/200 loss: 0.2322\n",
            "Epoch 87/200 loss: 0.2310\n",
            "Epoch 88/200 loss: 0.2317\n",
            "Epoch 89/200 loss: 0.2314\n",
            "Epoch 90/200 loss: 0.2305\n",
            "Epoch 91/200 loss: 0.2311\n",
            "Epoch 92/200 loss: 0.2293\n",
            "Epoch 93/200 loss: 0.2300\n",
            "Epoch 94/200 loss: 0.2294\n",
            "Epoch 95/200 loss: 0.2293\n",
            "Epoch 96/200 loss: 0.2287\n",
            "Epoch 97/200 loss: 0.2289\n",
            "Epoch 98/200 loss: 0.2286\n",
            "Epoch 99/200 loss: 0.2274\n",
            "Epoch 100/200 loss: 0.2284\n",
            "Epoch 101/200 loss: 0.2281\n",
            "Epoch 102/200 loss: 0.2278\n",
            "Epoch 103/200 loss: 0.2266\n",
            "Epoch 104/200 loss: 0.2270\n",
            "Epoch 105/200 loss: 0.2270\n",
            "Epoch 106/200 loss: 0.2269\n",
            "Epoch 107/200 loss: 0.2263\n",
            "Epoch 108/200 loss: 0.2267\n",
            "Epoch 109/200 loss: 0.2269\n",
            "Epoch 110/200 loss: 0.2264\n",
            "Epoch 111/200 loss: 0.2263\n",
            "Epoch 112/200 loss: 0.2261\n",
            "Epoch 113/200 loss: 0.2249\n",
            "Epoch 114/200 loss: 0.2253\n",
            "Epoch 115/200 loss: 0.2264\n",
            "Epoch 116/200 loss: 0.2244\n",
            "Epoch 117/200 loss: 0.2256\n",
            "Epoch 118/200 loss: 0.2251\n",
            "Epoch 119/200 loss: 0.2238\n",
            "Epoch 120/200 loss: 0.2238\n",
            "Epoch 121/200 loss: 0.2252\n",
            "Epoch 122/200 loss: 0.2242\n",
            "Epoch 123/200 loss: 0.2242\n",
            "Epoch 124/200 loss: 0.2238\n",
            "Epoch 125/200 loss: 0.2238\n",
            "Epoch 126/200 loss: 0.2229\n",
            "Epoch 127/200 loss: 0.2233\n",
            "Epoch 128/200 loss: 0.2233\n",
            "Epoch 129/200 loss: 0.2234\n",
            "Epoch 130/200 loss: 0.2221\n",
            "Epoch 131/200 loss: 0.2219\n",
            "Epoch 132/200 loss: 0.2221\n",
            "Epoch 133/200 loss: 0.2221\n",
            "Epoch 134/200 loss: 0.2228\n",
            "Epoch 135/200 loss: 0.2219\n",
            "Epoch 136/200 loss: 0.2219\n",
            "Epoch 137/200 loss: 0.2219\n",
            "Epoch 138/200 loss: 0.2212\n",
            "Epoch 139/200 loss: 0.2224\n",
            "Epoch 140/200 loss: 0.2209\n",
            "Epoch 141/200 loss: 0.2209\n",
            "Epoch 142/200 loss: 0.2203\n",
            "Epoch 143/200 loss: 0.2206\n",
            "Epoch 144/200 loss: 0.2218\n",
            "Epoch 145/200 loss: 0.2209\n",
            "Epoch 146/200 loss: 0.2211\n",
            "Epoch 147/200 loss: 0.2191\n",
            "Epoch 148/200 loss: 0.2202\n",
            "Epoch 149/200 loss: 0.2205\n",
            "Epoch 150/200 loss: 0.2212\n",
            "Epoch 151/200 loss: 0.2205\n",
            "Epoch 152/200 loss: 0.2196\n",
            "Epoch 153/200 loss: 0.2204\n",
            "Epoch 154/200 loss: 0.2193\n",
            "Epoch 155/200 loss: 0.2198\n",
            "Epoch 156/200 loss: 0.2189\n",
            "Epoch 157/200 loss: 0.2190\n",
            "Epoch 158/200 loss: 0.2184\n",
            "Epoch 159/200 loss: 0.2199\n",
            "Epoch 160/200 loss: 0.2190\n",
            "Epoch 161/200 loss: 0.2186\n",
            "Epoch 162/200 loss: 0.2190\n",
            "Epoch 163/200 loss: 0.2196\n",
            "Epoch 164/200 loss: 0.2182\n",
            "Epoch 165/200 loss: 0.2181\n",
            "Epoch 166/200 loss: 0.2183\n",
            "Epoch 167/200 loss: 0.2177\n",
            "Epoch 168/200 loss: 0.2186\n",
            "Epoch 169/200 loss: 0.2179\n",
            "Epoch 170/200 loss: 0.2178\n",
            "Epoch 171/200 loss: 0.2184\n",
            "Epoch 172/200 loss: 0.2174\n",
            "Epoch 173/200 loss: 0.2172\n",
            "Epoch 174/200 loss: 0.2176\n",
            "Epoch 175/200 loss: 0.2166\n",
            "Epoch 176/200 loss: 0.2166\n",
            "Epoch 177/200 loss: 0.2170\n",
            "Epoch 178/200 loss: 0.2162\n",
            "Epoch 179/200 loss: 0.2165\n",
            "Epoch 180/200 loss: 0.2170\n",
            "Epoch 181/200 loss: 0.2174\n",
            "Epoch 182/200 loss: 0.2168\n",
            "Epoch 183/200 loss: 0.2165\n",
            "Epoch 184/200 loss: 0.2166\n",
            "Epoch 185/200 loss: 0.2168\n",
            "Epoch 186/200 loss: 0.2161\n",
            "Epoch 187/200 loss: 0.2165\n",
            "Epoch 188/200 loss: 0.2157\n",
            "Epoch 189/200 loss: 0.2162\n",
            "Epoch 190/200 loss: 0.2166\n",
            "Epoch 191/200 loss: 0.2148\n",
            "Epoch 192/200 loss: 0.2164\n",
            "Epoch 193/200 loss: 0.2152\n",
            "Epoch 194/200 loss: 0.2160\n",
            "Epoch 195/200 loss: 0.2152\n",
            "Epoch 196/200 loss: 0.2154\n",
            "Epoch 197/200 loss: 0.2157\n",
            "Epoch 198/200 loss: 0.2143\n",
            "Epoch 199/200 loss: 0.2152\n",
            "Epoch 200/200 loss: 0.2145\n",
            "batch size: 256, learning rate: 0.01, num epochs: 20\n",
            "Epoch 1/20 loss: 2.0714\n",
            "Epoch 2/20 loss: 1.9406\n",
            "Epoch 3/20 loss: 1.8231\n",
            "Epoch 4/20 loss: 1.7139\n",
            "Epoch 5/20 loss: 1.6153\n",
            "Epoch 6/20 loss: 1.5222\n",
            "Epoch 7/20 loss: 1.4346\n",
            "Epoch 8/20 loss: 1.3536\n",
            "Epoch 9/20 loss: 1.2770\n",
            "Epoch 10/20 loss: 1.2047\n",
            "Epoch 11/20 loss: 1.1387\n",
            "Epoch 12/20 loss: 1.0764\n",
            "Epoch 13/20 loss: 1.0191\n",
            "Epoch 14/20 loss: 0.9674\n",
            "Epoch 15/20 loss: 0.9181\n",
            "Epoch 16/20 loss: 0.8748\n",
            "Epoch 17/20 loss: 0.8344\n",
            "Epoch 18/20 loss: 0.7976\n",
            "Epoch 19/20 loss: 0.7630\n",
            "Epoch 20/20 loss: 0.7318\n",
            "batch size: 256, learning rate: 0.01, num epochs: 50\n",
            "Epoch 1/50 loss: 2.0698\n",
            "Epoch 2/50 loss: 1.9491\n",
            "Epoch 3/50 loss: 1.8447\n",
            "Epoch 4/50 loss: 1.7385\n",
            "Epoch 5/50 loss: 1.6390\n",
            "Epoch 6/50 loss: 1.5467\n",
            "Epoch 7/50 loss: 1.4540\n",
            "Epoch 8/50 loss: 1.3743\n",
            "Epoch 9/50 loss: 1.2989\n",
            "Epoch 10/50 loss: 1.2276\n",
            "Epoch 11/50 loss: 1.1610\n",
            "Epoch 12/50 loss: 1.1056\n",
            "Epoch 13/50 loss: 1.0523\n",
            "Epoch 14/50 loss: 1.0046\n",
            "Epoch 15/50 loss: 0.9612\n",
            "Epoch 16/50 loss: 0.9219\n",
            "Epoch 17/50 loss: 0.8891\n",
            "Epoch 18/50 loss: 0.8573\n",
            "Epoch 19/50 loss: 0.8266\n",
            "Epoch 20/50 loss: 0.7987\n",
            "Epoch 21/50 loss: 0.7715\n",
            "Epoch 22/50 loss: 0.7484\n",
            "Epoch 23/50 loss: 0.7250\n",
            "Epoch 24/50 loss: 0.7036\n",
            "Epoch 25/50 loss: 0.6848\n",
            "Epoch 26/50 loss: 0.6664\n",
            "Epoch 27/50 loss: 0.6485\n",
            "Epoch 28/50 loss: 0.6322\n",
            "Epoch 29/50 loss: 0.6161\n",
            "Epoch 30/50 loss: 0.6010\n",
            "Epoch 31/50 loss: 0.5886\n",
            "Epoch 32/50 loss: 0.5745\n",
            "Epoch 33/50 loss: 0.5626\n",
            "Epoch 34/50 loss: 0.5509\n",
            "Epoch 35/50 loss: 0.5398\n",
            "Epoch 36/50 loss: 0.5291\n",
            "Epoch 37/50 loss: 0.5190\n",
            "Epoch 38/50 loss: 0.5083\n",
            "Epoch 39/50 loss: 0.4998\n",
            "Epoch 40/50 loss: 0.4915\n",
            "Epoch 41/50 loss: 0.4827\n",
            "Epoch 42/50 loss: 0.4749\n",
            "Epoch 43/50 loss: 0.4669\n",
            "Epoch 44/50 loss: 0.4619\n",
            "Epoch 45/50 loss: 0.4533\n",
            "Epoch 46/50 loss: 0.4481\n",
            "Epoch 47/50 loss: 0.4410\n",
            "Epoch 48/50 loss: 0.4345\n",
            "Epoch 49/50 loss: 0.4298\n",
            "Epoch 50/50 loss: 0.4240\n",
            "batch size: 256, learning rate: 0.01, num epochs: 100\n",
            "Epoch 1/100 loss: 1.2296\n",
            "Epoch 2/100 loss: 1.1554\n",
            "Epoch 3/100 loss: 1.0874\n",
            "Epoch 4/100 loss: 1.0317\n",
            "Epoch 5/100 loss: 0.9788\n",
            "Epoch 6/100 loss: 0.9292\n",
            "Epoch 7/100 loss: 0.8896\n",
            "Epoch 8/100 loss: 0.8509\n",
            "Epoch 9/100 loss: 0.8189\n",
            "Epoch 10/100 loss: 0.7864\n",
            "Epoch 11/100 loss: 0.7580\n",
            "Epoch 12/100 loss: 0.7330\n",
            "Epoch 13/100 loss: 0.7087\n",
            "Epoch 14/100 loss: 0.6872\n",
            "Epoch 15/100 loss: 0.6644\n",
            "Epoch 16/100 loss: 0.6460\n",
            "Epoch 17/100 loss: 0.6283\n",
            "Epoch 18/100 loss: 0.6105\n",
            "Epoch 19/100 loss: 0.5949\n",
            "Epoch 20/100 loss: 0.5803\n",
            "Epoch 21/100 loss: 0.5662\n",
            "Epoch 22/100 loss: 0.5532\n",
            "Epoch 23/100 loss: 0.5397\n",
            "Epoch 24/100 loss: 0.5296\n",
            "Epoch 25/100 loss: 0.5180\n",
            "Epoch 26/100 loss: 0.5090\n",
            "Epoch 27/100 loss: 0.4992\n",
            "Epoch 28/100 loss: 0.4883\n",
            "Epoch 29/100 loss: 0.4801\n",
            "Epoch 30/100 loss: 0.4727\n",
            "Epoch 31/100 loss: 0.4659\n",
            "Epoch 32/100 loss: 0.4577\n",
            "Epoch 33/100 loss: 0.4511\n",
            "Epoch 34/100 loss: 0.4440\n",
            "Epoch 35/100 loss: 0.4376\n",
            "Epoch 36/100 loss: 0.4314\n",
            "Epoch 37/100 loss: 0.4269\n",
            "Epoch 38/100 loss: 0.4202\n",
            "Epoch 39/100 loss: 0.4154\n",
            "Epoch 40/100 loss: 0.4110\n",
            "Epoch 41/100 loss: 0.4060\n",
            "Epoch 42/100 loss: 0.4016\n",
            "Epoch 43/100 loss: 0.3967\n",
            "Epoch 44/100 loss: 0.3927\n",
            "Epoch 45/100 loss: 0.3904\n",
            "Epoch 46/100 loss: 0.3843\n",
            "Epoch 47/100 loss: 0.3842\n",
            "Epoch 48/100 loss: 0.3781\n",
            "Epoch 49/100 loss: 0.3744\n",
            "Epoch 50/100 loss: 0.3715\n",
            "Epoch 51/100 loss: 0.3687\n",
            "Epoch 52/100 loss: 0.3653\n",
            "Epoch 53/100 loss: 0.3634\n",
            "Epoch 54/100 loss: 0.3602\n",
            "Epoch 55/100 loss: 0.3579\n",
            "Epoch 56/100 loss: 0.3546\n",
            "Epoch 57/100 loss: 0.3540\n",
            "Epoch 58/100 loss: 0.3511\n",
            "Epoch 59/100 loss: 0.3487\n",
            "Epoch 60/100 loss: 0.3477\n",
            "Epoch 61/100 loss: 0.3454\n",
            "Epoch 62/100 loss: 0.3422\n",
            "Epoch 63/100 loss: 0.3407\n",
            "Epoch 64/100 loss: 0.3395\n",
            "Epoch 65/100 loss: 0.3378\n",
            "Epoch 66/100 loss: 0.3350\n",
            "Epoch 67/100 loss: 0.3340\n",
            "Epoch 68/100 loss: 0.3312\n",
            "Epoch 69/100 loss: 0.3294\n",
            "Epoch 70/100 loss: 0.3287\n",
            "Epoch 71/100 loss: 0.3277\n",
            "Epoch 72/100 loss: 0.3260\n",
            "Epoch 73/100 loss: 0.3234\n",
            "Epoch 74/100 loss: 0.3228\n",
            "Epoch 75/100 loss: 0.3212\n",
            "Epoch 76/100 loss: 0.3199\n",
            "Epoch 77/100 loss: 0.3194\n",
            "Epoch 78/100 loss: 0.3174\n",
            "Epoch 79/100 loss: 0.3156\n",
            "Epoch 80/100 loss: 0.3144\n",
            "Epoch 81/100 loss: 0.3153\n",
            "Epoch 82/100 loss: 0.3132\n",
            "Epoch 83/100 loss: 0.3116\n",
            "Epoch 84/100 loss: 0.3113\n",
            "Epoch 85/100 loss: 0.3094\n",
            "Epoch 86/100 loss: 0.3089\n",
            "Epoch 87/100 loss: 0.3072\n",
            "Epoch 88/100 loss: 0.3073\n",
            "Epoch 89/100 loss: 0.3056\n",
            "Epoch 90/100 loss: 0.3046\n",
            "Epoch 91/100 loss: 0.3038\n",
            "Epoch 92/100 loss: 0.3035\n",
            "Epoch 93/100 loss: 0.3017\n",
            "Epoch 94/100 loss: 0.3009\n",
            "Epoch 95/100 loss: 0.3017\n",
            "Epoch 96/100 loss: 0.2993\n",
            "Epoch 97/100 loss: 0.2990\n",
            "Epoch 98/100 loss: 0.2991\n",
            "Epoch 99/100 loss: 0.2975\n",
            "Epoch 100/100 loss: 0.2962\n",
            "batch size: 256, learning rate: 0.01, num epochs: 200\n",
            "Epoch 1/200 loss: 2.0736\n",
            "Epoch 2/200 loss: 1.7859\n",
            "Epoch 3/200 loss: 1.6106\n",
            "Epoch 4/200 loss: 1.4944\n",
            "Epoch 5/200 loss: 1.4145\n",
            "Epoch 6/200 loss: 1.3409\n",
            "Epoch 7/200 loss: 1.2810\n",
            "Epoch 8/200 loss: 1.2257\n",
            "Epoch 9/200 loss: 1.1766\n",
            "Epoch 10/200 loss: 1.1309\n",
            "Epoch 11/200 loss: 1.0844\n",
            "Epoch 12/200 loss: 1.0438\n",
            "Epoch 13/200 loss: 1.0058\n",
            "Epoch 14/200 loss: 0.9705\n",
            "Epoch 15/200 loss: 0.9360\n",
            "Epoch 16/200 loss: 0.9042\n",
            "Epoch 17/200 loss: 0.8755\n",
            "Epoch 18/200 loss: 0.8466\n",
            "Epoch 19/200 loss: 0.8209\n",
            "Epoch 20/200 loss: 0.7948\n",
            "Epoch 21/200 loss: 0.7704\n",
            "Epoch 22/200 loss: 0.7494\n",
            "Epoch 23/200 loss: 0.7279\n",
            "Epoch 24/200 loss: 0.7088\n",
            "Epoch 25/200 loss: 0.6909\n",
            "Epoch 26/200 loss: 0.6737\n",
            "Epoch 27/200 loss: 0.6559\n",
            "Epoch 28/200 loss: 0.6404\n",
            "Epoch 29/200 loss: 0.6251\n",
            "Epoch 30/200 loss: 0.6128\n",
            "Epoch 31/200 loss: 0.5994\n",
            "Epoch 32/200 loss: 0.5848\n",
            "Epoch 33/200 loss: 0.5738\n",
            "Epoch 34/200 loss: 0.5638\n",
            "Epoch 35/200 loss: 0.5499\n",
            "Epoch 36/200 loss: 0.5402\n",
            "Epoch 37/200 loss: 0.5308\n",
            "Epoch 38/200 loss: 0.5214\n",
            "Epoch 39/200 loss: 0.5130\n",
            "Epoch 40/200 loss: 0.5056\n",
            "Epoch 41/200 loss: 0.4956\n",
            "Epoch 42/200 loss: 0.4886\n",
            "Epoch 43/200 loss: 0.4820\n",
            "Epoch 44/200 loss: 0.4742\n",
            "Epoch 45/200 loss: 0.4682\n",
            "Epoch 46/200 loss: 0.4618\n",
            "Epoch 47/200 loss: 0.4551\n",
            "Epoch 48/200 loss: 0.4498\n",
            "Epoch 49/200 loss: 0.4446\n",
            "Epoch 50/200 loss: 0.4392\n",
            "Epoch 51/200 loss: 0.4336\n",
            "Epoch 52/200 loss: 0.4287\n",
            "Epoch 53/200 loss: 0.4238\n",
            "Epoch 54/200 loss: 0.4191\n",
            "Epoch 55/200 loss: 0.4146\n",
            "Epoch 56/200 loss: 0.4111\n",
            "Epoch 57/200 loss: 0.4072\n",
            "Epoch 58/200 loss: 0.4029\n",
            "Epoch 59/200 loss: 0.3995\n",
            "Epoch 60/200 loss: 0.3959\n",
            "Epoch 61/200 loss: 0.3915\n",
            "Epoch 62/200 loss: 0.3899\n",
            "Epoch 63/200 loss: 0.3866\n",
            "Epoch 64/200 loss: 0.3833\n",
            "Epoch 65/200 loss: 0.3789\n",
            "Epoch 66/200 loss: 0.3765\n",
            "Epoch 67/200 loss: 0.3733\n",
            "Epoch 68/200 loss: 0.3717\n",
            "Epoch 69/200 loss: 0.3689\n",
            "Epoch 70/200 loss: 0.3680\n",
            "Epoch 71/200 loss: 0.3640\n",
            "Epoch 72/200 loss: 0.3604\n",
            "Epoch 73/200 loss: 0.3592\n",
            "Epoch 74/200 loss: 0.3569\n",
            "Epoch 75/200 loss: 0.3540\n",
            "Epoch 76/200 loss: 0.3529\n",
            "Epoch 77/200 loss: 0.3515\n",
            "Epoch 78/200 loss: 0.3487\n",
            "Epoch 79/200 loss: 0.3459\n",
            "Epoch 80/200 loss: 0.3455\n",
            "Epoch 81/200 loss: 0.3435\n",
            "Epoch 82/200 loss: 0.3413\n",
            "Epoch 83/200 loss: 0.3399\n",
            "Epoch 84/200 loss: 0.3387\n",
            "Epoch 85/200 loss: 0.3379\n",
            "Epoch 86/200 loss: 0.3341\n",
            "Epoch 87/200 loss: 0.3330\n",
            "Epoch 88/200 loss: 0.3323\n",
            "Epoch 89/200 loss: 0.3314\n",
            "Epoch 90/200 loss: 0.3293\n",
            "Epoch 91/200 loss: 0.3276\n",
            "Epoch 92/200 loss: 0.3279\n",
            "Epoch 93/200 loss: 0.3266\n",
            "Epoch 94/200 loss: 0.3233\n",
            "Epoch 95/200 loss: 0.3220\n",
            "Epoch 96/200 loss: 0.3222\n",
            "Epoch 97/200 loss: 0.3205\n",
            "Epoch 98/200 loss: 0.3188\n",
            "Epoch 99/200 loss: 0.3186\n",
            "Epoch 100/200 loss: 0.3168\n",
            "Epoch 101/200 loss: 0.3170\n",
            "Epoch 102/200 loss: 0.3154\n",
            "Epoch 103/200 loss: 0.3138\n",
            "Epoch 104/200 loss: 0.3129\n",
            "Epoch 105/200 loss: 0.3113\n",
            "Epoch 106/200 loss: 0.3113\n",
            "Epoch 107/200 loss: 0.3096\n",
            "Epoch 108/200 loss: 0.3090\n",
            "Epoch 109/200 loss: 0.3076\n",
            "Epoch 110/200 loss: 0.3071\n",
            "Epoch 111/200 loss: 0.3075\n",
            "Epoch 112/200 loss: 0.3051\n",
            "Epoch 113/200 loss: 0.3039\n",
            "Epoch 114/200 loss: 0.3039\n",
            "Epoch 115/200 loss: 0.3039\n",
            "Epoch 116/200 loss: 0.3019\n",
            "Epoch 117/200 loss: 0.3022\n",
            "Epoch 118/200 loss: 0.3009\n",
            "Epoch 119/200 loss: 0.3005\n",
            "Epoch 120/200 loss: 0.2986\n",
            "Epoch 121/200 loss: 0.2984\n",
            "Epoch 122/200 loss: 0.2978\n",
            "Epoch 123/200 loss: 0.2979\n",
            "Epoch 124/200 loss: 0.2962\n",
            "Epoch 125/200 loss: 0.2972\n",
            "Epoch 126/200 loss: 0.2953\n",
            "Epoch 127/200 loss: 0.2948\n",
            "Epoch 128/200 loss: 0.2943\n",
            "Epoch 129/200 loss: 0.2937\n",
            "Epoch 130/200 loss: 0.2933\n",
            "Epoch 131/200 loss: 0.2914\n",
            "Epoch 132/200 loss: 0.2908\n",
            "Epoch 133/200 loss: 0.2908\n",
            "Epoch 134/200 loss: 0.2912\n",
            "Epoch 135/200 loss: 0.2902\n",
            "Epoch 136/200 loss: 0.2890\n",
            "Epoch 137/200 loss: 0.2881\n",
            "Epoch 138/200 loss: 0.2897\n",
            "Epoch 139/200 loss: 0.2884\n",
            "Epoch 140/200 loss: 0.2874\n",
            "Epoch 141/200 loss: 0.2870\n",
            "Epoch 142/200 loss: 0.2868\n",
            "Epoch 143/200 loss: 0.2858\n",
            "Epoch 144/200 loss: 0.2851\n",
            "Epoch 145/200 loss: 0.2853\n",
            "Epoch 146/200 loss: 0.2849\n",
            "Epoch 147/200 loss: 0.2845\n",
            "Epoch 148/200 loss: 0.2845\n",
            "Epoch 149/200 loss: 0.2834\n",
            "Epoch 150/200 loss: 0.2831\n",
            "Epoch 151/200 loss: 0.2822\n",
            "Epoch 152/200 loss: 0.2832\n",
            "Epoch 153/200 loss: 0.2814\n",
            "Epoch 154/200 loss: 0.2805\n",
            "Epoch 155/200 loss: 0.2802\n",
            "Epoch 156/200 loss: 0.2816\n",
            "Epoch 157/200 loss: 0.2787\n",
            "Epoch 158/200 loss: 0.2799\n",
            "Epoch 159/200 loss: 0.2813\n",
            "Epoch 160/200 loss: 0.2785\n",
            "Epoch 161/200 loss: 0.2772\n",
            "Epoch 162/200 loss: 0.2777\n",
            "Epoch 163/200 loss: 0.2771\n",
            "Epoch 164/200 loss: 0.2777\n",
            "Epoch 165/200 loss: 0.2770\n",
            "Epoch 166/200 loss: 0.2761\n",
            "Epoch 167/200 loss: 0.2748\n",
            "Epoch 168/200 loss: 0.2760\n",
            "Epoch 169/200 loss: 0.2743\n",
            "Epoch 170/200 loss: 0.2747\n",
            "Epoch 171/200 loss: 0.2747\n",
            "Epoch 172/200 loss: 0.2747\n",
            "Epoch 173/200 loss: 0.2730\n",
            "Epoch 174/200 loss: 0.2729\n",
            "Epoch 175/200 loss: 0.2733\n",
            "Epoch 176/200 loss: 0.2729\n",
            "Epoch 177/200 loss: 0.2722\n",
            "Epoch 178/200 loss: 0.2721\n",
            "Epoch 179/200 loss: 0.2721\n",
            "Epoch 180/200 loss: 0.2721\n",
            "Epoch 181/200 loss: 0.2714\n",
            "Epoch 182/200 loss: 0.2710\n",
            "Epoch 183/200 loss: 0.2714\n",
            "Epoch 184/200 loss: 0.2703\n",
            "Epoch 185/200 loss: 0.2709\n",
            "Epoch 186/200 loss: 0.2698\n",
            "Epoch 187/200 loss: 0.2692\n",
            "Epoch 188/200 loss: 0.2698\n",
            "Epoch 189/200 loss: 0.2687\n",
            "Epoch 190/200 loss: 0.2682\n",
            "Epoch 191/200 loss: 0.2687\n",
            "Epoch 192/200 loss: 0.2682\n",
            "Epoch 193/200 loss: 0.2682\n",
            "Epoch 194/200 loss: 0.2672\n",
            "Epoch 195/200 loss: 0.2678\n",
            "Epoch 196/200 loss: 0.2665\n",
            "Epoch 197/200 loss: 0.2671\n",
            "Epoch 198/200 loss: 0.2667\n",
            "Epoch 199/200 loss: 0.2663\n",
            "Epoch 200/200 loss: 0.2657\n",
            "batch size: 256, learning rate: 0.001, num epochs: 20\n",
            "Epoch 1/20 loss: 1.8390\n",
            "Epoch 2/20 loss: 1.7786\n",
            "Epoch 3/20 loss: 1.7253\n",
            "Epoch 4/20 loss: 1.6722\n",
            "Epoch 5/20 loss: 1.6245\n",
            "Epoch 6/20 loss: 1.5737\n",
            "Epoch 7/20 loss: 1.5338\n",
            "Epoch 8/20 loss: 1.4848\n",
            "Epoch 9/20 loss: 1.4436\n",
            "Epoch 10/20 loss: 1.4041\n",
            "Epoch 11/20 loss: 1.3682\n",
            "Epoch 12/20 loss: 1.3290\n",
            "Epoch 13/20 loss: 1.3010\n",
            "Epoch 14/20 loss: 1.2652\n",
            "Epoch 15/20 loss: 1.2364\n",
            "Epoch 16/20 loss: 1.2090\n",
            "Epoch 17/20 loss: 1.1822\n",
            "Epoch 18/20 loss: 1.1577\n",
            "Epoch 19/20 loss: 1.1300\n",
            "Epoch 20/20 loss: 1.1095\n",
            "batch size: 256, learning rate: 0.001, num epochs: 50\n",
            "Epoch 1/50 loss: 1.9772\n",
            "Epoch 2/50 loss: 1.9251\n",
            "Epoch 3/50 loss: 1.8783\n",
            "Epoch 4/50 loss: 1.8290\n",
            "Epoch 5/50 loss: 1.7870\n",
            "Epoch 6/50 loss: 1.7449\n",
            "Epoch 7/50 loss: 1.7049\n",
            "Epoch 8/50 loss: 1.6685\n",
            "Epoch 9/50 loss: 1.6309\n",
            "Epoch 10/50 loss: 1.6026\n",
            "Epoch 11/50 loss: 1.5652\n",
            "Epoch 12/50 loss: 1.5339\n",
            "Epoch 13/50 loss: 1.5084\n",
            "Epoch 14/50 loss: 1.4784\n",
            "Epoch 15/50 loss: 1.4538\n",
            "Epoch 16/50 loss: 1.4292\n",
            "Epoch 17/50 loss: 1.4090\n",
            "Epoch 18/50 loss: 1.3842\n",
            "Epoch 19/50 loss: 1.3615\n",
            "Epoch 20/50 loss: 1.3470\n",
            "Epoch 21/50 loss: 1.3246\n",
            "Epoch 22/50 loss: 1.3099\n",
            "Epoch 23/50 loss: 1.2919\n",
            "Epoch 24/50 loss: 1.2751\n",
            "Epoch 25/50 loss: 1.2612\n",
            "Epoch 26/50 loss: 1.2474\n",
            "Epoch 27/50 loss: 1.2347\n",
            "Epoch 28/50 loss: 1.2226\n",
            "Epoch 29/50 loss: 1.2091\n",
            "Epoch 30/50 loss: 1.1987\n",
            "Epoch 31/50 loss: 1.1863\n",
            "Epoch 32/50 loss: 1.1780\n",
            "Epoch 33/50 loss: 1.1667\n",
            "Epoch 34/50 loss: 1.1567\n",
            "Epoch 35/50 loss: 1.1459\n",
            "Epoch 36/50 loss: 1.1398\n",
            "Epoch 37/50 loss: 1.1335\n",
            "Epoch 38/50 loss: 1.1250\n",
            "Epoch 39/50 loss: 1.1153\n",
            "Epoch 40/50 loss: 1.1092\n",
            "Epoch 41/50 loss: 1.0999\n",
            "Epoch 42/50 loss: 1.0945\n",
            "Epoch 43/50 loss: 1.0863\n",
            "Epoch 44/50 loss: 1.0815\n",
            "Epoch 45/50 loss: 1.0749\n",
            "Epoch 46/50 loss: 1.0693\n",
            "Epoch 47/50 loss: 1.0631\n",
            "Epoch 48/50 loss: 1.0569\n",
            "Epoch 49/50 loss: 1.0514\n",
            "Epoch 50/50 loss: 1.0479\n",
            "batch size: 256, learning rate: 0.001, num epochs: 100\n",
            "Epoch 1/100 loss: 1.2719\n",
            "Epoch 2/100 loss: 1.2606\n",
            "Epoch 3/100 loss: 1.2503\n",
            "Epoch 4/100 loss: 1.2364\n",
            "Epoch 5/100 loss: 1.2277\n",
            "Epoch 6/100 loss: 1.2189\n",
            "Epoch 7/100 loss: 1.2103\n",
            "Epoch 8/100 loss: 1.2006\n",
            "Epoch 9/100 loss: 1.1906\n",
            "Epoch 10/100 loss: 1.1802\n",
            "Epoch 11/100 loss: 1.1784\n",
            "Epoch 12/100 loss: 1.1704\n",
            "Epoch 13/100 loss: 1.1572\n",
            "Epoch 14/100 loss: 1.1478\n",
            "Epoch 15/100 loss: 1.1428\n",
            "Epoch 16/100 loss: 1.1322\n",
            "Epoch 17/100 loss: 1.1232\n",
            "Epoch 18/100 loss: 1.1152\n",
            "Epoch 19/100 loss: 1.1077\n",
            "Epoch 20/100 loss: 1.0998\n",
            "Epoch 21/100 loss: 1.0934\n",
            "Epoch 22/100 loss: 1.0857\n",
            "Epoch 23/100 loss: 1.0796\n",
            "Epoch 24/100 loss: 1.0706\n",
            "Epoch 25/100 loss: 1.0630\n",
            "Epoch 26/100 loss: 1.0535\n",
            "Epoch 27/100 loss: 1.0479\n",
            "Epoch 28/100 loss: 1.0440\n",
            "Epoch 29/100 loss: 1.0351\n",
            "Epoch 30/100 loss: 1.0341\n",
            "Epoch 31/100 loss: 1.0237\n",
            "Epoch 32/100 loss: 1.0205\n",
            "Epoch 33/100 loss: 1.0128\n",
            "Epoch 34/100 loss: 1.0033\n",
            "Epoch 35/100 loss: 0.9977\n",
            "Epoch 36/100 loss: 0.9932\n",
            "Epoch 37/100 loss: 0.9874\n",
            "Epoch 38/100 loss: 0.9785\n",
            "Epoch 39/100 loss: 0.9731\n",
            "Epoch 40/100 loss: 0.9671\n",
            "Epoch 41/100 loss: 0.9610\n",
            "Epoch 42/100 loss: 0.9547\n",
            "Epoch 43/100 loss: 0.9507\n",
            "Epoch 44/100 loss: 0.9476\n",
            "Epoch 45/100 loss: 0.9398\n",
            "Epoch 46/100 loss: 0.9344\n",
            "Epoch 47/100 loss: 0.9277\n",
            "Epoch 48/100 loss: 0.9268\n",
            "Epoch 49/100 loss: 0.9194\n",
            "Epoch 50/100 loss: 0.9122\n",
            "Epoch 51/100 loss: 0.9099\n",
            "Epoch 52/100 loss: 0.9027\n",
            "Epoch 53/100 loss: 0.8979\n",
            "Epoch 54/100 loss: 0.8942\n",
            "Epoch 55/100 loss: 0.8859\n",
            "Epoch 56/100 loss: 0.8822\n",
            "Epoch 57/100 loss: 0.8759\n",
            "Epoch 58/100 loss: 0.8722\n",
            "Epoch 59/100 loss: 0.8675\n",
            "Epoch 60/100 loss: 0.8622\n",
            "Epoch 61/100 loss: 0.8552\n",
            "Epoch 62/100 loss: 0.8509\n",
            "Epoch 63/100 loss: 0.8482\n",
            "Epoch 64/100 loss: 0.8414\n",
            "Epoch 65/100 loss: 0.8384\n",
            "Epoch 66/100 loss: 0.8303\n",
            "Epoch 67/100 loss: 0.8274\n",
            "Epoch 68/100 loss: 0.8238\n",
            "Epoch 69/100 loss: 0.8192\n",
            "Epoch 70/100 loss: 0.8142\n",
            "Epoch 71/100 loss: 0.8076\n",
            "Epoch 72/100 loss: 0.8037\n",
            "Epoch 73/100 loss: 0.7995\n",
            "Epoch 74/100 loss: 0.7965\n",
            "Epoch 75/100 loss: 0.7903\n",
            "Epoch 76/100 loss: 0.7848\n",
            "Epoch 77/100 loss: 0.7818\n",
            "Epoch 78/100 loss: 0.7776\n",
            "Epoch 79/100 loss: 0.7754\n",
            "Epoch 80/100 loss: 0.7706\n",
            "Epoch 81/100 loss: 0.7680\n",
            "Epoch 82/100 loss: 0.7639\n",
            "Epoch 83/100 loss: 0.7573\n",
            "Epoch 84/100 loss: 0.7553\n",
            "Epoch 85/100 loss: 0.7498\n",
            "Epoch 86/100 loss: 0.7463\n",
            "Epoch 87/100 loss: 0.7402\n",
            "Epoch 88/100 loss: 0.7359\n",
            "Epoch 89/100 loss: 0.7341\n",
            "Epoch 90/100 loss: 0.7297\n",
            "Epoch 91/100 loss: 0.7250\n",
            "Epoch 92/100 loss: 0.7208\n",
            "Epoch 93/100 loss: 0.7204\n",
            "Epoch 94/100 loss: 0.7153\n",
            "Epoch 95/100 loss: 0.7114\n",
            "Epoch 96/100 loss: 0.7056\n",
            "Epoch 97/100 loss: 0.7045\n",
            "Epoch 98/100 loss: 0.6974\n",
            "Epoch 99/100 loss: 0.6947\n",
            "Epoch 100/100 loss: 0.6932\n",
            "batch size: 256, learning rate: 0.001, num epochs: 200\n",
            "Epoch 1/200 loss: 1.2966\n",
            "Epoch 2/200 loss: 1.2919\n",
            "Epoch 3/200 loss: 1.2869\n",
            "Epoch 4/200 loss: 1.2786\n",
            "Epoch 5/200 loss: 1.2723\n",
            "Epoch 6/200 loss: 1.2652\n",
            "Epoch 7/200 loss: 1.2585\n",
            "Epoch 8/200 loss: 1.2533\n",
            "Epoch 9/200 loss: 1.2475\n",
            "Epoch 10/200 loss: 1.2406\n",
            "Epoch 11/200 loss: 1.2359\n",
            "Epoch 12/200 loss: 1.2292\n",
            "Epoch 13/200 loss: 1.2249\n",
            "Epoch 14/200 loss: 1.2184\n",
            "Epoch 15/200 loss: 1.2110\n",
            "Epoch 16/200 loss: 1.2069\n",
            "Epoch 17/200 loss: 1.1985\n",
            "Epoch 18/200 loss: 1.1935\n",
            "Epoch 19/200 loss: 1.1918\n",
            "Epoch 20/200 loss: 1.1849\n",
            "Epoch 21/200 loss: 1.1799\n",
            "Epoch 22/200 loss: 1.1729\n",
            "Epoch 23/200 loss: 1.1675\n",
            "Epoch 24/200 loss: 1.1625\n",
            "Epoch 25/200 loss: 1.1583\n",
            "Epoch 26/200 loss: 1.1540\n",
            "Epoch 27/200 loss: 1.1483\n",
            "Epoch 28/200 loss: 1.1438\n",
            "Epoch 29/200 loss: 1.1354\n",
            "Epoch 30/200 loss: 1.1327\n",
            "Epoch 31/200 loss: 1.1268\n",
            "Epoch 32/200 loss: 1.1224\n",
            "Epoch 33/200 loss: 1.1156\n",
            "Epoch 34/200 loss: 1.1119\n",
            "Epoch 35/200 loss: 1.1076\n",
            "Epoch 36/200 loss: 1.1021\n",
            "Epoch 37/200 loss: 1.0979\n",
            "Epoch 38/200 loss: 1.0936\n",
            "Epoch 39/200 loss: 1.0884\n",
            "Epoch 40/200 loss: 1.0855\n",
            "Epoch 41/200 loss: 1.0775\n",
            "Epoch 42/200 loss: 1.0753\n",
            "Epoch 43/200 loss: 1.0692\n",
            "Epoch 44/200 loss: 1.0651\n",
            "Epoch 45/200 loss: 1.0601\n",
            "Epoch 46/200 loss: 1.0582\n",
            "Epoch 47/200 loss: 1.0519\n",
            "Epoch 48/200 loss: 1.0468\n",
            "Epoch 49/200 loss: 1.0430\n",
            "Epoch 50/200 loss: 1.0400\n",
            "Epoch 51/200 loss: 1.0332\n",
            "Epoch 52/200 loss: 1.0301\n",
            "Epoch 53/200 loss: 1.0269\n",
            "Epoch 54/200 loss: 1.0231\n",
            "Epoch 55/200 loss: 1.0182\n",
            "Epoch 56/200 loss: 1.0130\n",
            "Epoch 57/200 loss: 1.0125\n",
            "Epoch 58/200 loss: 1.0051\n",
            "Epoch 59/200 loss: 1.0016\n",
            "Epoch 60/200 loss: 0.9961\n",
            "Epoch 61/200 loss: 0.9947\n",
            "Epoch 62/200 loss: 0.9891\n",
            "Epoch 63/200 loss: 0.9852\n",
            "Epoch 64/200 loss: 0.9829\n",
            "Epoch 65/200 loss: 0.9776\n",
            "Epoch 66/200 loss: 0.9730\n",
            "Epoch 67/200 loss: 0.9699\n",
            "Epoch 68/200 loss: 0.9665\n",
            "Epoch 69/200 loss: 0.9639\n",
            "Epoch 70/200 loss: 0.9613\n",
            "Epoch 71/200 loss: 0.9571\n",
            "Epoch 72/200 loss: 0.9514\n",
            "Epoch 73/200 loss: 0.9491\n",
            "Epoch 74/200 loss: 0.9455\n",
            "Epoch 75/200 loss: 0.9410\n",
            "Epoch 76/200 loss: 0.9351\n",
            "Epoch 77/200 loss: 0.9354\n",
            "Epoch 78/200 loss: 0.9292\n",
            "Epoch 79/200 loss: 0.9261\n",
            "Epoch 80/200 loss: 0.9238\n",
            "Epoch 81/200 loss: 0.9195\n",
            "Epoch 82/200 loss: 0.9160\n",
            "Epoch 83/200 loss: 0.9117\n",
            "Epoch 84/200 loss: 0.9091\n",
            "Epoch 85/200 loss: 0.9078\n",
            "Epoch 86/200 loss: 0.9039\n",
            "Epoch 87/200 loss: 0.8988\n",
            "Epoch 88/200 loss: 0.8975\n",
            "Epoch 89/200 loss: 0.8933\n",
            "Epoch 90/200 loss: 0.8893\n",
            "Epoch 91/200 loss: 0.8861\n",
            "Epoch 92/200 loss: 0.8825\n",
            "Epoch 93/200 loss: 0.8813\n",
            "Epoch 94/200 loss: 0.8779\n",
            "Epoch 95/200 loss: 0.8738\n",
            "Epoch 96/200 loss: 0.8717\n",
            "Epoch 97/200 loss: 0.8674\n",
            "Epoch 98/200 loss: 0.8636\n",
            "Epoch 99/200 loss: 0.8610\n",
            "Epoch 100/200 loss: 0.8560\n",
            "Epoch 101/200 loss: 0.8549\n",
            "Epoch 102/200 loss: 0.8515\n",
            "Epoch 103/200 loss: 0.8477\n",
            "Epoch 104/200 loss: 0.8446\n",
            "Epoch 105/200 loss: 0.8429\n",
            "Epoch 106/200 loss: 0.8406\n",
            "Epoch 107/200 loss: 0.8374\n",
            "Epoch 108/200 loss: 0.8327\n",
            "Epoch 109/200 loss: 0.8317\n",
            "Epoch 110/200 loss: 0.8271\n",
            "Epoch 111/200 loss: 0.8284\n",
            "Epoch 112/200 loss: 0.8214\n",
            "Epoch 113/200 loss: 0.8182\n",
            "Epoch 114/200 loss: 0.8166\n",
            "Epoch 115/200 loss: 0.8140\n",
            "Epoch 116/200 loss: 0.8102\n",
            "Epoch 117/200 loss: 0.8079\n",
            "Epoch 118/200 loss: 0.8051\n",
            "Epoch 119/200 loss: 0.8033\n",
            "Epoch 120/200 loss: 0.8012\n",
            "Epoch 121/200 loss: 0.7976\n",
            "Epoch 122/200 loss: 0.7937\n",
            "Epoch 123/200 loss: 0.7928\n",
            "Epoch 124/200 loss: 0.7897\n",
            "Epoch 125/200 loss: 0.7878\n",
            "Epoch 126/200 loss: 0.7836\n",
            "Epoch 127/200 loss: 0.7823\n",
            "Epoch 128/200 loss: 0.7795\n",
            "Epoch 129/200 loss: 0.7765\n",
            "Epoch 130/200 loss: 0.7755\n",
            "Epoch 131/200 loss: 0.7719\n",
            "Epoch 132/200 loss: 0.7682\n",
            "Epoch 133/200 loss: 0.7671\n",
            "Epoch 134/200 loss: 0.7647\n",
            "Epoch 135/200 loss: 0.7620\n",
            "Epoch 136/200 loss: 0.7591\n",
            "Epoch 137/200 loss: 0.7561\n",
            "Epoch 138/200 loss: 0.7550\n",
            "Epoch 139/200 loss: 0.7531\n",
            "Epoch 140/200 loss: 0.7495\n",
            "Epoch 141/200 loss: 0.7468\n",
            "Epoch 142/200 loss: 0.7436\n",
            "Epoch 143/200 loss: 0.7436\n",
            "Epoch 144/200 loss: 0.7406\n",
            "Epoch 145/200 loss: 0.7376\n",
            "Epoch 146/200 loss: 0.7340\n",
            "Epoch 147/200 loss: 0.7331\n",
            "Epoch 148/200 loss: 0.7306\n",
            "Epoch 149/200 loss: 0.7284\n",
            "Epoch 150/200 loss: 0.7259\n",
            "Epoch 151/200 loss: 0.7243\n",
            "Epoch 152/200 loss: 0.7218\n",
            "Epoch 153/200 loss: 0.7197\n",
            "Epoch 154/200 loss: 0.7178\n",
            "Epoch 155/200 loss: 0.7159\n",
            "Epoch 156/200 loss: 0.7146\n",
            "Epoch 157/200 loss: 0.7108\n",
            "Epoch 158/200 loss: 0.7108\n",
            "Epoch 159/200 loss: 0.7069\n",
            "Epoch 160/200 loss: 0.7060\n",
            "Epoch 161/200 loss: 0.7026\n",
            "Epoch 162/200 loss: 0.7007\n",
            "Epoch 163/200 loss: 0.6997\n",
            "Epoch 164/200 loss: 0.6970\n",
            "Epoch 165/200 loss: 0.6940\n",
            "Epoch 166/200 loss: 0.6915\n",
            "Epoch 167/200 loss: 0.6902\n",
            "Epoch 168/200 loss: 0.6891\n",
            "Epoch 169/200 loss: 0.6855\n",
            "Epoch 170/200 loss: 0.6847\n",
            "Epoch 171/200 loss: 0.6816\n",
            "Epoch 172/200 loss: 0.6792\n",
            "Epoch 173/200 loss: 0.6778\n",
            "Epoch 174/200 loss: 0.6764\n",
            "Epoch 175/200 loss: 0.6755\n",
            "Epoch 176/200 loss: 0.6713\n",
            "Epoch 177/200 loss: 0.6724\n",
            "Epoch 178/200 loss: 0.6696\n",
            "Epoch 179/200 loss: 0.6663\n",
            "Epoch 180/200 loss: 0.6648\n",
            "Epoch 181/200 loss: 0.6629\n",
            "Epoch 182/200 loss: 0.6622\n",
            "Epoch 183/200 loss: 0.6609\n",
            "Epoch 184/200 loss: 0.6570\n",
            "Epoch 185/200 loss: 0.6566\n",
            "Epoch 186/200 loss: 0.6532\n",
            "Epoch 187/200 loss: 0.6519\n",
            "Epoch 188/200 loss: 0.6495\n",
            "Epoch 189/200 loss: 0.6478\n",
            "Epoch 190/200 loss: 0.6463\n",
            "Epoch 191/200 loss: 0.6447\n",
            "Epoch 192/200 loss: 0.6422\n",
            "Epoch 193/200 loss: 0.6411\n",
            "Epoch 194/200 loss: 0.6400\n",
            "Epoch 195/200 loss: 0.6367\n",
            "Epoch 196/200 loss: 0.6354\n",
            "Epoch 197/200 loss: 0.6342\n",
            "Epoch 198/200 loss: 0.6317\n",
            "Epoch 199/200 loss: 0.6307\n",
            "Epoch 200/200 loss: 0.6286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy = max(validation_accuracies, key=validation_accuracies.get)\n",
        "print(f\"Best accuracy before regularization is: {validation_accuracies[best_accuracy]} with batch size: {best_accuracy[0]}, learning rate: {best_accuracy[1]}, num epochs: {best_accuracy[2]}\")"
      ],
      "metadata": {
        "id": "mWTlz-L3dfE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_accuracies"
      ],
      "metadata": {
        "id": "bm6r5r2UZyjr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# regularization"
      ],
      "metadata": {
        "id": "2wj9wA9kd-K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# L2 regularization\n",
        "lambdas = [0.001, 0.01, 1]\n",
        "validation_accuracies_regularized = {}\n",
        "best_bs = best_accuracy[0] # batch size\n",
        "best_lr = best_accuracy[1] # learning rate\n",
        "best_ne = best_accuracy[2] # num epochs\n",
        "\n",
        "for lamb in lambdas:\n",
        "  print(f\"lambda: {lamb}\")\n",
        "  model = LogisticRegression()\n",
        "  optimizer = tf.optimizers.SGD(best_lr)\n",
        "  train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "  train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
        "  epoch_loss = 0\n",
        "  num_batches = 0\n",
        "\n",
        "  for epoch in range(best_ne):\n",
        "    epoch_loss = 0  # reset loss per epoch\n",
        "    num_batches = 0\n",
        "    for x_batch, y_batch in train_dataset:\n",
        "      # forward pass\n",
        "      with tf.GradientTape() as tape:\n",
        "        z = model(x_batch)  # raw logits\n",
        "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_batch, logits=z))\n",
        "        # L2 reg term 0.5 * lambda * (w • w)\n",
        "        reg_loss = 0.5 * lamb * tf.reduce_sum(tf.square(model.W))\n",
        "        loss += reg_loss\n",
        "      gradients = tape.gradient(loss, [model.W, model.b])\n",
        "      optimizer.apply_gradients(zip(gradients, [model.W, model.b]))\n",
        "      epoch_loss += loss.numpy()\n",
        "      num_batches += 1\n",
        "      print(f\"Epoch {epoch + 1}/{num_epoch} loss: {epoch_loss / num_batches:.4f}\")"
      ],
      "metadata": {
        "id": "zunaTba_ZLJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the best lambda\n",
        "best_lambda = max(validation_accuracies_regularized, key=validation_accuracies_regularized.get)"
      ],
      "metadata": {
        "id": "1Xvup5FHn5Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# actually compile it now"
      ],
      "metadata": {
        "id": "K_W4l2kNoNCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "logistic_model = keras.Sequential([\n",
        "    keras.layers.Dense(1,\n",
        "                       activation='sigmoid',\n",
        "                       input_shape=(X_train.shape[1],),\n",
        "                       kernel_regularizer = regularizers.l2(0.001)\n",
        "                       )\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhXl5dkW0BV_",
        "outputId": "d4326a9e-ca50-4b05-fbf5-f914bc7dcaf0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.05),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy', 'loss']\n",
        ")"
      ],
      "metadata": {
        "id": "BrxP_7x40Q4l"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB0o8tOP1NIY",
        "outputId": "8dfd76f2-5567-4ccc-83f4-d9efe2248fda"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([11409, 140])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0G9jTEX1hyN",
        "outputId": "c63f2a7b-feee-4589-b308-9f0bbc5415ed"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([11409])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = ces24_df.sample(frac=0.75, random_state=0)\n",
        "# test_dataset = ces24_df.drop(train_dataset.index)\n",
        "# train and test dummy data\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "train_dataset = ces24_dummies.sample(frac=0.75, random_state=0)\n",
        "test_dataset = ces24_dummies.drop(train_dataset.index)\n",
        "\n",
        "# feature matrices and target vectors\n",
        "X_train, y_train = train_dataset.drop(columns=['vote_trump']), train_dataset['vote_trump']\n",
        "X_test, y_test = test_dataset.drop(columns=['vote_trump']), test_dataset['vote_trump']\n",
        "\n",
        "lr, l2 = 0.05, 0.001\n",
        "# define the logistic model architecture\n",
        "logistic_model = keras.Sequential([\n",
        "    keras.layers.Dense(1,\n",
        "                       activation='sigmoid',\n",
        "                       input_shape=(X_train.shape[1],),\n",
        "                       kernel_regularizer = regularizers.l2(l2)\n",
        "                       )\n",
        "])\n",
        "\n",
        "# compile model using adam optimizer, binary cross entropy loss, and accuracy metric\n",
        "logistic_model.compile(\n",
        "    optimizer=tf.keras.optimizers.SGD(learning_rate=lr),\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# fit it\n",
        "history = logistic_model.fit(X_train, y_train,\n",
        "                             epochs=100,\n",
        "                             batch_size=32,\n",
        "                             validation_data=(X_test, y_test),\n",
        "                             verbose=0)\n",
        "\n",
        "y_pred_prob = logistic_model.predict(X_test)\n",
        "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7vYIB29182E",
        "outputId": "6a580645-c2c1-4ce9-f491-41823b57e350"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m357/357\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.93      0.94      6498\n",
            "           1       0.91      0.92      0.92      4911\n",
            "\n",
            "    accuracy                           0.93     11409\n",
            "   macro avg       0.93      0.93      0.93     11409\n",
            "weighted avg       0.93      0.93      0.93     11409\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Binary Crossentropy Loss')\n",
        "plt.title('Training and Validation Loss vs Number of Iterations')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "-KpKSoR_4f4L",
        "outputId": "55949eab-b05d-401d-e194-a8e77f0130ff"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy vs Number of Iterations')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(top=0.935)\n",
        "# add vertical line and text to consider stopping earlier (at 100 epochs?)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "wo7ENyOY46mH",
        "outputId": "d67562f1-d2ae-4bc8-cff9-06d7ee798f22"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}